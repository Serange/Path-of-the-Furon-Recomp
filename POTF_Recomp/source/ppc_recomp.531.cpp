#include "ppc_recomp_shared.h"

__attribute__((alias("__imp__sub_830BBF88"))) PPC_WEAK_FUNC(sub_830BBF88);
PPC_FUNC_IMPL(__imp__sub_830BBF88) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10ec
	ctx.lr = 0x830BBF90;
	__savegprlr_29(ctx, base);
	// addi r12,r1,-32
	ctx.r12.s64 = ctx.r1.s64 + -32;
	// bl 0x82cb6ab8
	ctx.lr = 0x830BBF98;
	__savefpr_16(ctx, base);
	// stwu r1,-352(r1)
	ea = -352 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lfs f10,16(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f11,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// fmr f4,f10
	ctx.f4.f64 = ctx.f10.f64;
	// lfs f8,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f8.f64 = double(temp.f32);
	// fmr f6,f11
	ctx.f6.f64 = ctx.f11.f64;
	// fmr f2,f8
	ctx.f2.f64 = ctx.f8.f64;
	// lfs f0,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fadds f9,f0,f11
	ctx.f9.f64 = double(float(ctx.f0.f64 + ctx.f11.f64));
	// lfs f12,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fadds f7,f10,f13
	ctx.f7.f64 = double(float(ctx.f10.f64 + ctx.f13.f64));
	// fadds f5,f8,f12
	ctx.f5.f64 = double(float(ctx.f8.f64 + ctx.f12.f64));
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f31,6380(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6380);
	ctx.f31.f64 = double(temp.f32);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r11,r4,12
	ctx.r11.s64 = ctx.r4.s64 + 12;
	// fsubs f1,f4,f13
	ctx.f1.f64 = double(float(ctx.f4.f64 - ctx.f13.f64));
	// lfs f30,6140(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6140);
	ctx.f30.f64 = double(temp.f32);
	// fsubs f3,f6,f0
	ctx.f3.f64 = double(float(ctx.f6.f64 - ctx.f0.f64));
	// lfs f0,6048(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6048);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f13,f2,f12
	ctx.f13.f64 = double(float(ctx.f2.f64 - ctx.f12.f64));
	// stfs f30,152(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f12,f9,f31
	ctx.f12.f64 = double(float(ctx.f9.f64 * ctx.f31.f64));
	// stfs f0,156(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 156, temp.u32);
	// fmuls f11,f7,f31
	ctx.f11.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// stfs f0,160(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// fmuls f10,f5,f31
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// stfs f0,164(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// stfs f30,168(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 168, temp.u32);
	// lwz r30,268(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// stfs f0,172(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// stfs f0,176(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// stfs f0,180(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f8,f1,f31
	ctx.f8.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// stfs f30,184(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// fmuls f9,f3,f31
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// stfs f12,128(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// fmuls f7,f13,f31
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// stfs f11,132(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 132, temp.u32);
	// stfs f10,136(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f9,140(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 140, temp.u32);
	// stfs f8,144(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// stfs f7,148(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// lwz r3,308(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 308);
	// bl 0x83073d18
	ctx.lr = 0x830BC058;
	sub_83073D18(ctx, base);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830bc25c
	if (ctx.cr6.eq) goto loc_830BC25C;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830bc25c
	if (ctx.cr6.eq) goto loc_830BC25C;
	// lfs f13,252(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f13.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f12,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f12.f64 = double(temp.f32);
	// fmr f11,f13
	ctx.f11.f64 = ctx.f13.f64;
	// fmuls f10,f12,f13
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f8,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f8.f64 = double(temp.f32);
	// lfs f6,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f5,f12,f8
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// lfs f9,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f3,f6,f8
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f8.f64));
	// lfs f2,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f2.f64 = double(temp.f32);
	// fmsubs f7,f9,f9,f31
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f9.f64 - ctx.f31.f64));
	// fmuls f1,f6,f13
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// lfs f4,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f4.f64 = double(temp.f32);
	// fmr f31,f8
	ctx.f31.f64 = ctx.f8.f64;
	// lfs f29,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f29.f64 = double(temp.f32);
	// fmr f28,f2
	ctx.f28.f64 = ctx.f2.f64;
	// lfs f27,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f26.f64 = double(temp.f32);
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// lfs f25,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f25.f64 = double(temp.f32);
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// fmuls f24,f26,f11
	ctx.f24.f64 = double(float(ctx.f26.f64 * ctx.f11.f64));
	// lfs f0,7676(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f10,f4,f9,f10
	ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f9.f64 + ctx.f10.f64));
	// lfs f23,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f23.f64 = double(temp.f32);
	// fmsubs f5,f6,f9,f5
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f9.f64 - ctx.f5.f64));
	// lfs f21,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f3,f12,f9,f3
	ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f9.f64 + ctx.f3.f64));
	// lfs f20,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f22,f25,f7
	ctx.f22.f64 = double(float(ctx.f25.f64 * ctx.f7.f64));
	// addi r9,r31,12
	ctx.r9.s64 = ctx.r31.s64 + 12;
	// fmadds f1,f29,f9,f1
	ctx.f1.f64 = double(float(ctx.f29.f64 * ctx.f9.f64 + ctx.f1.f64));
	// fmuls f19,f27,f31
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f31.f64));
	// fmuls f17,f26,f28
	ctx.f17.f64 = double(float(ctx.f26.f64 * ctx.f28.f64));
	// fmuls f18,f25,f28
	ctx.f18.f64 = double(float(ctx.f25.f64 * ctx.f28.f64));
	// fmuls f16,f26,f7
	ctx.f16.f64 = double(float(ctx.f26.f64 * ctx.f7.f64));
	// fmsubs f24,f27,f28,f24
	ctx.f24.f64 = double(float(ctx.f27.f64 * ctx.f28.f64 - ctx.f24.f64));
	// fmadds f10,f6,f2,f10
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f2.f64 + ctx.f10.f64));
	// fnmsubs f6,f4,f2,f5
	ctx.f6.f64 = double(float(-(ctx.f4.f64 * ctx.f2.f64 - ctx.f5.f64)));
	// fmadds f5,f29,f2,f3
	ctx.f5.f64 = double(float(ctx.f29.f64 * ctx.f2.f64 + ctx.f3.f64));
	// fmuls f7,f27,f7
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f7.f64));
	// fmadds f3,f4,f8,f1
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f1.f64));
	// fmsubs f1,f25,f11,f19
	ctx.f1.f64 = double(float(ctx.f25.f64 * ctx.f11.f64 - ctx.f19.f64));
	// fmadds f25,f25,f31,f17
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f31.f64 + ctx.f17.f64));
	// fmsubs f26,f26,f31,f18
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f31.f64 - ctx.f18.f64));
	// fmuls f24,f24,f9
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f9.f64));
	// fnmsubs f10,f29,f8,f10
	ctx.f10.f64 = double(float(-(ctx.f29.f64 * ctx.f8.f64 - ctx.f10.f64)));
	// fnmsubs f8,f29,f13,f6
	ctx.f8.f64 = double(float(-(ctx.f29.f64 * ctx.f13.f64 - ctx.f6.f64)));
	// fnmsubs f6,f4,f13,f5
	ctx.f6.f64 = double(float(-(ctx.f4.f64 * ctx.f13.f64 - ctx.f5.f64)));
	// fnmsubs f5,f12,f2,f3
	ctx.f5.f64 = double(float(-(ctx.f12.f64 * ctx.f2.f64 - ctx.f3.f64)));
	// fmuls f4,f1,f9
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f9.f64));
	// fmadds f2,f27,f11,f25
	ctx.f2.f64 = double(float(ctx.f27.f64 * ctx.f11.f64 + ctx.f25.f64));
	// fmuls f3,f26,f9
	ctx.f3.f64 = double(float(ctx.f26.f64 * ctx.f9.f64));
	// fadds f1,f22,f24
	ctx.f1.f64 = double(float(ctx.f22.f64 + ctx.f24.f64));
	// fmuls f13,f10,f10
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f10.f64));
	// fmuls f12,f10,f6
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// fmuls f29,f8,f5
	ctx.f29.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// fadds f4,f16,f4
	ctx.f4.f64 = double(float(ctx.f16.f64 + ctx.f4.f64));
	// fmuls f31,f2,f31
	ctx.f31.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f28,f28,f2
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f2.f64));
	// fmuls f9,f5,f5
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f5.f64));
	// fmuls f2,f11,f2
	ctx.f2.f64 = double(float(ctx.f11.f64 * ctx.f2.f64));
	// fmuls f27,f13,f0
	ctx.f27.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fadds f13,f7,f3
	ctx.f13.f64 = double(float(ctx.f7.f64 + ctx.f3.f64));
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f7,f29,f0
	ctx.f7.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// fadds f3,f1,f31
	ctx.f3.f64 = double(float(ctx.f1.f64 + ctx.f31.f64));
	// fadds f1,f4,f28
	ctx.f1.f64 = double(float(ctx.f4.f64 + ctx.f28.f64));
	// fmuls f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fsubs f11,f30,f27
	ctx.f11.f64 = double(float(ctx.f30.f64 - ctx.f27.f64));
	// fadds f4,f13,f2
	ctx.f4.f64 = double(float(ctx.f13.f64 + ctx.f2.f64));
	// fsubs f2,f12,f7
	ctx.f2.f64 = double(float(ctx.f12.f64 - ctx.f7.f64));
	// stfs f2,84(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmuls f3,f3,f0
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmuls f2,f1,f0
	ctx.f2.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fsubs f1,f11,f9
	ctx.f1.f64 = double(float(ctx.f11.f64 - ctx.f9.f64));
	// stfs f1,80(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmuls f11,f4,f0
	ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fmuls f4,f5,f6
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmuls f1,f8,f10
	ctx.f1.f64 = double(float(ctx.f8.f64 * ctx.f10.f64));
	// fmuls f31,f6,f6
	ctx.f31.f64 = double(float(ctx.f6.f64 * ctx.f6.f64));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// fmuls f10,f5,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f10.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f8,f8,f6
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f6.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fadds f7,f7,f12
	ctx.f7.f64 = double(float(ctx.f7.f64 + ctx.f12.f64));
	// stfs f7,92(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fmuls f6,f4,f0
	ctx.f6.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fadds f12,f20,f11
	ctx.f12.f64 = double(float(ctx.f20.f64 + ctx.f11.f64));
	// fadds f13,f21,f2
	ctx.f13.f64 = double(float(ctx.f21.f64 + ctx.f2.f64));
	// fadds f11,f23,f3
	ctx.f11.f64 = double(float(ctx.f23.f64 + ctx.f3.f64));
	// fmuls f5,f1,f0
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fnmsubs f4,f31,f0,f30
	ctx.f4.f64 = double(float(-(ctx.f31.f64 * ctx.f0.f64 - ctx.f30.f64)));
	// fmuls f3,f10,f0
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f2,f8,f0
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fadds f1,f5,f6
	ctx.f1.f64 = double(float(ctx.f5.f64 + ctx.f6.f64));
	// stfs f1,88(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fsubs f10,f6,f5
	ctx.f10.f64 = double(float(ctx.f6.f64 - ctx.f5.f64));
	// stfs f10,104(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fsubs f0,f4,f9
	ctx.f0.f64 = double(float(ctx.f4.f64 - ctx.f9.f64));
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fsubs f7,f4,f27
	ctx.f7.f64 = double(float(ctx.f4.f64 - ctx.f27.f64));
	// stfs f7,112(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fsubs f9,f3,f2
	ctx.f9.f64 = double(float(ctx.f3.f64 - ctx.f2.f64));
	// stfs f9,100(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f8,f2,f3
	ctx.f8.f64 = double(float(ctx.f2.f64 + ctx.f3.f64));
	// stfs f8,108(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830BC230:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830bc230
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830BC230;
	// stfs f11,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f13,40(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f12,44(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
loc_830BC25C:
	// lwz r11,336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// addi r5,r31,12
	ctx.r5.s64 = ctx.r31.s64 + 12;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// lwz r4,48(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 48);
	// bl 0x830f9ae0
	ctx.lr = 0x830BC274;
	sub_830F9AE0(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r3,308(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 308);
	// bl 0x83073de8
	ctx.lr = 0x830BC284;
	sub_83073DE8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// addi r12,r1,-32
	ctx.r12.s64 = ctx.r1.s64 + -32;
	// bl 0x82cb6b04
	ctx.lr = 0x830BC294;
	__restfpr_16(ctx, base);
	// b 0x82cb113c
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830BC298"))) PPC_WEAK_FUNC(sub_830BC298);
PPC_FUNC_IMPL(__imp__sub_830BC298) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e8
	ctx.lr = 0x830BC2A0;
	__savegprlr_28(ctx, base);
	// addi r12,r1,-40
	ctx.r12.s64 = ctx.r1.s64 + -40;
	// bl 0x82cb6ac0
	ctx.lr = 0x830BC2A8;
	__savefpr_18(ctx, base);
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// lwz r30,268(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// lwz r3,308(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 308);
	// bl 0x83073d18
	ctx.lr = 0x830BC2C0;
	sub_83073D18(ctx, base);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830bc4d4
	if (ctx.cr6.eq) goto loc_830BC4D4;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830bc4d4
	if (ctx.cr6.eq) goto loc_830BC4D4;
	// lfs f12,252(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f12.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f11,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f12
	ctx.f10.f64 = ctx.f12.f64;
	// fmuls f9,f11,f12
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f7,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f6.f64 = double(temp.f32);
	// fmr f30,f7
	ctx.f30.f64 = ctx.f7.f64;
	// lfs f1,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f3,f6,f7
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// fmuls f2,f6,f12
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// lfs f8,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f8.f64 = double(temp.f32);
	// fmr f29,f1
	ctx.f29.f64 = ctx.f1.f64;
	// lfs f4,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f11,f7
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// lfs f31,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f31.f64 = double(temp.f32);
	// lfs f28,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f28.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f13,6380(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6380);
	ctx.f13.f64 = double(temp.f32);
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f27,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f27.f64 = double(temp.f32);
	// fmsubs f24,f8,f8,f13
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f8.f64 - ctx.f13.f64));
	// fmuls f25,f28,f10
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f10.f64));
	// lfs f26,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f9,f4,f8,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f9.f64));
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// fmuls f21,f28,f30
	ctx.f21.f64 = double(float(ctx.f28.f64 * ctx.f30.f64));
	// lfs f0,7676(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f3,f11,f8,f3
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f8.f64 + ctx.f3.f64));
	// lfs f13,6140(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 6140);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f2,f31,f8,f2
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f8.f64 + ctx.f2.f64));
	// lfs f23,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f19,f27,f29
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// lfs f22,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f22.f64 = double(temp.f32);
	// fmsubs f5,f6,f8,f5
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f8.f64 - ctx.f5.f64));
	// lfs f20,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f18,f26,f10
	ctx.f18.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// addi r9,r31,12
	ctx.r9.s64 = ctx.r31.s64 + 12;
	// fmadds f25,f26,f29,f25
	ctx.f25.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f25.f64));
	// fmadds f9,f6,f1,f9
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f1.f64 + ctx.f9.f64));
	// fmuls f6,f26,f24
	ctx.f6.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// fmadds f3,f31,f1,f3
	ctx.f3.f64 = double(float(ctx.f31.f64 * ctx.f1.f64 + ctx.f3.f64));
	// fmadds f2,f4,f7,f2
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f7.f64 + ctx.f2.f64));
	// fmsubs f21,f27,f10,f21
	ctx.f21.f64 = double(float(ctx.f27.f64 * ctx.f10.f64 - ctx.f21.f64));
	// fmsubs f26,f26,f30,f19
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f30.f64 - ctx.f19.f64));
	// fnmsubs f5,f4,f1,f5
	ctx.f5.f64 = double(float(-(ctx.f4.f64 * ctx.f1.f64 - ctx.f5.f64)));
	// fmuls f19,f28,f24
	ctx.f19.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// fmsubs f28,f28,f29,f18
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f29.f64 - ctx.f18.f64));
	// fmadds f25,f27,f30,f25
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f30.f64 + ctx.f25.f64));
	// fnmsubs f9,f31,f7,f9
	ctx.f9.f64 = double(float(-(ctx.f31.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// fmuls f7,f27,f24
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// fnmsubs f4,f4,f12,f3
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f12.f64 - ctx.f3.f64)));
	// fnmsubs f3,f11,f1,f2
	ctx.f3.f64 = double(float(-(ctx.f11.f64 * ctx.f1.f64 - ctx.f2.f64)));
	// fmuls f2,f21,f8
	ctx.f2.f64 = double(float(ctx.f21.f64 * ctx.f8.f64));
	// fmuls f1,f26,f8
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f8.f64));
	// fnmsubs f5,f31,f12,f5
	ctx.f5.f64 = double(float(-(ctx.f31.f64 * ctx.f12.f64 - ctx.f5.f64)));
	// fmuls f12,f28,f8
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f8.f64));
	// fmuls f31,f25,f30
	ctx.f31.f64 = double(float(ctx.f25.f64 * ctx.f30.f64));
	// fmuls f8,f9,f9
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f9.f64));
	// fmuls f11,f29,f25
	ctx.f11.f64 = double(float(ctx.f29.f64 * ctx.f25.f64));
	// fmuls f30,f9,f4
	ctx.f30.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// fmuls f29,f3,f3
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// fadds f6,f6,f2
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f2.f64));
	// fadds f2,f19,f1
	ctx.f2.f64 = double(float(ctx.f19.f64 + ctx.f1.f64));
	// fmuls f28,f5,f3
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fmuls f10,f10,f25
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f25.f64));
	// fadds f1,f7,f12
	ctx.f1.f64 = double(float(ctx.f7.f64 + ctx.f12.f64));
	// fmuls f12,f8,f0
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmuls f8,f30,f0
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmuls f7,f29,f0
	ctx.f7.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// fadds f6,f6,f11
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f11.f64));
	// fmuls f30,f28,f0
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// fadds f2,f2,f10
	ctx.f2.f64 = double(float(ctx.f2.f64 + ctx.f10.f64));
	// fsubs f11,f13,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// fsubs f10,f8,f30
	ctx.f10.f64 = double(float(ctx.f8.f64 - ctx.f30.f64));
	// stfs f10,84(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmuls f10,f3,f4
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f4.f64));
	// fsubs f11,f11,f7
	ctx.f11.f64 = double(float(ctx.f11.f64 - ctx.f7.f64));
	// stfs f11,80(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmuls f11,f5,f9
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// fmuls f9,f3,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f9.f64));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// fmuls f5,f5,f4
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f29,f4,f4
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f4.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fadds f4,f1,f31
	ctx.f4.f64 = double(float(ctx.f1.f64 + ctx.f31.f64));
	// fmuls f3,f2,f0
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fadds f1,f30,f8
	ctx.f1.f64 = double(float(ctx.f30.f64 + ctx.f8.f64));
	// stfs f1,92(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fmuls f8,f11,f0
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f2,f6,f0
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f1,f9,f0
	ctx.f1.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f11,f5,f0
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fnmsubs f6,f29,f0,f13
	ctx.f6.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f9,f4,f0
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fadds f13,f20,f3
	ctx.f13.f64 = double(float(ctx.f20.f64 + ctx.f3.f64));
	// fadds f0,f22,f2
	ctx.f0.f64 = double(float(ctx.f22.f64 + ctx.f2.f64));
	// fadds f5,f8,f10
	ctx.f5.f64 = double(float(ctx.f8.f64 + ctx.f10.f64));
	// stfs f5,88(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fsubs f3,f10,f8
	ctx.f3.f64 = double(float(ctx.f10.f64 - ctx.f8.f64));
	// stfs f3,104(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fsubs f2,f1,f11
	ctx.f2.f64 = double(float(ctx.f1.f64 - ctx.f11.f64));
	// stfs f2,100(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fsubs f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 - ctx.f12.f64));
	// stfs f12,112(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fsubs f4,f6,f7
	ctx.f4.f64 = double(float(ctx.f6.f64 - ctx.f7.f64));
	// stfs f4,96(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f1,f11,f1
	ctx.f1.f64 = double(float(ctx.f11.f64 + ctx.f1.f64));
	// stfs f1,108(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fadds f12,f23,f9
	ctx.f12.f64 = double(float(ctx.f23.f64 + ctx.f9.f64));
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830BC4A8:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830bc4a8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830BC4A8;
	// stfs f12,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f0,40(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f13,44(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
loc_830BC4D4:
	// lwz r11,336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// addi r5,r31,12
	ctx.r5.s64 = ctx.r31.s64 + 12;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// lwz r4,48(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 48);
	// bl 0x830f2ee8
	ctx.lr = 0x830BC4EC;
	sub_830F2EE8(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r3,308(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 308);
	// bl 0x83073de8
	ctx.lr = 0x830BC4FC;
	sub_83073DE8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// addi r12,r1,-40
	ctx.r12.s64 = ctx.r1.s64 + -40;
	// bl 0x82cb6b0c
	ctx.lr = 0x830BC50C;
	__restfpr_18(ctx, base);
	// b 0x82cb1138
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830BC510"))) PPC_WEAK_FUNC(sub_830BC510);
PPC_FUNC_IMPL(__imp__sub_830BC510) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10d4
	ctx.lr = 0x830BC518;
	__savegprlr_23(ctx, base);
	// addi r12,r1,-80
	ctx.r12.s64 = ctx.r1.s64 + -80;
	// bl 0x82cb6ab0
	ctx.lr = 0x830BC520;
	__savefpr_14(ctx, base);
	// stwu r1,-432(r1)
	ea = -432 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// clrlwi r11,r8,24
	ctx.r11.u64 = ctx.r8.u32 & 0xFF;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// lfs f0,6380(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6380);
	ctx.f0.f64 = double(temp.f32);
	// mr r24,r5
	ctx.r24.u64 = ctx.r5.u64;
	// lfs f19,6048(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 6048);
	ctx.f19.f64 = double(temp.f32);
	// mr r23,r6
	ctx.r23.u64 = ctx.r6.u64;
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// clrlwi r11,r9,24
	ctx.r11.u64 = ctx.r9.u32 & 0xFF;
	// stfs f19,108(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// beq cr6,0x830bca40
	if (ctx.cr6.eq) goto loc_830BCA40;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// addi r10,r11,6140
	ctx.r10.s64 = ctx.r11.s64 + 6140;
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lfs f18,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f18.f64 = double(temp.f32);
	// beq cr6,0x830bc7cc
	if (ctx.cr6.eq) goto loc_830BC7CC;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830bc770
	if (ctx.cr6.eq) goto loc_830BC770;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830bc770
	if (ctx.cr6.eq) goto loc_830BC770;
	// lfs f13,252(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f13.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f12,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f12.f64 = double(temp.f32);
	// fmr f11,f13
	ctx.f11.f64 = ctx.f13.f64;
	// fmuls f10,f12,f13
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f8,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f8.f64 = double(temp.f32);
	// lfs f6,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f5,f12,f8
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// lfs f2,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f3,f6,f8
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f8.f64));
	// lfs f9,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f9.f64 = double(temp.f32);
	// fmr f31,f8
	ctx.f31.f64 = ctx.f8.f64;
	// lfs f4,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f1,f6,f13
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// fmr f29,f2
	ctx.f29.f64 = ctx.f2.f64;
	// lfs f28,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f27.f64 = double(temp.f32);
	// fmsubs f7,f9,f9,f0
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f9.f64 - ctx.f0.f64));
	// lfs f30,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f30.f64 = double(temp.f32);
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// lfs f26,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f26.f64 = double(temp.f32);
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// fmuls f23,f11,f28
	ctx.f23.f64 = double(float(ctx.f11.f64 * ctx.f28.f64));
	// lfs f0,7676(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f10,f4,f9,f10
	ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f9.f64 + ctx.f10.f64));
	// lfs f24,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f24.f64 = double(temp.f32);
	// fmsubs f5,f6,f9,f5
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f9.f64 - ctx.f5.f64));
	// lfs f22,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f3,f12,f9,f3
	ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f9.f64 + ctx.f3.f64));
	// lfs f21,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f25,f11,f27
	ctx.f25.f64 = double(float(ctx.f11.f64 * ctx.f27.f64));
	// addi r9,r31,12
	ctx.r9.s64 = ctx.r31.s64 + 12;
	// fmadds f1,f30,f9,f1
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f9.f64 + ctx.f1.f64));
	// fmuls f20,f31,f28
	ctx.f20.f64 = double(float(ctx.f31.f64 * ctx.f28.f64));
	// fmuls f17,f29,f26
	ctx.f17.f64 = double(float(ctx.f29.f64 * ctx.f26.f64));
	// fmuls f15,f7,f27
	ctx.f15.f64 = double(float(ctx.f7.f64 * ctx.f27.f64));
	// fmuls f16,f7,f26
	ctx.f16.f64 = double(float(ctx.f7.f64 * ctx.f26.f64));
	// fmuls f7,f7,f28
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f28.f64));
	// fmadds f10,f6,f2,f10
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f2.f64 + ctx.f10.f64));
	// fmadds f6,f29,f27,f23
	ctx.f6.f64 = double(float(ctx.f29.f64 * ctx.f27.f64 + ctx.f23.f64));
	// fnmsubs f5,f4,f2,f5
	ctx.f5.f64 = double(float(-(ctx.f4.f64 * ctx.f2.f64 - ctx.f5.f64)));
	// fmadds f3,f30,f2,f3
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f2.f64 + ctx.f3.f64));
	// fmsubs f25,f29,f28,f25
	ctx.f25.f64 = double(float(ctx.f29.f64 * ctx.f28.f64 - ctx.f25.f64));
	// fmadds f1,f4,f8,f1
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f1.f64));
	// fmsubs f23,f11,f26,f20
	ctx.f23.f64 = double(float(ctx.f11.f64 * ctx.f26.f64 - ctx.f20.f64));
	// fmsubs f27,f31,f27,f17
	ctx.f27.f64 = double(float(ctx.f31.f64 * ctx.f27.f64 - ctx.f17.f64));
	// fnmsubs f10,f30,f8,f10
	ctx.f10.f64 = double(float(-(ctx.f30.f64 * ctx.f8.f64 - ctx.f10.f64)));
	// fmadds f8,f31,f26,f6
	ctx.f8.f64 = double(float(ctx.f31.f64 * ctx.f26.f64 + ctx.f6.f64));
	// fnmsubs f6,f30,f13,f5
	ctx.f6.f64 = double(float(-(ctx.f30.f64 * ctx.f13.f64 - ctx.f5.f64)));
	// fnmsubs f5,f4,f13,f3
	ctx.f5.f64 = double(float(-(ctx.f4.f64 * ctx.f13.f64 - ctx.f3.f64)));
	// fmuls f28,f25,f9
	ctx.f28.f64 = double(float(ctx.f25.f64 * ctx.f9.f64));
	// fnmsubs f4,f12,f2,f1
	ctx.f4.f64 = double(float(-(ctx.f12.f64 * ctx.f2.f64 - ctx.f1.f64)));
	// fmuls f3,f23,f9
	ctx.f3.f64 = double(float(ctx.f23.f64 * ctx.f9.f64));
	// fmuls f2,f27,f9
	ctx.f2.f64 = double(float(ctx.f27.f64 * ctx.f9.f64));
	// fmuls f13,f10,f10
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f10.f64));
	// fmuls f12,f8,f31
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fmuls f9,f29,f8
	ctx.f9.f64 = double(float(ctx.f29.f64 * ctx.f8.f64));
	// fmuls f8,f11,f8
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f8.f64));
	// fadds f1,f16,f28
	ctx.f1.f64 = double(float(ctx.f16.f64 + ctx.f28.f64));
	// fmuls f31,f10,f5
	ctx.f31.f64 = double(float(ctx.f10.f64 * ctx.f5.f64));
	// fadds f3,f15,f3
	ctx.f3.f64 = double(float(ctx.f15.f64 + ctx.f3.f64));
	// fadds f2,f7,f2
	ctx.f2.f64 = double(float(ctx.f7.f64 + ctx.f2.f64));
	// fmuls f29,f6,f4
	ctx.f29.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// fmuls f30,f4,f4
	ctx.f30.f64 = double(float(ctx.f4.f64 * ctx.f4.f64));
	// fmuls f7,f13,f0
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fadds f1,f1,f12
	ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f12.f64));
	// fmuls f13,f31,f0
	ctx.f13.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fadds f11,f3,f9
	ctx.f11.f64 = double(float(ctx.f3.f64 + ctx.f9.f64));
	// fadds f9,f2,f8
	ctx.f9.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// fmuls f12,f29,f0
	ctx.f12.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// fmuls f31,f30,f0
	ctx.f31.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fsubs f8,f18,f7
	ctx.f8.f64 = double(float(ctx.f18.f64 - ctx.f7.f64));
	// fmuls f3,f1,f0
	ctx.f3.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f1,f11,f0
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f11,f9,f0
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fsubs f2,f13,f12
	ctx.f2.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// stfs f2,116(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fmuls f2,f6,f10
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f10.f64));
	// fsubs f9,f8,f31
	ctx.f9.f64 = double(float(ctx.f8.f64 - ctx.f31.f64));
	// stfs f9,112(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fmuls f8,f4,f5
	ctx.f8.f64 = double(float(ctx.f4.f64 * ctx.f5.f64));
	// fmuls f9,f5,f5
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f5.f64));
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// fmuls f4,f4,f10
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f10.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f10,f6,f5
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f5.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fadds f6,f12,f13
	ctx.f6.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// stfs f6,124(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// fadds f12,f21,f11
	ctx.f12.f64 = double(float(ctx.f21.f64 + ctx.f11.f64));
	// fadds f11,f24,f3
	ctx.f11.f64 = double(float(ctx.f24.f64 + ctx.f3.f64));
	// fmuls f3,f2,f0
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fadds f13,f22,f1
	ctx.f13.f64 = double(float(ctx.f22.f64 + ctx.f1.f64));
	// fmuls f5,f8,f0
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fnmsubs f2,f9,f0,f18
	ctx.f2.f64 = double(float(-(ctx.f9.f64 * ctx.f0.f64 - ctx.f18.f64)));
	// fmuls f1,f4,f0
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fmuls f0,f10,f0
	ctx.f0.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fadds f10,f3,f5
	ctx.f10.f64 = double(float(ctx.f3.f64 + ctx.f5.f64));
	// stfs f10,120(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// fsubs f8,f5,f3
	ctx.f8.f64 = double(float(ctx.f5.f64 - ctx.f3.f64));
	// stfs f8,136(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// fsubs f9,f2,f31
	ctx.f9.f64 = double(float(ctx.f2.f64 - ctx.f31.f64));
	// stfs f9,128(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// fsubs f4,f2,f7
	ctx.f4.f64 = double(float(ctx.f2.f64 - ctx.f7.f64));
	// stfs f4,144(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// fsubs f6,f1,f0
	ctx.f6.f64 = double(float(ctx.f1.f64 - ctx.f0.f64));
	// stfs f6,132(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 132, temp.u32);
	// fadds f5,f0,f1
	ctx.f5.f64 = double(float(ctx.f0.f64 + ctx.f1.f64));
	// stfs f5,140(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 140, temp.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830BC744:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830bc744
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830BC744;
	// stfs f11,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f13,40(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f12,44(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
loc_830BC770:
	// addi r8,r31,12
	ctx.r8.s64 = ctx.r31.s64 + 12;
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// mr r11,r8
	ctx.r11.u64 = ctx.r8.u64;
	// li r9,9
	ctx.r9.s64 = 9;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_830BC784:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830bc784
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830BC784;
	// lfs f29,36(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 36);
	ctx.f29.f64 = double(temp.f32);
	// lfs f20,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f20.f64 = double(temp.f32);
	// lfs f30,40(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 40);
	ctx.f30.f64 = double(temp.f32);
	// lfs f21,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f21.f64 = double(temp.f32);
	// lfs f22,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f22.f64 = double(temp.f32);
	// lfs f23,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f23.f64 = double(temp.f32);
	// lfs f31,44(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 44);
	ctx.f31.f64 = double(temp.f32);
	// lfs f24,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f24.f64 = double(temp.f32);
	// lfs f25,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f25.f64 = double(temp.f32);
	// lfs f26,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f26.f64 = double(temp.f32);
	// lfs f27,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f27.f64 = double(temp.f32);
	// lfs f28,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f28.f64 = double(temp.f32);
	// b 0x830bccec
	goto loc_830BCCEC;
loc_830BC7CC:
	// fmr f23,f19
	ctx.fpscr.disableFlushMode();
	ctx.f23.f64 = ctx.f19.f64;
	// stfs f23,180(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmr f22,f19
	ctx.f22.f64 = ctx.f19.f64;
	// stfs f22,184(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// fmr f21,f19
	ctx.f21.f64 = ctx.f19.f64;
	// stfs f21,188(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 188, temp.u32);
	// fmr f24,f18
	ctx.f24.f64 = ctx.f18.f64;
	// stfs f18,96(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fmr f20,f18
	ctx.f20.f64 = ctx.f18.f64;
	// stfs f24,176(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// stfs f20,192(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 192, temp.u32);
	// fmr f27,f19
	ctx.f27.f64 = ctx.f19.f64;
	// fmr f26,f19
	ctx.f26.f64 = ctx.f19.f64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// fmr f25,f19
	ctx.f25.f64 = ctx.f19.f64;
	// fmr f28,f18
	ctx.f28.f64 = ctx.f18.f64;
	// beq cr6,0x830bca30
	if (ctx.cr6.eq) goto loc_830BCA30;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830bca30
	if (ctx.cr6.eq) goto loc_830BCA30;
	// lfs f13,252(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f13.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f10,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f10.f64 = double(temp.f32);
	// fmr f11,f13
	ctx.f11.f64 = ctx.f13.f64;
	// lfs f12,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f7,f10,f13
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// lfs f8,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f5,f10,f12
	ctx.f5.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
	// lfs f6,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f4,f8,f12
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f12.f64));
	// fmuls f2,f8,f13
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// lfs f3,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f3.f64 = double(temp.f32);
	// fmr f31,f6
	ctx.f31.f64 = ctx.f6.f64;
	// lfs f1,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f1.f64 = double(temp.f32);
	// fmr f9,f12
	ctx.f9.f64 = ctx.f12.f64;
	// lfs f30,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f30.f64 = double(temp.f32);
	// lfs f23,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f23.f64 = double(temp.f32);
	// fmsubs f24,f3,f3,f0
	ctx.f24.f64 = double(float(ctx.f3.f64 * ctx.f3.f64 - ctx.f0.f64));
	// lfs f29,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f29.f64 = double(temp.f32);
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// lfs f22,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f22.f64 = double(temp.f32);
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// fmuls f21,f11,f23
	ctx.f21.f64 = double(float(ctx.f11.f64 * ctx.f23.f64));
	// lfs f0,7676(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f7,f1,f3,f7
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f3.f64 + ctx.f7.f64));
	// stfs f0,88(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fmsubs f5,f8,f3,f5
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f3.f64 - ctx.f5.f64));
	// stfd f27,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f27.u64);
	// fmadds f4,f10,f3,f4
	ctx.f4.f64 = double(float(ctx.f10.f64 * ctx.f3.f64 + ctx.f4.f64));
	// stfd f28,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f28.u64);
	// fmadds f2,f30,f3,f2
	ctx.f2.f64 = double(float(ctx.f30.f64 * ctx.f3.f64 + ctx.f2.f64));
	// lfs f19,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f16,f31,f23
	ctx.f16.f64 = double(float(ctx.f31.f64 * ctx.f23.f64));
	// lfs f17,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f20,f9,f29
	ctx.f20.f64 = double(float(ctx.f9.f64 * ctx.f29.f64));
	// lfs f15,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f14,f31,f22
	ctx.f14.f64 = double(float(ctx.f31.f64 * ctx.f22.f64));
	// addi r9,r31,12
	ctx.r9.s64 = ctx.r31.s64 + 12;
	// fmuls f27,f24,f23
	ctx.f27.f64 = double(float(ctx.f24.f64 * ctx.f23.f64));
	// fmuls f28,f24,f22
	ctx.f28.f64 = double(float(ctx.f24.f64 * ctx.f22.f64));
	// fmsubs f21,f31,f29,f21
	ctx.f21.f64 = double(float(ctx.f31.f64 * ctx.f29.f64 - ctx.f21.f64));
	// fmadds f8,f8,f6,f7
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f6.f64 + ctx.f7.f64));
	// fnmsubs f7,f1,f6,f5
	ctx.f7.f64 = double(float(-(ctx.f1.f64 * ctx.f6.f64 - ctx.f5.f64)));
	// fmadds f5,f30,f6,f4
	ctx.f5.f64 = double(float(ctx.f30.f64 * ctx.f6.f64 + ctx.f4.f64));
	// fmadds f4,f1,f12,f2
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f12.f64 + ctx.f2.f64));
	// fmadds f2,f9,f22,f16
	ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f22.f64 + ctx.f16.f64));
	// fmsubs f20,f11,f22,f20
	ctx.f20.f64 = double(float(ctx.f11.f64 * ctx.f22.f64 - ctx.f20.f64));
	// fmsubs f23,f9,f23,f14
	ctx.f23.f64 = double(float(ctx.f9.f64 * ctx.f23.f64 - ctx.f14.f64));
	// fmuls f24,f24,f29
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f29.f64));
	// fmuls f22,f21,f3
	ctx.f22.f64 = double(float(ctx.f21.f64 * ctx.f3.f64));
	// fnmsubs f12,f30,f12,f8
	ctx.f12.f64 = double(float(-(ctx.f30.f64 * ctx.f12.f64 - ctx.f8.f64)));
	// fnmsubs f8,f30,f13,f7
	ctx.f8.f64 = double(float(-(ctx.f30.f64 * ctx.f13.f64 - ctx.f7.f64)));
	// fnmsubs f7,f1,f13,f5
	ctx.f7.f64 = double(float(-(ctx.f1.f64 * ctx.f13.f64 - ctx.f5.f64)));
	// fnmsubs f6,f10,f6,f4
	ctx.f6.f64 = double(float(-(ctx.f10.f64 * ctx.f6.f64 - ctx.f4.f64)));
	// fmadds f5,f11,f29,f2
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f29.f64 + ctx.f2.f64));
	// fmuls f21,f3,f20
	ctx.f21.f64 = double(float(ctx.f3.f64 * ctx.f20.f64));
	// fmuls f4,f3,f23
	ctx.f4.f64 = double(float(ctx.f3.f64 * ctx.f23.f64));
	// fadds f3,f28,f22
	ctx.f3.f64 = double(float(ctx.f28.f64 + ctx.f22.f64));
	// fmuls f1,f12,f12
	ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f12.f64));
	// fmuls f13,f7,f12
	ctx.f13.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// fmuls f10,f8,f6
	ctx.f10.f64 = double(float(ctx.f8.f64 * ctx.f6.f64));
	// fmuls f9,f5,f9
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// fmuls f31,f5,f31
	ctx.f31.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// fadds f2,f27,f21
	ctx.f2.f64 = double(float(ctx.f27.f64 + ctx.f21.f64));
	// fadds f4,f24,f4
	ctx.f4.f64 = double(float(ctx.f24.f64 + ctx.f4.f64));
	// fmuls f5,f5,f11
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// fmuls f30,f6,f6
	ctx.f30.f64 = double(float(ctx.f6.f64 * ctx.f6.f64));
	// fmuls f1,f1,f0
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fmuls f11,f10,f0
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fadds f10,f3,f9
	ctx.f10.f64 = double(float(ctx.f3.f64 + ctx.f9.f64));
	// fadds f9,f2,f31
	ctx.f9.f64 = double(float(ctx.f2.f64 + ctx.f31.f64));
	// fmuls f31,f8,f12
	ctx.f31.f64 = double(float(ctx.f8.f64 * ctx.f12.f64));
	// fadds f5,f4,f5
	ctx.f5.f64 = double(float(ctx.f4.f64 + ctx.f5.f64));
	// fmuls f3,f30,f0
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fsubs f2,f18,f1
	ctx.f2.f64 = double(float(ctx.f18.f64 - ctx.f1.f64));
	// fsubs f4,f13,f11
	ctx.f4.f64 = double(float(ctx.f13.f64 - ctx.f11.f64));
	// stfs f4,116(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f4,f7,f6
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f6.f64));
	// fmuls f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f30,f7,f7
	ctx.f30.f64 = double(float(ctx.f7.f64 * ctx.f7.f64));
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// fmuls f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f8,f7,f8
	ctx.f8.f64 = double(float(ctx.f7.f64 * ctx.f8.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fmuls f7,f5,f0
	ctx.f7.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fadds f5,f11,f13
	ctx.f5.f64 = double(float(ctx.f11.f64 + ctx.f13.f64));
	// stfs f5,124(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// fsubs f6,f2,f3
	ctx.f6.f64 = double(float(ctx.f2.f64 - ctx.f3.f64));
	// stfs f6,112(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fadds f11,f10,f19
	ctx.f11.f64 = double(float(ctx.f10.f64 + ctx.f19.f64));
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fmuls f2,f31,f0
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fadds f13,f17,f9
	ctx.f13.f64 = double(float(ctx.f17.f64 + ctx.f9.f64));
	// fnmsubs f10,f30,f0,f18
	ctx.f10.f64 = double(float(-(ctx.f30.f64 * ctx.f0.f64 - ctx.f18.f64)));
	// fmuls f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fadds f12,f15,f7
	ctx.f12.f64 = double(float(ctx.f15.f64 + ctx.f7.f64));
	// fadds f7,f2,f4
	ctx.f7.f64 = double(float(ctx.f2.f64 + ctx.f4.f64));
	// stfs f7,120(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// fsubs f5,f4,f2
	ctx.f5.f64 = double(float(ctx.f4.f64 - ctx.f2.f64));
	// stfs f5,136(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// fsubs f6,f10,f3
	ctx.f6.f64 = double(float(ctx.f10.f64 - ctx.f3.f64));
	// stfs f6,128(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// fsubs f2,f10,f1
	ctx.f2.f64 = double(float(ctx.f10.f64 - ctx.f1.f64));
	// stfs f2,144(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// fsubs f4,f9,f8
	ctx.f4.f64 = double(float(ctx.f9.f64 - ctx.f8.f64));
	// stfs f4,132(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 132, temp.u32);
	// fadds f3,f8,f9
	ctx.f3.f64 = double(float(ctx.f8.f64 + ctx.f9.f64));
	// stfs f3,140(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 140, temp.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// fmr f28,f18
	ctx.f28.f64 = ctx.f18.f64;
	// fmr f27,f25
	ctx.f27.f64 = ctx.f25.f64;
loc_830BC9EC:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830bc9ec
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830BC9EC;
	// stfs f13,40(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f11,36(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f12,44(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lfs f20,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f20.f64 = double(temp.f32);
	// lfs f21,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f21.f64 = double(temp.f32);
	// lfs f22,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f22.f64 = double(temp.f32);
	// lfs f23,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f23.f64 = double(temp.f32);
	// lfs f24,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f24.f64 = double(temp.f32);
	// lfs f19,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f19.f64 = double(temp.f32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
loc_830BCA30:
	// lfs f29,48(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	ctx.f29.f64 = double(temp.f32);
	// lfs f30,52(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 52);
	ctx.f30.f64 = double(temp.f32);
	// lfs f31,56(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 56);
	ctx.f31.f64 = double(temp.f32);
	// b 0x830bccec
	goto loc_830BCCEC;
loc_830BCA40:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// addi r10,r11,6140
	ctx.r10.s64 = ctx.r11.s64 + 6140;
	// beq cr6,0x830bccb8
	if (ctx.cr6.eq) goto loc_830BCCB8;
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lfs f18,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f18.f64 = double(temp.f32);
	// stfs f19,204(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// stfs f19,200(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 200, temp.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stfs f19,196(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 196, temp.u32);
	// beq cr6,0x830bcc60
	if (ctx.cr6.eq) goto loc_830BCC60;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830bcc60
	if (ctx.cr6.eq) goto loc_830BCC60;
	// lfs f13,252(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f13.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f12,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f12.f64 = double(temp.f32);
	// fmr f11,f13
	ctx.f11.f64 = ctx.f13.f64;
	// fmuls f10,f12,f13
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f8,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f8.f64 = double(temp.f32);
	// lfs f6,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f5,f12,f8
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// fmuls f3,f6,f8
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f8.f64));
	// lfs f2,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f6,f13
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// lfs f9,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f9.f64 = double(temp.f32);
	// fmr f31,f8
	ctx.f31.f64 = ctx.f8.f64;
	// lfs f4,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f4.f64 = double(temp.f32);
	// fmr f29,f2
	ctx.f29.f64 = ctx.f2.f64;
	// lfs f30,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f30.f64 = double(temp.f32);
	// lfs f28,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f28.f64 = double(temp.f32);
	// fmsubs f7,f9,f9,f0
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f9.f64 - ctx.f0.f64));
	// lfs f27,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f27.f64 = double(temp.f32);
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// lfs f26,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f26.f64 = double(temp.f32);
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// fmuls f25,f27,f11
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f11.f64));
	// lfs f0,7676(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f10,f4,f9,f10
	ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f9.f64 + ctx.f10.f64));
	// lfs f24,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f24.f64 = double(temp.f32);
	// fmsubs f5,f6,f9,f5
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f9.f64 - ctx.f5.f64));
	// lfs f22,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f3,f12,f9,f3
	ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f9.f64 + ctx.f3.f64));
	// lfs f21,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f1,f30,f9,f1
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f9.f64 + ctx.f1.f64));
	// addi r9,r31,12
	ctx.r9.s64 = ctx.r31.s64 + 12;
	// fmuls f20,f31,f28
	ctx.f20.f64 = double(float(ctx.f31.f64 * ctx.f28.f64));
	// fmuls f16,f27,f29
	ctx.f16.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// fmuls f17,f26,f29
	ctx.f17.f64 = double(float(ctx.f26.f64 * ctx.f29.f64));
	// fmuls f23,f7,f26
	ctx.f23.f64 = double(float(ctx.f7.f64 * ctx.f26.f64));
	// fmuls f15,f7,f27
	ctx.f15.f64 = double(float(ctx.f7.f64 * ctx.f27.f64));
	// fmsubs f25,f28,f29,f25
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f29.f64 - ctx.f25.f64));
	// fmadds f10,f6,f2,f10
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f2.f64 + ctx.f10.f64));
	// fnmsubs f6,f4,f2,f5
	ctx.f6.f64 = double(float(-(ctx.f4.f64 * ctx.f2.f64 - ctx.f5.f64)));
	// fmadds f5,f30,f2,f3
	ctx.f5.f64 = double(float(ctx.f30.f64 * ctx.f2.f64 + ctx.f3.f64));
	// fmadds f3,f4,f8,f1
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f1.f64));
	// fmsubs f1,f26,f11,f20
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f11.f64 - ctx.f20.f64));
	// fmadds f26,f31,f26,f16
	ctx.f26.f64 = double(float(ctx.f31.f64 * ctx.f26.f64 + ctx.f16.f64));
	// fmsubs f27,f31,f27,f17
	ctx.f27.f64 = double(float(ctx.f31.f64 * ctx.f27.f64 - ctx.f17.f64));
	// fmuls f7,f7,f28
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f28.f64));
	// fmuls f25,f25,f9
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f9.f64));
	// fnmsubs f10,f30,f8,f10
	ctx.f10.f64 = double(float(-(ctx.f30.f64 * ctx.f8.f64 - ctx.f10.f64)));
	// fnmsubs f8,f30,f13,f6
	ctx.f8.f64 = double(float(-(ctx.f30.f64 * ctx.f13.f64 - ctx.f6.f64)));
	// fnmsubs f6,f4,f13,f5
	ctx.f6.f64 = double(float(-(ctx.f4.f64 * ctx.f13.f64 - ctx.f5.f64)));
	// fnmsubs f5,f12,f2,f3
	ctx.f5.f64 = double(float(-(ctx.f12.f64 * ctx.f2.f64 - ctx.f3.f64)));
	// fmuls f4,f9,f1
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f1.f64));
	// fmadds f2,f28,f11,f26
	ctx.f2.f64 = double(float(ctx.f28.f64 * ctx.f11.f64 + ctx.f26.f64));
	// fmuls f3,f9,f27
	ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f27.f64));
	// fadds f1,f23,f25
	ctx.f1.f64 = double(float(ctx.f23.f64 + ctx.f25.f64));
	// fmuls f13,f10,f10
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f10.f64));
	// fmuls f12,f6,f10
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f10.f64));
	// fmuls f30,f8,f5
	ctx.f30.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// fadds f4,f15,f4
	ctx.f4.f64 = double(float(ctx.f15.f64 + ctx.f4.f64));
	// fmuls f31,f2,f31
	ctx.f31.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f29,f2,f29
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f29.f64));
	// fmuls f9,f5,f5
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f5.f64));
	// fmuls f2,f2,f11
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// fmuls f28,f13,f0
	ctx.f28.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fadds f13,f7,f3
	ctx.f13.f64 = double(float(ctx.f7.f64 + ctx.f3.f64));
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f7,f30,f0
	ctx.f7.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fadds f3,f1,f31
	ctx.f3.f64 = double(float(ctx.f1.f64 + ctx.f31.f64));
	// fadds f1,f4,f29
	ctx.f1.f64 = double(float(ctx.f4.f64 + ctx.f29.f64));
	// fmuls f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fsubs f11,f18,f28
	ctx.f11.f64 = double(float(ctx.f18.f64 - ctx.f28.f64));
	// fadds f4,f13,f2
	ctx.f4.f64 = double(float(ctx.f13.f64 + ctx.f2.f64));
	// fsubs f2,f12,f7
	ctx.f2.f64 = double(float(ctx.f12.f64 - ctx.f7.f64));
	// stfs f2,116(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fmuls f3,f3,f0
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmuls f2,f1,f0
	ctx.f2.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fsubs f1,f11,f9
	ctx.f1.f64 = double(float(ctx.f11.f64 - ctx.f9.f64));
	// stfs f1,112(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fmuls f11,f4,f0
	ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fmuls f4,f6,f5
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f5.f64));
	// fmuls f1,f8,f10
	ctx.f1.f64 = double(float(ctx.f8.f64 * ctx.f10.f64));
	// fmuls f31,f6,f6
	ctx.f31.f64 = double(float(ctx.f6.f64 * ctx.f6.f64));
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// fmuls f10,f5,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f10.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f8,f6,f8
	ctx.f8.f64 = double(float(ctx.f6.f64 * ctx.f8.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fadds f7,f7,f12
	ctx.f7.f64 = double(float(ctx.f7.f64 + ctx.f12.f64));
	// stfs f7,124(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// fmuls f6,f4,f0
	ctx.f6.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fadds f12,f21,f11
	ctx.f12.f64 = double(float(ctx.f21.f64 + ctx.f11.f64));
	// fadds f13,f22,f2
	ctx.f13.f64 = double(float(ctx.f22.f64 + ctx.f2.f64));
	// fadds f11,f3,f24
	ctx.f11.f64 = double(float(ctx.f3.f64 + ctx.f24.f64));
	// fmuls f5,f1,f0
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fnmsubs f4,f31,f0,f18
	ctx.f4.f64 = double(float(-(ctx.f31.f64 * ctx.f0.f64 - ctx.f18.f64)));
	// fmuls f3,f10,f0
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f2,f8,f0
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fadds f1,f5,f6
	ctx.f1.f64 = double(float(ctx.f5.f64 + ctx.f6.f64));
	// stfs f1,120(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// fsubs f10,f6,f5
	ctx.f10.f64 = double(float(ctx.f6.f64 - ctx.f5.f64));
	// stfs f10,136(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// fsubs f0,f4,f9
	ctx.f0.f64 = double(float(ctx.f4.f64 - ctx.f9.f64));
	// stfs f0,128(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// fsubs f7,f4,f28
	ctx.f7.f64 = double(float(ctx.f4.f64 - ctx.f28.f64));
	// stfs f7,144(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// fsubs f9,f3,f2
	ctx.f9.f64 = double(float(ctx.f3.f64 - ctx.f2.f64));
	// stfs f9,132(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 132, temp.u32);
	// fadds f8,f2,f3
	ctx.f8.f64 = double(float(ctx.f2.f64 + ctx.f3.f64));
	// stfs f8,140(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 140, temp.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830BCC34:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830bcc34
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830BCC34;
	// stfs f12,44(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// stfs f13,40(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f11,36(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
loc_830BCC60:
	// addi r11,r31,12
	ctx.r11.s64 = ctx.r31.s64 + 12;
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// li r9,9
	ctx.r9.s64 = 9;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_830BCC70:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830bcc70
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830BCC70;
	// lfs f31,204(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f29.f64 = double(temp.f32);
	// lfs f20,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f20.f64 = double(temp.f32);
	// lfs f21,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f21.f64 = double(temp.f32);
	// lfs f22,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f22.f64 = double(temp.f32);
	// lfs f23,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f23.f64 = double(temp.f32);
	// lfs f24,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f24.f64 = double(temp.f32);
	// lfs f25,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f25.f64 = double(temp.f32);
	// lfs f26,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f26.f64 = double(temp.f32);
	// lfs f27,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f27.f64 = double(temp.f32);
	// lfs f28,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f28.f64 = double(temp.f32);
	// b 0x830bccec
	goto loc_830BCCEC;
loc_830BCCB8:
	// lfs f18,6140(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6140);
	ctx.f18.f64 = double(temp.f32);
	// fmr f27,f19
	ctx.f27.f64 = ctx.f19.f64;
	// fmr f26,f19
	ctx.f26.f64 = ctx.f19.f64;
	// fmr f25,f19
	ctx.f25.f64 = ctx.f19.f64;
	// fmr f23,f19
	ctx.f23.f64 = ctx.f19.f64;
	// fmr f22,f19
	ctx.f22.f64 = ctx.f19.f64;
	// fmr f21,f19
	ctx.f21.f64 = ctx.f19.f64;
	// fmr f28,f18
	ctx.f28.f64 = ctx.f18.f64;
	// fmr f24,f18
	ctx.f24.f64 = ctx.f18.f64;
	// fmr f31,f19
	ctx.f31.f64 = ctx.f19.f64;
	// fmr f30,f19
	ctx.f30.f64 = ctx.f19.f64;
	// fmr f20,f18
	ctx.f20.f64 = ctx.f18.f64;
	// fmr f29,f19
	ctx.f29.f64 = ctx.f19.f64;
loc_830BCCEC:
	// lwz r10,336(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// rlwinm r11,r7,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r30,r27,12
	ctx.r30.s64 = ctx.r27.s64 + 12;
	// add r9,r7,r11
	ctx.r9.u64 = ctx.r7.u64 + ctx.r11.u64;
	// addi r29,r27,24
	ctx.r29.s64 = ctx.r27.s64 + 24;
	// rlwinm r28,r9,2,0,29
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,16(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// li r25,0
	ctx.r25.s64 = 0;
	// lwz r9,12(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// add r11,r11,r28
	ctx.r11.u64 = ctx.r11.u64 + ctx.r28.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lfs f0,4(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f0,f27
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f27.f64));
	// lfs f11,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f10,f13,f20
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f20.f64));
	// fmuls f9,f11,f25
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f25.f64));
	// fmadds f8,f11,f28,f12
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f28.f64 + ctx.f12.f64));
	// fmadds f7,f11,f22,f10
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f22.f64 + ctx.f10.f64));
	// fmadds f6,f13,f23,f9
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f23.f64 + ctx.f9.f64));
	// fmadds f5,f13,f26,f8
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f26.f64 + ctx.f8.f64));
	// fmadds f4,f0,f21,f7
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f21.f64 + ctx.f7.f64));
	// fmadds f3,f0,f24,f6
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f24.f64 + ctx.f6.f64));
	// fadds f2,f5,f29
	ctx.f2.f64 = double(float(ctx.f5.f64 + ctx.f29.f64));
	// stfs f2,0(r27)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r27.u32 + 0, temp.u32);
	// fadds f1,f4,f31
	ctx.f1.f64 = double(float(ctx.f4.f64 + ctx.f31.f64));
	// stfs f1,8(r27)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r27.u32 + 8, temp.u32);
	// fadds f0,f3,f30
	ctx.f0.f64 = double(float(ctx.f3.f64 + ctx.f30.f64));
	// stfs f0,4(r27)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r27.u32 + 4, temp.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r10,r8
	ctx.r7.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lfs f12,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f8,f13,f25
	ctx.f8.f64 = double(float(ctx.f13.f64 * ctx.f25.f64));
	// fmuls f9,f12,f27
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f27.f64));
	// fmuls f10,f11,f20
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f20.f64));
	// fmadds f5,f11,f23,f8
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f23.f64 + ctx.f8.f64));
	// fmadds f6,f13,f28,f9
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f28.f64 + ctx.f9.f64));
	// fmadds f7,f13,f22,f10
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f22.f64 + ctx.f10.f64));
	// fmadds f2,f12,f24,f5
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f24.f64 + ctx.f5.f64));
	// fmadds f3,f11,f26,f6
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f26.f64 + ctx.f6.f64));
	// fmadds f4,f12,f21,f7
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f21.f64 + ctx.f7.f64));
	// fadds f13,f2,f30
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f30.f64));
	// stfs f13,16(r27)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r27.u32 + 16, temp.u32);
	// fadds f0,f3,f29
	ctx.f0.f64 = double(float(ctx.f3.f64 + ctx.f29.f64));
	// stfs f0,12(r27)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r27.u32 + 12, temp.u32);
	// fadds f1,f4,f31
	ctx.f1.f64 = double(float(ctx.f4.f64 + ctx.f31.f64));
	// stfs f1,20(r27)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r27.u32 + 20, temp.u32);
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r6,r11,r10
	ctx.r6.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r6,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lfs f11,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// lfs f12,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f7,f12,f25
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f25.f64));
	// fmuls f9,f10,f20
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f20.f64));
	// fmuls f8,f11,f27
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f27.f64));
	// fmadds f4,f10,f23,f7
	ctx.f4.f64 = double(float(ctx.f10.f64 * ctx.f23.f64 + ctx.f7.f64));
	// fmadds f6,f12,f22,f9
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f22.f64 + ctx.f9.f64));
	// fmadds f5,f12,f28,f8
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f28.f64 + ctx.f8.f64));
	// fmadds f1,f11,f24,f4
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f24.f64 + ctx.f4.f64));
	// fmadds f3,f11,f21,f6
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f21.f64 + ctx.f6.f64));
	// fmadds f2,f10,f26,f5
	ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f26.f64 + ctx.f5.f64));
	// fadds f12,f1,f30
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f30.f64));
	// stfs f12,28(r27)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r27.u32 + 28, temp.u32);
	// fadds f0,f3,f31
	ctx.f0.f64 = double(float(ctx.f3.f64 + ctx.f31.f64));
	// stfs f0,32(r27)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r27.u32 + 32, temp.u32);
	// fadds f13,f2,f29
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f29.f64));
	// stfs f13,24(r27)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r27.u32 + 24, temp.u32);
	// bne cr6,0x830bce38
	if (!ctx.cr6.eq) goto loc_830BCE38;
	// cmplwi cr6,r23,0
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 0, ctx.xer);
	// beq cr6,0x830bd4c0
	if (ctx.cr6.eq) goto loc_830BD4C0;
loc_830BCE38:
	// lwz r11,336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// addi r26,r11,4
	ctx.r26.s64 = ctx.r11.s64 + 4;
	// lwz r11,88(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 88);
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// beq cr6,0x830bce70
	if (ctx.cr6.eq) goto loc_830BCE70;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x830bce5c
	if (!ctx.cr6.eq) goto loc_830BCE5C;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x82d52f10
	ctx.lr = 0x830BCE5C;
	sub_82D52F10(ctx, base);
loc_830BCE5C:
	// lwz r11,84(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x830bce74
	if (!ctx.cr6.eq) goto loc_830BCE74;
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r10,84(r26)
	PPC_STORE_U32(ctx.r26.u32 + 84, ctx.r10.u32);
loc_830BCE70:
	// li r11,0
	ctx.r11.s64 = 0;
loc_830BCE74:
	// lfs f0,0(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,0(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// fsubs f9,f0,f12
	ctx.f9.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
	// fsubs f7,f13,f11
	ctx.f7.f64 = double(float(ctx.f13.f64 - ctx.f11.f64));
	// lfs f4,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f4.f64 = double(temp.f32);
	// lfs f6,8(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// fsubs f1,f0,f4
	ctx.f1.f64 = double(float(ctx.f0.f64 - ctx.f4.f64));
	// lfs f5,4(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	ctx.f5.f64 = double(temp.f32);
	// lfs f2,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f2.f64 = double(temp.f32);
	// fsubs f3,f13,f5
	ctx.f3.f64 = double(float(ctx.f13.f64 - ctx.f5.f64));
	// fsubs f0,f6,f2
	ctx.f0.f64 = double(float(ctx.f6.f64 - ctx.f2.f64));
	// lfs f13,8(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f6,f13
	ctx.f12.f64 = double(float(ctx.f6.f64 - ctx.f13.f64));
	// fmuls f11,f9,f7
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// fmuls f10,f0,f3
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f3.f64));
	// fmuls f6,f1,f12
	ctx.f6.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// fmsubs f8,f1,f3,f11
	ctx.f8.f64 = double(float(ctx.f1.f64 * ctx.f3.f64 - ctx.f11.f64));
	// stfs f8,104(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fmsubs f10,f7,f12,f10
	ctx.f10.f64 = double(float(ctx.f7.f64 * ctx.f12.f64 - ctx.f10.f64));
	// stfs f10,96(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fmsubs f9,f9,f0,f6
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64 - ctx.f6.f64));
	// stfs f9,100(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fmuls f5,f8,f8
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f8.f64));
	// fmadds f4,f10,f10,f5
	ctx.f4.f64 = double(float(ctx.f10.f64 * ctx.f10.f64 + ctx.f5.f64));
	// fmadds f0,f9,f9,f4
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f9.f64 + ctx.f4.f64));
	// fcmpu cr6,f0,f19
	ctx.cr6.compare(ctx.f0.f64, ctx.f19.f64);
	// beq cr6,0x830bcf08
	if (ctx.cr6.eq) goto loc_830BCF08;
	// fsqrts f0,f0
	ctx.f0.f64 = double(float(sqrt(ctx.f0.f64)));
	// fdivs f13,f18,f0
	ctx.f13.f64 = double(float(ctx.f18.f64 / ctx.f0.f64));
	// fmuls f10,f10,f13
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// stfs f10,96(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fmuls f9,f13,f9
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// stfs f9,100(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fmuls f8,f13,f8
	ctx.f8.f64 = double(float(ctx.f13.f64 * ctx.f8.f64));
	// stfs f8,104(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
loc_830BCF08:
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lis r10,8191
	ctx.r10.s64 = 536805376;
	// add r3,r11,r28
	ctx.r3.u64 = ctx.r11.u64 + ctx.r28.u64;
	// ori r4,r10,65535
	ctx.r4.u64 = ctx.r10.u64 | 65535;
	// lwzx r11,r11,r28
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r28.u32);
	// rlwinm r9,r11,0,2,2
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20000000;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x830bd0f4
	if (ctx.cr6.eq) goto loc_830BD0F4;
	// clrlwi r11,r11,3
	ctx.r11.u64 = ctx.r11.u32 & 0x1FFFFFFF;
	// li r25,1
	ctx.r25.s64 = 1;
	// cmplw cr6,r11,r4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r4.u32, ctx.xer);
	// bne cr6,0x830bcf54
	if (!ctx.cr6.eq) goto loc_830BCF54;
	// li r25,17
	ctx.r25.s64 = 17;
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// beq cr6,0x830bd0f4
	if (ctx.cr6.eq) goto loc_830BD0F4;
	// stfs f10,0(r24)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r24.u32 + 0, temp.u32);
	// stfs f9,4(r24)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r24.u32 + 4, temp.u32);
	// stfs f8,8(r24)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r24.u32 + 8, temp.u32);
	// b 0x830bd0f4
	goto loc_830BD0F4;
loc_830BCF54:
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r9,336(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// add r8,r11,r10
	ctx.r8.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,16(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// lwz r9,12(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r6,r8,r6
	ctx.r6.u64 = ctx.r8.u64 + ctx.r6.u64;
	// add r5,r11,r5
	ctx.r5.u64 = ctx.r11.u64 + ctx.r5.u64;
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r5,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f0,f28
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f28.f64));
	// lfs f11,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f10,f0,f25
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f25.f64));
	// fmuls f9,f13,f25
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f25.f64));
	// lfs f6,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f7,f11,f28
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f28.f64));
	// lfs f3,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f6,f21
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f21.f64));
	// lfs f8,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f15,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f2,f13,f28
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f28.f64));
	// lfs f5,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f17,f3,f21
	ctx.f17.f64 = double(float(ctx.f3.f64 * ctx.f21.f64));
	// lfs f1,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f16,f11,f25
	ctx.f16.f64 = double(float(ctx.f11.f64 * ctx.f25.f64));
	// fmuls f14,f15,f21
	ctx.f14.f64 = double(float(ctx.f15.f64 * ctx.f21.f64));
	// fmadds f12,f8,f26,f12
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f26.f64 + ctx.f12.f64));
	// fmadds f10,f8,f23,f10
	ctx.f10.f64 = double(float(ctx.f8.f64 * ctx.f23.f64 + ctx.f10.f64));
	// fmadds f9,f5,f23,f9
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f23.f64 + ctx.f9.f64));
	// fmadds f7,f1,f26,f7
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f26.f64 + ctx.f7.f64));
	// fmadds f4,f0,f22,f4
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f22.f64 + ctx.f4.f64));
	// fmadds f2,f5,f26,f2
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f26.f64 + ctx.f2.f64));
	// fmadds f0,f13,f22,f17
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f22.f64 + ctx.f17.f64));
	// fmadds f13,f1,f23,f16
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f23.f64 + ctx.f16.f64));
	// fmadds f11,f11,f22,f14
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f22.f64 + ctx.f14.f64));
	// fmadds f12,f6,f27,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f27.f64 + ctx.f12.f64));
	// fmadds f10,f6,f24,f10
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f24.f64 + ctx.f10.f64));
	// fmadds f9,f3,f24,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f24.f64 + ctx.f9.f64));
	// fmadds f7,f15,f27,f7
	ctx.f7.f64 = double(float(ctx.f15.f64 * ctx.f27.f64 + ctx.f7.f64));
	// fmadds f6,f8,f20,f4
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f20.f64 + ctx.f4.f64));
	// fmadds f4,f3,f27,f2
	ctx.f4.f64 = double(float(ctx.f3.f64 * ctx.f27.f64 + ctx.f2.f64));
	// fmadds f3,f5,f20,f0
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f20.f64 + ctx.f0.f64));
	// fmadds f2,f15,f24,f13
	ctx.f2.f64 = double(float(ctx.f15.f64 * ctx.f24.f64 + ctx.f13.f64));
	// fmadds f1,f1,f20,f11
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f20.f64 + ctx.f11.f64));
	// fadds f0,f12,f29
	ctx.f0.f64 = double(float(ctx.f12.f64 + ctx.f29.f64));
	// fadds f13,f10,f30
	ctx.f13.f64 = double(float(ctx.f10.f64 + ctx.f30.f64));
	// fadds f12,f9,f30
	ctx.f12.f64 = double(float(ctx.f9.f64 + ctx.f30.f64));
	// fadds f11,f7,f29
	ctx.f11.f64 = double(float(ctx.f7.f64 + ctx.f29.f64));
	// fadds f10,f6,f31
	ctx.f10.f64 = double(float(ctx.f6.f64 + ctx.f31.f64));
	// fadds f9,f4,f29
	ctx.f9.f64 = double(float(ctx.f4.f64 + ctx.f29.f64));
	// fadds f8,f3,f31
	ctx.f8.f64 = double(float(ctx.f3.f64 + ctx.f31.f64));
	// fadds f7,f2,f30
	ctx.f7.f64 = double(float(ctx.f2.f64 + ctx.f30.f64));
	// fadds f6,f1,f31
	ctx.f6.f64 = double(float(ctx.f1.f64 + ctx.f31.f64));
	// fsubs f5,f13,f12
	ctx.f5.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// fsubs f4,f0,f11
	ctx.f4.f64 = double(float(ctx.f0.f64 - ctx.f11.f64));
	// fsubs f3,f0,f9
	ctx.f3.f64 = double(float(ctx.f0.f64 - ctx.f9.f64));
	// fsubs f2,f10,f8
	ctx.f2.f64 = double(float(ctx.f10.f64 - ctx.f8.f64));
	// fsubs f1,f13,f7
	ctx.f1.f64 = double(float(ctx.f13.f64 - ctx.f7.f64));
	// fsubs f0,f10,f6
	ctx.f0.f64 = double(float(ctx.f10.f64 - ctx.f6.f64));
	// fmuls f13,f4,f5
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f5.f64));
	// fmuls f11,f2,f1
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// fmuls f10,f3,f0
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmsubs f12,f3,f1,f13
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f1.f64 - ctx.f13.f64));
	// fmsubs f0,f5,f0,f11
	ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f0.f64 - ctx.f11.f64));
	// fmsubs f13,f4,f2,f10
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f2.f64 - ctx.f10.f64));
	// fmuls f9,f12,f12
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f12.f64));
	// fmadds f8,f0,f0,f9
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f0.f64 + ctx.f9.f64));
	// fmadds f11,f13,f13,f8
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 + ctx.f8.f64));
	// fcmpu cr6,f11,f19
	ctx.cr6.compare(ctx.f11.f64, ctx.f19.f64);
	// beq cr6,0x830bd0b8
	if (ctx.cr6.eq) goto loc_830BD0B8;
	// fsqrts f11,f11
	ctx.f11.f64 = double(float(sqrt(ctx.f11.f64)));
	// fdivs f10,f18,f11
	ctx.f10.f64 = double(float(ctx.f18.f64 / ctx.f11.f64));
	// fmuls f0,f0,f10
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// fmuls f13,f10,f13
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// fmuls f12,f10,f12
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
loc_830BD0B8:
	// lfs f10,96(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f10.f64 = double(temp.f32);
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// lfs f9,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f8.f64 = double(temp.f32);
	// beq cr6,0x830bd0f4
	if (ctx.cr6.eq) goto loc_830BD0F4;
	// fadds f11,f0,f10
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f10.f64));
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// fadds f7,f13,f9
	ctx.f7.f64 = double(float(ctx.f13.f64 + ctx.f9.f64));
	// fadds f6,f12,f8
	ctx.f6.f64 = double(float(ctx.f12.f64 + ctx.f8.f64));
	// fmuls f5,f11,f0
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f5,0(r24)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r24.u32 + 0, temp.u32);
	// fmuls f4,f7,f0
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stfs f4,4(r24)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r24.u32 + 4, temp.u32);
	// fmuls f3,f6,f0
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f3,8(r24)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r24.u32 + 8, temp.u32);
loc_830BD0F4:
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r10,r11,0,2,2
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20000000;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x830bd2d0
	if (ctx.cr6.eq) goto loc_830BD2D0;
	// clrlwi r11,r11,3
	ctx.r11.u64 = ctx.r11.u32 & 0x1FFFFFFF;
	// ori r25,r25,2
	ctx.r25.u64 = ctx.r25.u64 | 2;
	// cmplw cr6,r11,r4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r4.u32, ctx.xer);
	// bne cr6,0x830bd130
	if (!ctx.cr6.eq) goto loc_830BD130;
	// ori r25,r25,32
	ctx.r25.u64 = ctx.r25.u64 | 32;
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// beq cr6,0x830bd2d0
	if (ctx.cr6.eq) goto loc_830BD2D0;
	// stfs f10,12(r24)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r24.u32 + 12, temp.u32);
	// stfs f9,16(r24)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r24.u32 + 16, temp.u32);
	// stfs f8,20(r24)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r24.u32 + 20, temp.u32);
	// b 0x830bd2d0
	goto loc_830BD2D0;
loc_830BD130:
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r9,336(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// add r8,r11,r10
	ctx.r8.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,16(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// lwz r9,12(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r6,r8,r6
	ctx.r6.u64 = ctx.r8.u64 + ctx.r6.u64;
	// add r5,r11,r5
	ctx.r5.u64 = ctx.r11.u64 + ctx.r5.u64;
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r5,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f28,f0
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// lfs f11,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f10,f25,f0
	ctx.f10.f64 = double(float(ctx.f25.f64 * ctx.f0.f64));
	// fmuls f9,f25,f13
	ctx.f9.f64 = double(float(ctx.f25.f64 * ctx.f13.f64));
	// lfs f6,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f7,f28,f11
	ctx.f7.f64 = double(float(ctx.f28.f64 * ctx.f11.f64));
	// lfs f3,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f21,f6
	ctx.f4.f64 = double(float(ctx.f21.f64 * ctx.f6.f64));
	// lfs f8,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f15,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f2,f28,f13
	ctx.f2.f64 = double(float(ctx.f28.f64 * ctx.f13.f64));
	// lfs f5,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f17,f21,f3
	ctx.f17.f64 = double(float(ctx.f21.f64 * ctx.f3.f64));
	// lfs f1,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f16,f25,f11
	ctx.f16.f64 = double(float(ctx.f25.f64 * ctx.f11.f64));
	// fmuls f14,f21,f15
	ctx.f14.f64 = double(float(ctx.f21.f64 * ctx.f15.f64));
	// fmadds f12,f26,f8,f12
	ctx.f12.f64 = double(float(ctx.f26.f64 * ctx.f8.f64 + ctx.f12.f64));
	// fmadds f10,f23,f8,f10
	ctx.f10.f64 = double(float(ctx.f23.f64 * ctx.f8.f64 + ctx.f10.f64));
	// fmadds f9,f23,f5,f9
	ctx.f9.f64 = double(float(ctx.f23.f64 * ctx.f5.f64 + ctx.f9.f64));
	// fmadds f7,f26,f1,f7
	ctx.f7.f64 = double(float(ctx.f26.f64 * ctx.f1.f64 + ctx.f7.f64));
	// fmadds f4,f22,f0,f4
	ctx.f4.f64 = double(float(ctx.f22.f64 * ctx.f0.f64 + ctx.f4.f64));
	// fmadds f2,f26,f5,f2
	ctx.f2.f64 = double(float(ctx.f26.f64 * ctx.f5.f64 + ctx.f2.f64));
	// fmadds f0,f22,f13,f17
	ctx.f0.f64 = double(float(ctx.f22.f64 * ctx.f13.f64 + ctx.f17.f64));
	// fmadds f13,f23,f1,f16
	ctx.f13.f64 = double(float(ctx.f23.f64 * ctx.f1.f64 + ctx.f16.f64));
	// fmadds f11,f22,f11,f14
	ctx.f11.f64 = double(float(ctx.f22.f64 * ctx.f11.f64 + ctx.f14.f64));
	// fmadds f12,f27,f6,f12
	ctx.f12.f64 = double(float(ctx.f27.f64 * ctx.f6.f64 + ctx.f12.f64));
	// fmadds f10,f24,f6,f10
	ctx.f10.f64 = double(float(ctx.f24.f64 * ctx.f6.f64 + ctx.f10.f64));
	// fmadds f9,f24,f3,f9
	ctx.f9.f64 = double(float(ctx.f24.f64 * ctx.f3.f64 + ctx.f9.f64));
	// fmadds f7,f27,f15,f7
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f15.f64 + ctx.f7.f64));
	// fmadds f6,f20,f8,f4
	ctx.f6.f64 = double(float(ctx.f20.f64 * ctx.f8.f64 + ctx.f4.f64));
	// fmadds f4,f27,f3,f2
	ctx.f4.f64 = double(float(ctx.f27.f64 * ctx.f3.f64 + ctx.f2.f64));
	// fmadds f3,f20,f5,f0
	ctx.f3.f64 = double(float(ctx.f20.f64 * ctx.f5.f64 + ctx.f0.f64));
	// fmadds f2,f24,f15,f13
	ctx.f2.f64 = double(float(ctx.f24.f64 * ctx.f15.f64 + ctx.f13.f64));
	// fmadds f1,f20,f1,f11
	ctx.f1.f64 = double(float(ctx.f20.f64 * ctx.f1.f64 + ctx.f11.f64));
	// fadds f0,f29,f12
	ctx.f0.f64 = double(float(ctx.f29.f64 + ctx.f12.f64));
	// fadds f13,f10,f30
	ctx.f13.f64 = double(float(ctx.f10.f64 + ctx.f30.f64));
	// fadds f12,f9,f30
	ctx.f12.f64 = double(float(ctx.f9.f64 + ctx.f30.f64));
	// fadds f11,f29,f7
	ctx.f11.f64 = double(float(ctx.f29.f64 + ctx.f7.f64));
	// fadds f10,f6,f31
	ctx.f10.f64 = double(float(ctx.f6.f64 + ctx.f31.f64));
	// fadds f9,f29,f4
	ctx.f9.f64 = double(float(ctx.f29.f64 + ctx.f4.f64));
	// fadds f8,f3,f31
	ctx.f8.f64 = double(float(ctx.f3.f64 + ctx.f31.f64));
	// fadds f7,f2,f30
	ctx.f7.f64 = double(float(ctx.f2.f64 + ctx.f30.f64));
	// fadds f6,f1,f31
	ctx.f6.f64 = double(float(ctx.f1.f64 + ctx.f31.f64));
	// fsubs f5,f13,f12
	ctx.f5.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// fsubs f4,f0,f11
	ctx.f4.f64 = double(float(ctx.f0.f64 - ctx.f11.f64));
	// fsubs f3,f0,f9
	ctx.f3.f64 = double(float(ctx.f0.f64 - ctx.f9.f64));
	// fsubs f2,f10,f8
	ctx.f2.f64 = double(float(ctx.f10.f64 - ctx.f8.f64));
	// fsubs f1,f13,f7
	ctx.f1.f64 = double(float(ctx.f13.f64 - ctx.f7.f64));
	// fsubs f0,f10,f6
	ctx.f0.f64 = double(float(ctx.f10.f64 - ctx.f6.f64));
	// fmuls f13,f5,f4
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// fmuls f11,f2,f1
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// fmuls f10,f0,f3
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f3.f64));
	// fmsubs f12,f1,f3,f13
	ctx.f12.f64 = double(float(ctx.f1.f64 * ctx.f3.f64 - ctx.f13.f64));
	// fmsubs f0,f5,f0,f11
	ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f0.f64 - ctx.f11.f64));
	// fmsubs f13,f2,f4,f10
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f4.f64 - ctx.f10.f64));
	// fmuls f9,f12,f12
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f12.f64));
	// fmadds f8,f0,f0,f9
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f0.f64 + ctx.f9.f64));
	// fmadds f11,f13,f13,f8
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 + ctx.f8.f64));
	// fcmpu cr6,f11,f19
	ctx.cr6.compare(ctx.f11.f64, ctx.f19.f64);
	// beq cr6,0x830bd294
	if (ctx.cr6.eq) goto loc_830BD294;
	// fsqrts f11,f11
	ctx.f11.f64 = double(float(sqrt(ctx.f11.f64)));
	// fdivs f10,f18,f11
	ctx.f10.f64 = double(float(ctx.f18.f64 / ctx.f11.f64));
	// fmuls f0,f0,f10
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// fmuls f13,f13,f10
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// fmuls f12,f12,f10
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
loc_830BD294:
	// lfs f10,96(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f10.f64 = double(temp.f32);
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// lfs f9,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f8.f64 = double(temp.f32);
	// beq cr6,0x830bd2d0
	if (ctx.cr6.eq) goto loc_830BD2D0;
	// fadds f11,f10,f0
	ctx.f11.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// fadds f7,f13,f9
	ctx.f7.f64 = double(float(ctx.f13.f64 + ctx.f9.f64));
	// fadds f6,f12,f8
	ctx.f6.f64 = double(float(ctx.f12.f64 + ctx.f8.f64));
	// fmuls f5,f11,f0
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f5,12(r24)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r24.u32 + 12, temp.u32);
	// fmuls f4,f7,f0
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stfs f4,16(r24)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r24.u32 + 16, temp.u32);
	// fmuls f3,f6,f0
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f3,20(r24)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r24.u32 + 20, temp.u32);
loc_830BD2D0:
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// rlwinm r10,r11,0,2,2
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20000000;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x830bd4a0
	if (ctx.cr6.eq) goto loc_830BD4A0;
	// clrlwi r11,r11,3
	ctx.r11.u64 = ctx.r11.u32 & 0x1FFFFFFF;
	// ori r25,r25,4
	ctx.r25.u64 = ctx.r25.u64 | 4;
	// cmplw cr6,r11,r4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r4.u32, ctx.xer);
	// bne cr6,0x830bd30c
	if (!ctx.cr6.eq) goto loc_830BD30C;
	// ori r25,r25,64
	ctx.r25.u64 = ctx.r25.u64 | 64;
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// beq cr6,0x830bd4a0
	if (ctx.cr6.eq) goto loc_830BD4A0;
	// stfs f10,24(r24)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r24.u32 + 24, temp.u32);
	// stfs f9,28(r24)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r24.u32 + 28, temp.u32);
	// stfs f8,32(r24)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r24.u32 + 32, temp.u32);
	// b 0x830bd4a0
	goto loc_830BD4A0;
loc_830BD30C:
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r9,336(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// add r8,r11,r10
	ctx.r8.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,16(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// lwz r9,12(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r11,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r10,r7
	ctx.r7.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r6,r8,r6
	ctx.r6.u64 = ctx.r8.u64 + ctx.r6.u64;
	// add r5,r11,r5
	ctx.r5.u64 = ctx.r11.u64 + ctx.r5.u64;
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r5,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f0,f28
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f28.f64));
	// lfs f11,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f7,f25,f0
	ctx.f7.f64 = double(float(ctx.f25.f64 * ctx.f0.f64));
	// fmuls f6,f25,f13
	ctx.f6.f64 = double(float(ctx.f25.f64 * ctx.f13.f64));
	// lfs f3,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f11,f28
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f28.f64));
	// lfs f17,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f1,f21,f3
	ctx.f1.f64 = double(float(ctx.f21.f64 * ctx.f3.f64));
	// lfs f5,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f28,f13,f28
	ctx.f28.f64 = double(float(ctx.f13.f64 * ctx.f28.f64));
	// lfs f14,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f15,f21,f17
	ctx.f15.f64 = double(float(ctx.f21.f64 * ctx.f17.f64));
	// lfs f2,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f2.f64 = double(temp.f32);
	// lfs f16,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f25,f11,f25
	ctx.f25.f64 = double(float(ctx.f11.f64 * ctx.f25.f64));
	// fmuls f21,f21,f14
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f14.f64));
	// fmadds f12,f26,f5,f12
	ctx.f12.f64 = double(float(ctx.f26.f64 * ctx.f5.f64 + ctx.f12.f64));
	// fmadds f7,f23,f5,f7
	ctx.f7.f64 = double(float(ctx.f23.f64 * ctx.f5.f64 + ctx.f7.f64));
	// fmadds f6,f23,f2,f6
	ctx.f6.f64 = double(float(ctx.f23.f64 * ctx.f2.f64 + ctx.f6.f64));
	// fmadds f4,f16,f26,f4
	ctx.f4.f64 = double(float(ctx.f16.f64 * ctx.f26.f64 + ctx.f4.f64));
	// fmadds f1,f22,f0,f1
	ctx.f1.f64 = double(float(ctx.f22.f64 * ctx.f0.f64 + ctx.f1.f64));
	// fmadds f0,f26,f2,f28
	ctx.f0.f64 = double(float(ctx.f26.f64 * ctx.f2.f64 + ctx.f28.f64));
	// fmadds f13,f22,f13,f15
	ctx.f13.f64 = double(float(ctx.f22.f64 * ctx.f13.f64 + ctx.f15.f64));
	// fmadds f28,f23,f16,f25
	ctx.f28.f64 = double(float(ctx.f23.f64 * ctx.f16.f64 + ctx.f25.f64));
	// fmadds f11,f22,f11,f21
	ctx.f11.f64 = double(float(ctx.f22.f64 * ctx.f11.f64 + ctx.f21.f64));
	// fmadds f12,f3,f27,f12
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f27.f64 + ctx.f12.f64));
	// fmadds f7,f24,f3,f7
	ctx.f7.f64 = double(float(ctx.f24.f64 * ctx.f3.f64 + ctx.f7.f64));
	// fmadds f6,f24,f17,f6
	ctx.f6.f64 = double(float(ctx.f24.f64 * ctx.f17.f64 + ctx.f6.f64));
	// fmadds f4,f14,f27,f4
	ctx.f4.f64 = double(float(ctx.f14.f64 * ctx.f27.f64 + ctx.f4.f64));
	// fmadds f3,f20,f5,f1
	ctx.f3.f64 = double(float(ctx.f20.f64 * ctx.f5.f64 + ctx.f1.f64));
	// fmadds f1,f17,f27,f0
	ctx.f1.f64 = double(float(ctx.f17.f64 * ctx.f27.f64 + ctx.f0.f64));
	// fmadds f0,f20,f2,f13
	ctx.f0.f64 = double(float(ctx.f20.f64 * ctx.f2.f64 + ctx.f13.f64));
	// fmadds f13,f14,f24,f28
	ctx.f13.f64 = double(float(ctx.f14.f64 * ctx.f24.f64 + ctx.f28.f64));
	// fmadds f11,f20,f16,f11
	ctx.f11.f64 = double(float(ctx.f20.f64 * ctx.f16.f64 + ctx.f11.f64));
	// fadds f5,f29,f12
	ctx.f5.f64 = double(float(ctx.f29.f64 + ctx.f12.f64));
	// fadds f2,f7,f30
	ctx.f2.f64 = double(float(ctx.f7.f64 + ctx.f30.f64));
	// fadds f12,f6,f30
	ctx.f12.f64 = double(float(ctx.f6.f64 + ctx.f30.f64));
	// fadds f7,f29,f4
	ctx.f7.f64 = double(float(ctx.f29.f64 + ctx.f4.f64));
	// fadds f6,f3,f31
	ctx.f6.f64 = double(float(ctx.f3.f64 + ctx.f31.f64));
	// fadds f4,f29,f1
	ctx.f4.f64 = double(float(ctx.f29.f64 + ctx.f1.f64));
	// fadds f3,f0,f31
	ctx.f3.f64 = double(float(ctx.f0.f64 + ctx.f31.f64));
	// fadds f1,f13,f30
	ctx.f1.f64 = double(float(ctx.f13.f64 + ctx.f30.f64));
	// fadds f0,f11,f31
	ctx.f0.f64 = double(float(ctx.f11.f64 + ctx.f31.f64));
	// fsubs f13,f2,f12
	ctx.f13.f64 = double(float(ctx.f2.f64 - ctx.f12.f64));
	// fsubs f11,f5,f7
	ctx.f11.f64 = double(float(ctx.f5.f64 - ctx.f7.f64));
	// fsubs f7,f5,f4
	ctx.f7.f64 = double(float(ctx.f5.f64 - ctx.f4.f64));
	// fsubs f5,f6,f3
	ctx.f5.f64 = double(float(ctx.f6.f64 - ctx.f3.f64));
	// fsubs f4,f2,f1
	ctx.f4.f64 = double(float(ctx.f2.f64 - ctx.f1.f64));
	// fsubs f3,f6,f0
	ctx.f3.f64 = double(float(ctx.f6.f64 - ctx.f0.f64));
	// fmuls f2,f13,f11
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fmuls f1,f5,f4
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// fmuls f6,f3,f7
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f7.f64));
	// fmsubs f12,f4,f7,f2
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f7.f64 - ctx.f2.f64));
	// fmsubs f0,f13,f3,f1
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f3.f64 - ctx.f1.f64));
	// fmsubs f13,f5,f11,f6
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f11.f64 - ctx.f6.f64));
	// fmuls f5,f12,f12
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f12.f64));
	// fmadds f4,f0,f0,f5
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f0.f64 + ctx.f5.f64));
	// fmadds f11,f13,f13,f4
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 + ctx.f4.f64));
	// fcmpu cr6,f11,f19
	ctx.cr6.compare(ctx.f11.f64, ctx.f19.f64);
	// beq cr6,0x830bd470
	if (ctx.cr6.eq) goto loc_830BD470;
	// fsqrts f11,f11
	ctx.f11.f64 = double(float(sqrt(ctx.f11.f64)));
	// fdivs f7,f18,f11
	ctx.f7.f64 = double(float(ctx.f18.f64 / ctx.f11.f64));
	// fmuls f0,f0,f7
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f7.f64));
	// fmuls f13,f13,f7
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f7.f64));
	// fmuls f12,f12,f7
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f7.f64));
loc_830BD470:
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// beq cr6,0x830bd4a0
	if (ctx.cr6.eq) goto loc_830BD4A0;
	// fadds f11,f10,f0
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// fadds f10,f13,f9
	ctx.f10.f64 = double(float(ctx.f13.f64 + ctx.f9.f64));
	// fadds f9,f12,f8
	ctx.f9.f64 = double(float(ctx.f12.f64 + ctx.f8.f64));
	// fmuls f8,f11,f0
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f8,24(r24)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r24.u32 + 24, temp.u32);
	// fmuls f7,f10,f0
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f7,28(r24)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r24.u32 + 28, temp.u32);
	// fmuls f6,f9,f0
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// stfs f6,32(r24)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r24.u32 + 32, temp.u32);
loc_830BD4A0:
	// cmplwi cr6,r23,0
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 0, ctx.xer);
	// li r3,0
	ctx.r3.s64 = 0;
	// beq cr6,0x830bd4c4
	if (ctx.cr6.eq) goto loc_830BD4C4;
	// stw r25,0(r23)
	PPC_STORE_U32(ctx.r23.u32 + 0, ctx.r25.u32);
	// addi r1,r1,432
	ctx.r1.s64 = ctx.r1.s64 + 432;
	// addi r12,r1,-80
	ctx.r12.s64 = ctx.r1.s64 + -80;
	// bl 0x82cb6afc
	ctx.lr = 0x830BD4BC;
	__restfpr_14(ctx, base);
	// b 0x82cb1124
	__restgprlr_23(ctx, base);
	return;
loc_830BD4C0:
	// li r3,0
	ctx.r3.s64 = 0;
loc_830BD4C4:
	// addi r1,r1,432
	ctx.r1.s64 = ctx.r1.s64 + 432;
	// addi r12,r1,-80
	ctx.r12.s64 = ctx.r1.s64 + -80;
	// bl 0x82cb6afc
	ctx.lr = 0x830BD4D0;
	__restfpr_14(ctx, base);
	// b 0x82cb1124
	__restgprlr_23(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830BD4D4"))) PPC_WEAK_FUNC(sub_830BD4D4);
PPC_FUNC_IMPL(__imp__sub_830BD4D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830BD4D8"))) PPC_WEAK_FUNC(sub_830BD4D8);
PPC_FUNC_IMPL(__imp__sub_830BD4D8) {
	PPC_FUNC_PROLOGUE();
	// lwz r10,352(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 352);
	// addi r11,r3,348
	ctx.r11.s64 = ctx.r3.s64 + 348;
	// lwz r9,348(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 348);
	// subf r8,r9,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r9.s64;
	// srawi r7,r8,4
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 4;
	// cmplw cr6,r4,r7
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r7.u32, ctx.xer);
	// blt cr6,0x830bd4fc
	if (ctx.cr6.lt) goto loc_830BD4FC;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_830BD4FC:
	// lwz r11,0(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r10,r4,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// lwzx r9,r10,r11
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// cntlzw r8,r9
	ctx.r8.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r7,r8,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// xori r3,r7,1
	ctx.r3.u64 = ctx.r7.u64 ^ 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830BD518"))) PPC_WEAK_FUNC(sub_830BD518);
PPC_FUNC_IMPL(__imp__sub_830BD518) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e8
	ctx.lr = 0x830BD520;
	__savegprlr_28(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// lwz r11,352(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 352);
	// lwz r10,348(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// rlwinm r8,r9,0,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFF0;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// beq cr6,0x830bd5fc
	if (ctx.cr6.eq) goto loc_830BD5FC;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x83097de8
	ctx.lr = 0x830BD550;
	sub_83097DE8(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x830bd5fc
	if (!ctx.cr6.eq) goto loc_830BD5FC;
	// lwz r10,4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x830bd56c
	if (!ctx.cr6.eq) goto loc_830BD56C;
	// mr r10,r29
	ctx.r10.u64 = ctx.r29.u64;
loc_830BD56C:
	// lwz r11,40(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 40);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// bne cr6,0x830bd57c
	if (!ctx.cr6.eq) goto loc_830BD57C;
	// lwz r11,44(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 44);
loc_830BD57C:
	// lwz r10,264(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x830bd590
	if (ctx.cr6.eq) goto loc_830BD590;
	// addi r11,r10,4
	ctx.r11.s64 = ctx.r10.s64 + 4;
	// b 0x830bd5a8
	goto loc_830BD5A8;
loc_830BD590:
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x830bd5a4
	if (ctx.cr6.eq) goto loc_830BD5A4;
	// lwz r11,332(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 332);
	// b 0x830bd5a8
	goto loc_830BD5A8;
loc_830BD5A4:
	// lwz r11,332(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 332);
loc_830BD5A8:
	// lwz r30,20(r11)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// lwz r31,16(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// cmplw cr6,r31,r30
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r30.u32, ctx.xer);
	// beq cr6,0x830bd5fc
	if (ctx.cr6.eq) goto loc_830BD5FC;
loc_830BD5B8:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// bne cr6,0x830bd5f0
	if (!ctx.cr6.eq) goto loc_830BD5F0;
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r10,104(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 104);
	// cmplw cr6,r10,r29
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r29.u32, ctx.xer);
	// bne cr6,0x830bd5f0
	if (!ctx.cr6.eq) goto loc_830BD5F0;
	// lwz r11,40(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// stw r28,44(r3)
	PPC_STORE_U32(ctx.r3.u32 + 44, ctx.r28.u32);
	// rlwinm r10,r11,30,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 30) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830bd5f0
	if (ctx.cr6.eq) goto loc_830BD5F0;
	// bl 0x830b95f8
	ctx.lr = 0x830BD5F0;
	sub_830B95F8(ctx, base);
loc_830BD5F0:
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// cmplw cr6,r31,r30
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r30.u32, ctx.xer);
	// bne cr6,0x830bd5b8
	if (!ctx.cr6.eq) goto loc_830BD5B8;
loc_830BD5FC:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82cb1138
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830BD604"))) PPC_WEAK_FUNC(sub_830BD604);
PPC_FUNC_IMPL(__imp__sub_830BD604) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830BD608"))) PPC_WEAK_FUNC(sub_830BD608);
PPC_FUNC_IMPL(__imp__sub_830BD608) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10dc
	ctx.lr = 0x830BD610;
	__savegprlr_25(ctx, base);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r7,4(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// addi r10,r1,92
	ctx.r10.s64 = ctx.r1.s64 + 92;
	// lwz r5,8(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r3,12(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// lwz r11,352(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 352);
	// lwz r9,348(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 348);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// subf r6,r9,r11
	ctx.r6.s64 = ctx.r11.s64 - ctx.r9.s64;
	// lwz r9,16(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// stw r7,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r7.u32);
	// srawi. r11,r6,4
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r11.s64 = ctx.r6.s32 >> 4;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r6,20(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	// lwz r4,24(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	// stw r5,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r5.u32);
	// stw r3,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r3.u32);
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// stw r6,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
	// stw r4,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r4.u32);
	// beq 0x830bd6c8
	if (ctx.cr0.eq) goto loc_830BD6C8;
	// lwz r28,104(r1)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// li r30,0
	ctx.r30.s64 = 0;
	// ld r27,96(r1)
	ctx.r27.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// ld r26,88(r1)
	ctx.r26.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// ld r25,80(r1)
	ctx.r25.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
loc_830BD680:
	// lwz r11,348(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 348);
	// lwzx r3,r11,r30
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r30.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x830bd6a8
	if (ctx.cr6.eq) goto loc_830BD6A8;
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// mr r6,r26
	ctx.r6.u64 = ctx.r26.u64;
	// mr r7,r27
	ctx.r7.u64 = ctx.r27.u64;
	// rldicr r8,r28,32,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r28.u64, 32) & 0xFFFFFFFF00000000;
	// li r4,4
	ctx.r4.s64 = 4;
	// bl 0x83160600
	ctx.lr = 0x830BD6A8;
	sub_83160600(ctx, base);
loc_830BD6A8:
	// lwz r11,352(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 352);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// lwz r10,348(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 348);
	// addi r30,r30,16
	ctx.r30.s64 = ctx.r30.s64 + 16;
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi r8,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 4;
	// cmplw cr6,r29,r8
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r8.u32, ctx.xer);
	// blt cr6,0x830bd680
	if (ctx.cr6.lt) goto loc_830BD680;
loc_830BD6C8:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82cb112c
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830BD6D0"))) PPC_WEAK_FUNC(sub_830BD6D0);
PPC_FUNC_IMPL(__imp__sub_830BD6D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// addi r12,r1,-24
	ctx.r12.s64 = ctx.r1.s64 + -24;
	// bl 0x82cb6ac0
	ctx.lr = 0x830BD6E8;
	__savefpr_18(ctx, base);
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// lwz r4,336(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830BD708;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f12,6380(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6380);
	ctx.f12.f64 = double(temp.f32);
	// beq cr6,0x830bd910
	if (ctx.cr6.eq) goto loc_830BD910;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830bd910
	if (ctx.cr6.eq) goto loc_830BD910;
	// lfs f11,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f11.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f10,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f10.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// fmuls f9,f10,f11
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
	// lfs f5,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f5.f64 = double(temp.f32);
	// lfs f7,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f2,f5,f11
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// lfs f1,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f1.f64 = double(temp.f32);
	// fmr f4,f7
	ctx.f4.f64 = ctx.f7.f64;
	// lfs f3,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f29,f1,f7
	ctx.f29.f64 = double(float(ctx.f1.f64 * ctx.f7.f64));
	// lfs f30,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f31,f3,f7
	ctx.f31.f64 = double(float(ctx.f3.f64 * ctx.f7.f64));
	// lfs f28,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f27,f30,f11
	ctx.f27.f64 = double(float(ctx.f30.f64 * ctx.f11.f64));
	// lfs f8,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f26,f28,f11
	ctx.f26.f64 = double(float(ctx.f28.f64 * ctx.f11.f64));
	// lfs f25,252(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f25.f64 = double(temp.f32);
	// fmsubs f6,f8,f8,f12
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f8.f64 - ctx.f12.f64));
	// fmuls f23,f30,f25
	ctx.f23.f64 = double(float(ctx.f30.f64 * ctx.f25.f64));
	// lfs f24,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f24.f64 = double(temp.f32);
	// lfs f0,7676(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// fmadds f9,f1,f8,f9
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f8.f64 + ctx.f9.f64));
	// lfs f13,6140(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6140);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f2,f3,f8,f2
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f8.f64 + ctx.f2.f64));
	// lfs f21,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f20,f24,f4
	ctx.f20.f64 = double(float(ctx.f24.f64 * ctx.f4.f64));
	// lfs f19,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f29,f5,f8,f29
	ctx.f29.f64 = double(float(ctx.f5.f64 * ctx.f8.f64 + ctx.f29.f64));
	// lfs f18,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f18.f64 = double(temp.f32);
	// fmsubs f31,f10,f8,f31
	ctx.f31.f64 = double(float(ctx.f10.f64 * ctx.f8.f64 - ctx.f31.f64));
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// fmadds f27,f24,f25,f27
	ctx.f27.f64 = double(float(ctx.f24.f64 * ctx.f25.f64 + ctx.f27.f64));
	// addi r9,r31,12
	ctx.r9.s64 = ctx.r31.s64 + 12;
	// fmsubs f26,f30,f4,f26
	ctx.f26.f64 = double(float(ctx.f30.f64 * ctx.f4.f64 - ctx.f26.f64));
	// fmuls f22,f24,f6
	ctx.f22.f64 = double(float(ctx.f24.f64 * ctx.f6.f64));
	// fmsubs f24,f24,f11,f23
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f11.f64 - ctx.f23.f64));
	// fmuls f30,f30,f6
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f6.f64));
	// fmadds f9,f3,f25,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f25.f64 + ctx.f9.f64));
	// fmadds f2,f10,f7,f2
	ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f7.f64 + ctx.f2.f64));
	// fmuls f6,f28,f6
	ctx.f6.f64 = double(float(ctx.f28.f64 * ctx.f6.f64));
	// fmadds f10,f10,f25,f29
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f25.f64 + ctx.f29.f64));
	// fnmsubs f31,f1,f11,f31
	ctx.f31.f64 = double(float(-(ctx.f1.f64 * ctx.f11.f64 - ctx.f31.f64)));
	// fmadds f29,f28,f4,f27
	ctx.f29.f64 = double(float(ctx.f28.f64 * ctx.f4.f64 + ctx.f27.f64));
	// fmsubs f23,f28,f25,f20
	ctx.f23.f64 = double(float(ctx.f28.f64 * ctx.f25.f64 - ctx.f20.f64));
	// fmuls f28,f26,f8
	ctx.f28.f64 = double(float(ctx.f26.f64 * ctx.f8.f64));
	// fmuls f27,f24,f8
	ctx.f27.f64 = double(float(ctx.f24.f64 * ctx.f8.f64));
	// fnmsubs f9,f5,f7,f9
	ctx.f9.f64 = double(float(-(ctx.f5.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// fnmsubs f7,f1,f25,f2
	ctx.f7.f64 = double(float(-(ctx.f1.f64 * ctx.f25.f64 - ctx.f2.f64)));
	// fnmsubs f3,f3,f11,f10
	ctx.f3.f64 = double(float(-(ctx.f3.f64 * ctx.f11.f64 - ctx.f10.f64)));
	// fnmsubs f5,f5,f25,f31
	ctx.f5.f64 = double(float(-(ctx.f5.f64 * ctx.f25.f64 - ctx.f31.f64)));
	// fmuls f10,f29,f4
	ctx.f10.f64 = double(float(ctx.f29.f64 * ctx.f4.f64));
	// fmuls f8,f23,f8
	ctx.f8.f64 = double(float(ctx.f23.f64 * ctx.f8.f64));
	// fmuls f2,f29,f25
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f25.f64));
	// fadds f1,f22,f28
	ctx.f1.f64 = double(float(ctx.f22.f64 + ctx.f28.f64));
	// fadds f6,f6,f27
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f27.f64));
	// fmuls f4,f9,f9
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f9.f64));
	// fmuls f11,f29,f11
	ctx.f11.f64 = double(float(ctx.f29.f64 * ctx.f11.f64));
	// fmuls f31,f7,f9
	ctx.f31.f64 = double(float(ctx.f7.f64 * ctx.f9.f64));
	// fmuls f29,f3,f5
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f5.f64));
	// fadds f8,f30,f8
	ctx.f8.f64 = double(float(ctx.f30.f64 + ctx.f8.f64));
	// fmuls f30,f3,f3
	ctx.f30.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// fadds f2,f1,f2
	ctx.f2.f64 = double(float(ctx.f1.f64 + ctx.f2.f64));
	// fadds f10,f6,f10
	ctx.f10.f64 = double(float(ctx.f6.f64 + ctx.f10.f64));
	// fmuls f1,f4,f0
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fmuls f6,f31,f0
	ctx.f6.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fmuls f31,f29,f0
	ctx.f31.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// fadds f8,f8,f11
	ctx.f8.f64 = double(float(ctx.f8.f64 + ctx.f11.f64));
	// fmuls f4,f30,f0
	ctx.f4.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmuls f2,f2,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fsubs f30,f13,f1
	ctx.f30.f64 = double(float(ctx.f13.f64 - ctx.f1.f64));
	// fsubs f11,f6,f31
	ctx.f11.f64 = double(float(ctx.f6.f64 - ctx.f31.f64));
	// stfs f11,84(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fadds f11,f21,f2
	ctx.f11.f64 = double(float(ctx.f21.f64 + ctx.f2.f64));
	// fsubs f2,f30,f4
	ctx.f2.f64 = double(float(ctx.f30.f64 - ctx.f4.f64));
	// stfs f2,80(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmuls f2,f7,f3
	ctx.f2.f64 = double(float(ctx.f7.f64 * ctx.f3.f64));
	// fmuls f30,f9,f5
	ctx.f30.f64 = double(float(ctx.f9.f64 * ctx.f5.f64));
	// fmuls f29,f7,f7
	ctx.f29.f64 = double(float(ctx.f7.f64 * ctx.f7.f64));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// fmuls f9,f9,f3
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f3.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f7,f7,f5
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fmuls f5,f10,f0
	ctx.f5.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fadds f3,f31,f6
	ctx.f3.f64 = double(float(ctx.f31.f64 + ctx.f6.f64));
	// stfs f3,92(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fadds f10,f19,f8
	ctx.f10.f64 = double(float(ctx.f19.f64 + ctx.f8.f64));
	// fmuls f2,f2,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmuls f8,f30,f0
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fnmsubs f6,f29,f0,f13
	ctx.f6.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f3,f9,f0
	ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f13,f7,f0
	ctx.f13.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fadds f0,f18,f5
	ctx.f0.f64 = double(float(ctx.f18.f64 + ctx.f5.f64));
	// fadds f9,f8,f2
	ctx.f9.f64 = double(float(ctx.f8.f64 + ctx.f2.f64));
	// stfs f9,88(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fsubs f5,f2,f8
	ctx.f5.f64 = double(float(ctx.f2.f64 - ctx.f8.f64));
	// stfs f5,104(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fsubs f7,f6,f4
	ctx.f7.f64 = double(float(ctx.f6.f64 - ctx.f4.f64));
	// stfs f7,96(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fsubs f2,f6,f1
	ctx.f2.f64 = double(float(ctx.f6.f64 - ctx.f1.f64));
	// stfs f2,112(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fsubs f4,f3,f13
	ctx.f4.f64 = double(float(ctx.f3.f64 - ctx.f13.f64));
	// stfs f4,100(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f3,f13,f3
	ctx.f3.f64 = double(float(ctx.f13.f64 + ctx.f3.f64));
	// stfs f3,108(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830BD8E4:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830bd8e4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830BD8E4;
	// stfs f0,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f10,40(r9)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f11,44(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
loc_830BD910:
	// lfs f11,20(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	ctx.f11.f64 = double(temp.f32);
	// addi r11,r30,12
	ctx.r11.s64 = ctx.r30.s64 + 12;
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// lfs f13,16(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// fmr f3,f13
	ctx.f3.f64 = ctx.f13.f64;
	// fsubs f10,f13,f0
	ctx.f10.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// lfs f8,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// fsubs f7,f11,f8
	ctx.f7.f64 = double(float(ctx.f11.f64 - ctx.f8.f64));
	// lfs f4,12(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f4.f64 = double(temp.f32);
	// lfs f6,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// addi r11,r31,12
	ctx.r11.s64 = ctx.r31.s64 + 12;
	// fsubs f2,f4,f6
	ctx.f2.f64 = double(float(ctx.f4.f64 - ctx.f6.f64));
	// lfs f11,32(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lfs f31,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,36(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,52(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 52);
	ctx.f29.f64 = double(temp.f32);
	// fadds f5,f9,f8
	ctx.f5.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
	// lfs f9,28(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	ctx.f9.f64 = double(temp.f32);
	// fadds f1,f3,f0
	ctx.f1.f64 = double(float(ctx.f3.f64 + ctx.f0.f64));
	// lfs f0,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f8,f10,f12
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
	// lfs f3,44(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	ctx.f3.f64 = double(temp.f32);
	// fadds f10,f6,f4
	ctx.f10.f64 = double(float(ctx.f6.f64 + ctx.f4.f64));
	// lfs f6,40(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f4,f7,f12
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// lfs f7,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f2,f2,f12
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// lfs f28,48(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,56(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 56);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f5,f5,f12
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// fmuls f1,f1,f12
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// fmuls f26,f8,f13
	ctx.f26.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// fmuls f25,f8,f9
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// fmuls f24,f4,f0
	ctx.f24.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fmuls f23,f4,f11
	ctx.f23.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmuls f8,f8,f6
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f6.f64));
	// fmuls f4,f4,f3
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// fmuls f0,f5,f0
	ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fmuls f11,f5,f11
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// fmuls f5,f5,f3
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fmuls f3,f10,f12
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
	// fmuls f10,f2,f31
	ctx.f10.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f12,f2,f7
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f7.f64));
	// fabs f26,f26
	ctx.f26.u64 = ctx.f26.u64 & ~0x8000000000000000;
	// fabs f24,f24
	ctx.f24.u64 = ctx.f24.u64 & ~0x8000000000000000;
	// fabs f25,f25
	ctx.f25.u64 = ctx.f25.u64 & ~0x8000000000000000;
	// fabs f4,f4
	ctx.f4.u64 = ctx.f4.u64 & ~0x8000000000000000;
	// fmadds f0,f1,f13,f0
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f13.f64 + ctx.f0.f64));
	// fmadds f13,f1,f9,f11
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f9.f64 + ctx.f11.f64));
	// fmadds f6,f1,f6,f5
	ctx.f6.f64 = double(float(ctx.f1.f64 * ctx.f6.f64 + ctx.f5.f64));
	// fabs f11,f23
	ctx.f11.u64 = ctx.f23.u64 & ~0x8000000000000000;
	// fmuls f9,f2,f30
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f30.f64));
	// fabs f5,f8
	ctx.f5.u64 = ctx.f8.u64 & ~0x8000000000000000;
	// fabs f1,f10
	ctx.f1.u64 = ctx.f10.u64 & ~0x8000000000000000;
	// fabs f2,f12
	ctx.f2.u64 = ctx.f12.u64 & ~0x8000000000000000;
	// fadds f12,f24,f26
	ctx.f12.f64 = double(float(ctx.f24.f64 + ctx.f26.f64));
	// fmadds f10,f3,f7,f0
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f7.f64 + ctx.f0.f64));
	// fmadds f8,f3,f31,f13
	ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f31.f64 + ctx.f13.f64));
	// fmadds f13,f3,f30,f6
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f30.f64 + ctx.f6.f64));
	// fadds f7,f11,f25
	ctx.f7.f64 = double(float(ctx.f11.f64 + ctx.f25.f64));
	// fabs f0,f9
	ctx.f0.u64 = ctx.f9.u64 & ~0x8000000000000000;
	// fadds f11,f4,f5
	ctx.f11.f64 = double(float(ctx.f4.f64 + ctx.f5.f64));
	// fadds f9,f12,f2
	ctx.f9.f64 = double(float(ctx.f12.f64 + ctx.f2.f64));
	// fadds f6,f28,f10
	ctx.f6.f64 = double(float(ctx.f28.f64 + ctx.f10.f64));
	// fadds f5,f29,f8
	ctx.f5.f64 = double(float(ctx.f29.f64 + ctx.f8.f64));
	// fadds f3,f27,f13
	ctx.f3.f64 = double(float(ctx.f27.f64 + ctx.f13.f64));
	// fadds f4,f7,f1
	ctx.f4.f64 = double(float(ctx.f7.f64 + ctx.f1.f64));
	// fadds f2,f11,f0
	ctx.f2.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// fsubs f1,f6,f9
	ctx.f1.f64 = double(float(ctx.f6.f64 - ctx.f9.f64));
	// stfs f1,0(r30)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r30.u32 + 0, temp.u32);
	// fadds f0,f9,f6
	ctx.f0.f64 = double(float(ctx.f9.f64 + ctx.f6.f64));
	// fsubs f13,f5,f4
	ctx.f13.f64 = double(float(ctx.f5.f64 - ctx.f4.f64));
	// stfs f13,4(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r30.u32 + 4, temp.u32);
	// fadds f12,f4,f5
	ctx.f12.f64 = double(float(ctx.f4.f64 + ctx.f5.f64));
	// fsubs f11,f3,f2
	ctx.f11.f64 = double(float(ctx.f3.f64 - ctx.f2.f64));
	// stfs f11,8(r30)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r30.u32 + 8, temp.u32);
	// stfs f0,12(r30)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r30.u32 + 12, temp.u32);
	// fadds f10,f2,f3
	ctx.f10.f64 = double(float(ctx.f2.f64 + ctx.f3.f64));
	// stfs f12,16(r30)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r30.u32 + 16, temp.u32);
	// stfs f10,20(r30)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r30.u32 + 20, temp.u32);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// addi r12,r1,-24
	ctx.r12.s64 = ctx.r1.s64 + -24;
	// bl 0x82cb6b0c
	ctx.lr = 0x830BDA68;
	__restfpr_18(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830BDA7C"))) PPC_WEAK_FUNC(sub_830BDA7C);
PPC_FUNC_IMPL(__imp__sub_830BDA7C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830BDA80"))) PPC_WEAK_FUNC(sub_830BDA80);
PPC_FUNC_IMPL(__imp__sub_830BDA80) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e4
	ctx.lr = 0x830BDA88;
	__savegprlr_27(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// rlwinm r11,r4,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r10,348(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cntlzw r8,r9
	ctx.r8.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// lwz r28,20(r10)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// rlwinm r7,r8,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// lwz r30,16(r10)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// xori r11,r7,1
	ctx.r11.u64 = ctx.r7.u64 ^ 1;
	// cmplw cr6,r30,r28
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r28.u32, ctx.xer);
	// beq cr6,0x830bdb3c
	if (ctx.cr6.eq) goto loc_830BDB3C;
	// clrlwi r27,r11,24
	ctx.r27.u64 = ctx.r11.u32 & 0xFF;
loc_830BDAC4:
	// lwz r31,0(r30)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x830b98b0
	ctx.lr = 0x830BDAD0;
	sub_830B98B0(ctx, base);
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// beq cr6,0x830bdb04
	if (ctx.cr6.eq) goto loc_830BDB04;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830bdb30
	if (ctx.cr6.eq) goto loc_830BDB30;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	// bl 0x83097de8
	ctx.lr = 0x830BDAF0;
	sub_83097DE8(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830bdb30
	if (ctx.cr6.eq) goto loc_830BDB30;
	// li r4,1
	ctx.r4.s64 = 1;
	// b 0x830bdb28
	goto loc_830BDB28;
loc_830BDB04:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x830bdb30
	if (!ctx.cr6.eq) goto loc_830BDB30;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	// bl 0x83097de8
	ctx.lr = 0x830BDB18;
	sub_83097DE8(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x830bdb30
	if (!ctx.cr6.eq) goto loc_830BDB30;
	// li r4,0
	ctx.r4.s64 = 0;
loc_830BDB28:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x830b9950
	ctx.lr = 0x830BDB30;
	sub_830B9950(ctx, base);
loc_830BDB30:
	// addi r30,r30,4
	ctx.r30.s64 = ctx.r30.s64 + 4;
	// cmplw cr6,r30,r28
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r28.u32, ctx.xer);
	// bne cr6,0x830bdac4
	if (!ctx.cr6.eq) goto loc_830BDAC4;
loc_830BDB3C:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82cb1134
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830BDB44"))) PPC_WEAK_FUNC(sub_830BDB44);
PPC_FUNC_IMPL(__imp__sub_830BDB44) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830BDB48"))) PPC_WEAK_FUNC(sub_830BDB48);
PPC_FUNC_IMPL(__imp__sub_830BDB48) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e4
	ctx.lr = 0x830BDB50;
	__savegprlr_27(ctx, base);
	// stfd f31,-56(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.f31.u64);
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// mr r30,r5
	ctx.r30.u64 = ctx.r5.u64;
	// mr r28,r6
	ctx.r28.u64 = ctx.r6.u64;
	// li r6,3
	ctx.r6.s64 = 3;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r29,1160
	ctx.r4.s64 = ctx.r29.s64 + 1160;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r27,r7
	ctx.r27.u64 = ctx.r7.u64;
	// bl 0x831f3598
	ctx.lr = 0x830BDB7C;
	sub_831F3598(ctx, base);
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// stw r28,100(r31)
	PPC_STORE_U32(ctx.r31.u32 + 100, ctx.r28.u32);
	// stw r30,104(r31)
	PPC_STORE_U32(ctx.r31.u32 + 104, ctx.r30.u32);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// addi r10,r11,4844
	ctx.r10.s64 = ctx.r11.s64 + 4844;
	// stw r10,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r10.u32);
	// bl 0x8315f1e0
	ctx.lr = 0x830BDB98;
	sub_8315F1E0(ctx, base);
	// li r9,1
	ctx.r9.s64 = 1;
	// stw r27,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r27.u32);
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// stw r9,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r9.u32);
	// stw r31,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r31.u32);
	// lwz r3,1416(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1416);
	// bl 0x8315f200
	ctx.lr = 0x830BDBB4;
	sub_8315F200(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,72(r31)
	PPC_STORE_U32(ctx.r31.u32 + 72, ctx.r3.u32);
	// bne cr6,0x830bdbdc
	if (!ctx.cr6.eq) goto loc_830BDBDC;
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r4,r11,4472
	ctx.r4.s64 = ctx.r11.s64 + 4472;
	// li r5,56
	ctx.r5.s64 = 56;
	// addi r7,r4,-40
	ctx.r7.s64 = ctx.r4.s64 + -40;
	// li r3,4
	ctx.r3.s64 = 4;
	// bl 0x82d17988
	ctx.lr = 0x830BDBDC;
	sub_82D17988(ctx, base);
loc_830BDBDC:
	// lwz r11,264(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 264);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x830bdbf0
	if (ctx.cr6.eq) goto loc_830BDBF0;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// b 0x830bdc08
	goto loc_830BDC08;
loc_830BDBF0:
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x830bdc04
	if (ctx.cr6.eq) goto loc_830BDC04;
	// lwz r11,332(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 332);
	// b 0x830bdc08
	goto loc_830BDC08;
loc_830BDC04:
	// lwz r11,332(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 332);
loc_830BDC08:
	// stw r11,108(r31)
	PPC_STORE_U32(ctx.r31.u32 + 108, ctx.r11.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lwz r5,100(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 100);
	// lwz r4,104(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 104);
	// bl 0x830bd6d0
	ctx.lr = 0x830BDC1C;
	sub_830BD6D0(ctx, base);
	// lfs f0,96(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f12,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// lfs f10,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f10.f64 = double(temp.f32);
	// lwz r3,72(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	// lfs f9,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f9.f64 = double(temp.f32);
	// fadds f8,f12,f10
	ctx.f8.f64 = double(float(ctx.f12.f64 + ctx.f10.f64));
	// lfs f7,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f7.f64 = double(temp.f32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// fadds f6,f7,f9
	ctx.f6.f64 = double(float(ctx.f7.f64 + ctx.f9.f64));
	// lfs f31,6380(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6380);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f5,f11,f31
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// stfs f5,80(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmuls f4,f8,f31
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// stfs f4,84(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmuls f3,f6,f31
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// stfs f3,88(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// beq cr6,0x830bdc80
	if (ctx.cr6.eq) goto loc_830BDC80;
	// lwz r11,88(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// li r4,0
	ctx.r4.s64 = 0;
	// ld r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// rldicr r6,r11,32,63
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// bl 0x8315f360
	ctx.lr = 0x830BDC80;
	sub_8315F360(ctx, base);
loc_830BDC80:
	// lfs f0,108(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f0.f64 = double(temp.f32);
	// lwz r3,72(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	// lfs f13,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f11,f0,f13
	ctx.f11.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// lfs f10,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f10.f64 = double(temp.f32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// lfs f9,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f9.f64 = double(temp.f32);
	// fsubs f8,f12,f10
	ctx.f8.f64 = double(float(ctx.f12.f64 - ctx.f10.f64));
	// lfs f7,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f7.f64 = double(temp.f32);
	// fsubs f6,f9,f7
	ctx.f6.f64 = double(float(ctx.f9.f64 - ctx.f7.f64));
	// fmuls f5,f11,f31
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// stfs f5,80(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmuls f4,f8,f31
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// stfs f4,84(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmuls f3,f6,f31
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// stfs f3,88(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// beq cr6,0x830bdcdc
	if (ctx.cr6.eq) goto loc_830BDCDC;
	// lwz r11,88(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// li r4,1
	ctx.r4.s64 = 1;
	// ld r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// rldicr r6,r11,32,63
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// bl 0x8315f360
	ctx.lr = 0x830BDCDC;
	sub_8315F360(ctx, base);
loc_830BDCDC:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// lfd f31,-56(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// b 0x82cb1134
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830BDCEC"))) PPC_WEAK_FUNC(sub_830BDCEC);
PPC_FUNC_IMPL(__imp__sub_830BDCEC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830BDCF0"))) PPC_WEAK_FUNC(sub_830BDCF0);
PPC_FUNC_IMPL(__imp__sub_830BDCF0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10ec
	ctx.lr = 0x830BDCF8;
	__savegprlr_29(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// lwz r3,72(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 72);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x830bdd18
	if (ctx.cr6.eq) goto loc_830BDD18;
	// bl 0x8315f270
	ctx.lr = 0x830BDD10;
	sub_8315F270(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,72(r29)
	PPC_STORE_U32(ctx.r29.u32 + 72, ctx.r11.u32);
loc_830BDD18:
	// lwz r11,16(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 16);
	// lwz r10,20(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 20);
	// addi r30,r11,-4
	ctx.r30.s64 = ctx.r11.s64 + -4;
	// addi r31,r10,-4
	ctx.r31.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r31,r30
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r30.u32, ctx.xer);
	// beq cr6,0x830bdd5c
	if (ctx.cr6.eq) goto loc_830BDD5C;
loc_830BDD30:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x830bdd50
	if (ctx.cr6.eq) goto loc_830BDD50;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830BDD50;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830BDD50:
	// addi r31,r31,-4
	ctx.r31.s64 = ctx.r31.s64 + -4;
	// cmplw cr6,r31,r30
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r30.u32, ctx.xer);
	// bne cr6,0x830bdd30
	if (!ctx.cr6.eq) goto loc_830BDD30;
loc_830BDD5C:
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x831f3728
	ctx.lr = 0x830BDD64;
	sub_831F3728(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82cb113c
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830BDD6C"))) PPC_WEAK_FUNC(sub_830BDD6C);
PPC_FUNC_IMPL(__imp__sub_830BDD6C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830BDD70"))) PPC_WEAK_FUNC(sub_830BDD70);
PPC_FUNC_IMPL(__imp__sub_830BDD70) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10dc
	ctx.lr = 0x830BDD78;
	__savegprlr_25(ctx, base);
	// addi r12,r1,-64
	ctx.r12.s64 = ctx.r1.s64 + -64;
	// bl 0x82cb6ab8
	ctx.lr = 0x830BDD80;
	__savefpr_16(ctx, base);
	// stwu r1,-464(r1)
	ea = -464 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r25,r3
	ctx.r25.u64 = ctx.r3.u64;
	// lwz r11,40(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 40);
	// lwz r10,36(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 36);
	// rlwinm r9,r11,0,31,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFFFD;
	// stw r9,40(r25)
	PPC_STORE_U32(ctx.r25.u32 + 40, ctx.r9.u32);
	// lwz r3,104(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 104);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r7,352(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 352);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x830BDDAC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmpwi cr6,r3,1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 1, ctx.xer);
	// beq cr6,0x830be51c
	if (ctx.cr6.eq) goto loc_830BE51C;
	// lwz r11,36(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 36);
	// lwz r3,104(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 104);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,352(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 352);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830BDDCC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmpwi cr6,r3,2
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 2, ctx.xer);
	// beq cr6,0x830be51c
	if (ctx.cr6.eq) goto loc_830BE51C;
	// lwz r11,40(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 40);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x830becdc
	if (!ctx.cr6.eq) goto loc_830BECDC;
	// lwz r10,36(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 36);
	// ori r9,r11,1
	ctx.r9.u64 = ctx.r11.u64 | 1;
	// stw r9,40(r25)
	PPC_STORE_U32(ctx.r25.u32 + 40, ctx.r9.u32);
	// lwz r29,104(r10)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + 104);
	// lwz r26,100(r10)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r10.u32 + 100);
	// rlwinm r28,r26,4,0,27
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r11,348(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// add r11,r11,r28
	ctx.r11.u64 = ctx.r11.u64 + ctx.r28.u64;
	// addi r10,r11,8
	ctx.r10.s64 = ctx.r11.s64 + 8;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// addi r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 1;
	// stw r8,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r8.u32);
	// lwz r7,344(r29)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r29.u32 + 344);
	// cmpwi cr6,r7,2
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 2, ctx.xer);
	// bne cr6,0x830be4ec
	if (!ctx.cr6.eq) goto loc_830BE4EC;
	// lwz r11,348(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// add r11,r11,r28
	ctx.r11.u64 = ctx.r11.u64 + ctx.r28.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x830be4ec
	if (!ctx.cr6.gt) goto loc_830BE4EC;
	// lwz r11,0(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x830be4ec
	if (!ctx.cr6.eq) goto loc_830BE4EC;
	// lwz r11,348(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// li r27,0
	ctx.r27.s64 = 0;
	// add r11,r11,r28
	ctx.r11.u64 = ctx.r11.u64 + ctx.r28.u64;
	// stb r27,12(r11)
	PPC_STORE_U8(ctx.r11.u32 + 12, ctx.r27.u8);
	// lwz r10,348(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// lwzx r9,r10,r28
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r28.u32);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x830be4ec
	if (!ctx.cr6.eq) goto loc_830BE4EC;
	// lfs f0,112(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 112);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f13,116(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 116);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f11,120(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 120);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// lfs f5,124(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 124);
	ctx.f5.f64 = double(temp.f32);
	// lfs f8,184(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 184);
	ctx.f8.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f30,6380(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6380);
	ctx.f30.f64 = double(temp.f32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lfs f7,180(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 180);
	ctx.f7.f64 = double(temp.f32);
	// fmsubs f4,f5,f5,f30
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f5.f64 - ctx.f30.f64));
	// lfs f6,176(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 176);
	ctx.f6.f64 = double(temp.f32);
	// addi r31,r29,112
	ctx.r31.s64 = ctx.r29.s64 + 112;
	// lfs f3,160(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 160);
	ctx.f3.f64 = double(temp.f32);
	// addi r30,r29,160
	ctx.r30.s64 = ctx.r29.s64 + 160;
	// lfs f2,172(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 172);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f3,f11
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// fmuls f29,f2,f0
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// lfs f26,164(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 164);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f27,f12,f8
	ctx.f27.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// lfs f28,168(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 168);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f25,f10,f6
	ctx.f25.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// lfs f31,7676(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 7676);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f24,f10,f7
	ctx.f24.f64 = double(float(ctx.f10.f64 * ctx.f7.f64));
	// lfs f23,132(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 132);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f22,f9,f7
	ctx.f22.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// lfs f21,136(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 136);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f20,f2,f11
	ctx.f20.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// lfs f19,128(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 128);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f17,f4,f7
	ctx.f17.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// addi r9,r1,124
	ctx.r9.s64 = ctx.r1.s64 + 124;
	// fmuls f16,f4,f8
	ctx.f16.f64 = double(float(ctx.f4.f64 * ctx.f8.f64));
	// fmuls f4,f4,f6
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f6.f64));
	// fmuls f18,f3,f0
	ctx.f18.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmadds f1,f26,f5,f1
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f5.f64 + ctx.f1.f64));
	// fmsubs f27,f9,f6,f27
	ctx.f27.f64 = double(float(ctx.f9.f64 * ctx.f6.f64 - ctx.f27.f64));
	// fmsubs f7,f12,f7,f25
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f7.f64 - ctx.f25.f64));
	// fmadds f6,f12,f6,f24
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f6.f64 + ctx.f24.f64));
	// fmsubs f25,f10,f8,f22
	ctx.f25.f64 = double(float(ctx.f10.f64 * ctx.f8.f64 - ctx.f22.f64));
	// fmadds f29,f3,f5,f29
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f5.f64 + ctx.f29.f64));
	// fmadds f24,f28,f5,f20
	ctx.f24.f64 = double(float(ctx.f28.f64 * ctx.f5.f64 + ctx.f20.f64));
	// fmsubs f22,f2,f5,f18
	ctx.f22.f64 = double(float(ctx.f2.f64 * ctx.f5.f64 - ctx.f18.f64));
	// fmadds f2,f2,f13,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f13.f64 + ctx.f1.f64));
	// fmuls f27,f5,f27
	ctx.f27.f64 = double(float(ctx.f5.f64 * ctx.f27.f64));
	// fmuls f7,f5,f7
	ctx.f7.f64 = double(float(ctx.f5.f64 * ctx.f7.f64));
	// fmadds f6,f9,f8,f6
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f8.f64 + ctx.f6.f64));
	// fmuls f5,f25,f5
	ctx.f5.f64 = double(float(ctx.f25.f64 * ctx.f5.f64));
	// fmadds f8,f28,f13,f29
	ctx.f8.f64 = double(float(ctx.f28.f64 * ctx.f13.f64 + ctx.f29.f64));
	// fmadds f1,f26,f0,f24
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f0.f64 + ctx.f24.f64));
	// fnmsubs f29,f26,f13,f22
	ctx.f29.f64 = double(float(-(ctx.f26.f64 * ctx.f13.f64 - ctx.f22.f64)));
	// fnmsubs f2,f28,f0,f2
	ctx.f2.f64 = double(float(-(ctx.f28.f64 * ctx.f0.f64 - ctx.f2.f64)));
	// stfs f2,84(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fadds f27,f17,f27
	ctx.f27.f64 = double(float(ctx.f17.f64 + ctx.f27.f64));
	// fadds f7,f16,f7
	ctx.f7.f64 = double(float(ctx.f16.f64 + ctx.f7.f64));
	// fmuls f10,f6,f10
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f10.f64));
	// fmuls f9,f6,f9
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f9.f64));
	// fmuls f6,f6,f12
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// fadds f5,f4,f5
	ctx.f5.f64 = double(float(ctx.f4.f64 + ctx.f5.f64));
	// fnmsubs f4,f26,f11,f8
	ctx.f4.f64 = double(float(-(ctx.f26.f64 * ctx.f11.f64 - ctx.f8.f64)));
	// stfs f4,80(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fnmsubs f1,f3,f13,f1
	ctx.f1.f64 = double(float(-(ctx.f3.f64 * ctx.f13.f64 - ctx.f1.f64)));
	// stfs f1,88(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fnmsubs f0,f28,f11,f29
	ctx.f0.f64 = double(float(-(ctx.f28.f64 * ctx.f11.f64 - ctx.f29.f64)));
	// stfs f0,92(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fadds f13,f27,f10
	ctx.f13.f64 = double(float(ctx.f27.f64 + ctx.f10.f64));
	// fadds f12,f7,f9
	ctx.f12.f64 = double(float(ctx.f7.f64 + ctx.f9.f64));
	// fadds f11,f5,f6
	ctx.f11.f64 = double(float(ctx.f5.f64 + ctx.f6.f64));
	// fmuls f10,f13,f31
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// fmuls f9,f12,f31
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// fmuls f8,f11,f31
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// fadds f7,f23,f10
	ctx.f7.f64 = double(float(ctx.f23.f64 + ctx.f10.f64));
	// stfs f7,100(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f6,f21,f9
	ctx.f6.f64 = double(float(ctx.f21.f64 + ctx.f9.f64));
	// stfs f6,104(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fadds f5,f8,f19
	ctx.f5.f64 = double(float(ctx.f8.f64 + ctx.f19.f64));
	// stfs f5,96(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lwz r7,8(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// lwz r6,12(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lwz r11,4(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r5,100(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r4,104(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r8,96(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r11.u32);
	// stw r7,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r7.u32);
	// stw r6,12(r9)
	PPC_STORE_U32(ctx.r9.u32 + 12, ctx.r6.u32);
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// stw r5,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r5.u32);
	// stw r8,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r8.u32);
	// stw r4,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r4.u32);
	// bl 0x8315fb70
	ctx.lr = 0x830BDFDC;
	sub_8315FB70(ctx, base);
	// li r7,10
	ctx.r7.s64 = 10;
	// stw r7,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r7.u32);
	// lwz r11,264(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 264);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830be2c0
	if (ctx.cr6.eq) goto loc_830BE2C0;
	// lfs f13,220(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 220);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r11,216
	ctx.r10.s64 = ctx.r11.s64 + 216;
	// lfs f11,224(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 224);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// lfs f9,232(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 232);
	ctx.f9.f64 = double(temp.f32);
	// fmr f8,f11
	ctx.f8.f64 = ctx.f11.f64;
	// lfs f7,236(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 236);
	ctx.f7.f64 = double(temp.f32);
	// fneg f6,f9
	ctx.f6.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// lfs f0,216(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 216);
	ctx.f0.f64 = double(temp.f32);
	// fneg f4,f7
	ctx.f4.u64 = ctx.f7.u64 ^ 0x8000000000000000;
	// lfs f5,240(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 240);
	ctx.f5.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// fneg f3,f5
	ctx.f3.u64 = ctx.f5.u64 ^ 0x8000000000000000;
	// lfs f2,228(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 228);
	ctx.f2.f64 = double(temp.f32);
	// fneg f1,f0
	ctx.f1.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lfs f0,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f0.f64 = double(temp.f32);
	// fneg f7,f13
	ctx.f7.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// lfs f9,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f9.f64 = double(temp.f32);
	// fneg f11,f11
	ctx.f11.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// lfs f5,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f5.f64 = double(temp.f32);
	// fmr f13,f2
	ctx.f13.f64 = ctx.f2.f64;
	// lfs f29,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f29.f64 = double(temp.f32);
	// fmsubs f28,f2,f2,f30
	ctx.f28.f64 = double(float(ctx.f2.f64 * ctx.f2.f64 - ctx.f30.f64));
	// lfs f27,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f26,f6,f10
	ctx.f26.f64 = double(float(ctx.f6.f64 * ctx.f10.f64));
	// fmuls f25,f8,f4
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// fmuls f21,f10,f4
	ctx.f21.f64 = double(float(ctx.f10.f64 * ctx.f4.f64));
	// fmuls f24,f3,f12
	ctx.f24.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// fmuls f23,f0,f1
	ctx.f23.f64 = double(float(ctx.f0.f64 * ctx.f1.f64));
	// fmuls f20,f9,f7
	ctx.f20.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// fmuls f22,f9,f11
	ctx.f22.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// fmuls f19,f5,f7
	ctx.f19.f64 = double(float(ctx.f5.f64 * ctx.f7.f64));
	// fmsubs f18,f13,f13,f30
	ctx.f18.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 - ctx.f30.f64));
	// fmuls f17,f6,f28
	ctx.f17.f64 = double(float(ctx.f6.f64 * ctx.f28.f64));
	// fmsubs f26,f4,f12,f26
	ctx.f26.f64 = double(float(ctx.f4.f64 * ctx.f12.f64 - ctx.f26.f64));
	// fmsubs f25,f10,f3,f25
	ctx.f25.f64 = double(float(ctx.f10.f64 * ctx.f3.f64 - ctx.f25.f64));
	// fmadds f21,f8,f3,f21
	ctx.f21.f64 = double(float(ctx.f8.f64 * ctx.f3.f64 + ctx.f21.f64));
	// fmsubs f24,f6,f8,f24
	ctx.f24.f64 = double(float(ctx.f6.f64 * ctx.f8.f64 - ctx.f24.f64));
	// fmsubs f23,f5,f11,f23
	ctx.f23.f64 = double(float(ctx.f5.f64 * ctx.f11.f64 - ctx.f23.f64));
	// fmadds f20,f5,f1,f20
	ctx.f20.f64 = double(float(ctx.f5.f64 * ctx.f1.f64 + ctx.f20.f64));
	// fmsubs f22,f0,f7,f22
	ctx.f22.f64 = double(float(ctx.f0.f64 * ctx.f7.f64 - ctx.f22.f64));
	// fmsubs f19,f9,f1,f19
	ctx.f19.f64 = double(float(ctx.f9.f64 * ctx.f1.f64 - ctx.f19.f64));
	// fmuls f4,f4,f28
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f28.f64));
	// fmuls f3,f3,f28
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f28.f64));
	// fmuls f26,f2,f26
	ctx.f26.f64 = double(float(ctx.f2.f64 * ctx.f26.f64));
	// fmuls f25,f2,f25
	ctx.f25.f64 = double(float(ctx.f2.f64 * ctx.f25.f64));
	// fmadds f6,f6,f12,f21
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f12.f64 + ctx.f21.f64));
	// fmuls f2,f2,f24
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f24.f64));
	// fmuls f28,f0,f18
	ctx.f28.f64 = double(float(ctx.f0.f64 * ctx.f18.f64));
	// fmuls f9,f9,f18
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f18.f64));
	// fmuls f24,f23,f13
	ctx.f24.f64 = double(float(ctx.f23.f64 * ctx.f13.f64));
	// fmadds f0,f0,f11,f20
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 + ctx.f20.f64));
	// fmuls f5,f5,f18
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f18.f64));
	// fmuls f23,f13,f22
	ctx.f23.f64 = double(float(ctx.f13.f64 * ctx.f22.f64));
	// fmuls f22,f19,f13
	ctx.f22.f64 = double(float(ctx.f19.f64 * ctx.f13.f64));
	// fsubs f3,f3,f26
	ctx.f3.f64 = double(float(ctx.f3.f64 - ctx.f26.f64));
	// fmuls f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// fsubs f4,f4,f2
	ctx.f4.f64 = double(float(ctx.f4.f64 - ctx.f2.f64));
	// fmuls f10,f10,f6
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// fmuls f8,f8,f6
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f6.f64));
	// fadds f2,f9,f24
	ctx.f2.f64 = double(float(ctx.f9.f64 + ctx.f24.f64));
	// fsubs f26,f17,f25
	ctx.f26.f64 = double(float(ctx.f17.f64 - ctx.f25.f64));
	// fmuls f6,f0,f1
	ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f1.f64));
	// fadds f9,f5,f23
	ctx.f9.f64 = double(float(ctx.f5.f64 + ctx.f23.f64));
	// fmuls f25,f7,f0
	ctx.f25.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fadds f5,f28,f22
	ctx.f5.f64 = double(float(ctx.f28.f64 + ctx.f22.f64));
	// fmuls f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fadds f4,f4,f10
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f10.f64));
	// fadds f8,f3,f8
	ctx.f8.f64 = double(float(ctx.f3.f64 + ctx.f8.f64));
	// fadds f12,f26,f12
	ctx.f12.f64 = double(float(ctx.f26.f64 + ctx.f12.f64));
	// fadds f3,f2,f25
	ctx.f3.f64 = double(float(ctx.f2.f64 + ctx.f25.f64));
	// fadds f2,f9,f6
	ctx.f2.f64 = double(float(ctx.f9.f64 + ctx.f6.f64));
	// fadds f0,f5,f0
	ctx.f0.f64 = double(float(ctx.f5.f64 + ctx.f0.f64));
	// fmuls f9,f4,f31
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// fmuls f10,f8,f31
	ctx.f10.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fmuls f4,f29,f13
	ctx.f4.f64 = double(float(ctx.f29.f64 * ctx.f13.f64));
	// fmuls f12,f12,f31
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// fmuls f8,f3,f31
	ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// lfs f3,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f6,f2,f31
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f5,f0,f31
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fmuls f28,f27,f11
	ctx.f28.f64 = double(float(ctx.f27.f64 * ctx.f11.f64));
	// lfs f0,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f2,f27,f13
	ctx.f2.f64 = double(float(ctx.f27.f64 * ctx.f13.f64));
	// lfs f23,16(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f4,f0,f1,f4
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f1.f64 + ctx.f4.f64));
	// lfs f26,24(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 24);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f25,f27,f1
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f1.f64));
	// lfs f24,20(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	ctx.f24.f64 = double(temp.f32);
	// fadds f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 + ctx.f12.f64));
	// lfs f6,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// fadds f9,f8,f9
	ctx.f9.f64 = double(float(ctx.f8.f64 + ctx.f9.f64));
	// lfs f8,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// fadds f5,f5,f10
	ctx.f5.f64 = double(float(ctx.f5.f64 + ctx.f10.f64));
	// lfs f10,12(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// lfs f22,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f22.f64 = double(temp.f32);
	// lwz r8,184(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 184);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r9,r1,172
	ctx.r9.s64 = ctx.r1.s64 + 172;
	// fmadds f28,f0,f13,f28
	ctx.f28.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 + ctx.f28.f64));
	// fmadds f2,f3,f1,f2
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f1.f64 + ctx.f2.f64));
	// fmadds f4,f3,f11,f4
	ctx.f4.f64 = double(float(ctx.f3.f64 * ctx.f11.f64 + ctx.f4.f64));
	// fmsubs f13,f3,f13,f25
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f13.f64 - ctx.f25.f64));
	// fmadds f3,f3,f7,f28
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f7.f64 + ctx.f28.f64));
	// fmadds f2,f29,f7,f2
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f7.f64 + ctx.f2.f64));
	// fnmsubs f13,f0,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f0.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// fnmsubs f7,f27,f7,f4
	ctx.f7.f64 = double(float(-(ctx.f27.f64 * ctx.f7.f64 - ctx.f4.f64)));
	// fnmsubs f3,f29,f1,f3
	ctx.f3.f64 = double(float(-(ctx.f29.f64 * ctx.f1.f64 - ctx.f3.f64)));
	// fnmsubs f4,f0,f11,f2
	ctx.f4.f64 = double(float(-(ctx.f0.f64 * ctx.f11.f64 - ctx.f2.f64)));
	// fnmsubs f2,f29,f11,f13
	ctx.f2.f64 = double(float(-(ctx.f29.f64 * ctx.f11.f64 - ctx.f13.f64)));
	// fmuls f1,f24,f7
	ctx.f1.f64 = double(float(ctx.f24.f64 * ctx.f7.f64));
	// fmuls f11,f26,f7
	ctx.f11.f64 = double(float(ctx.f26.f64 * ctx.f7.f64));
	// fmuls f28,f8,f7
	ctx.f28.f64 = double(float(ctx.f8.f64 * ctx.f7.f64));
	// fmuls f27,f10,f7
	ctx.f27.f64 = double(float(ctx.f10.f64 * ctx.f7.f64));
	// fmuls f13,f23,f3
	ctx.f13.f64 = double(float(ctx.f23.f64 * ctx.f3.f64));
	// fmuls f0,f26,f4
	ctx.f0.f64 = double(float(ctx.f26.f64 * ctx.f4.f64));
	// fmuls f29,f6,f3
	ctx.f29.f64 = double(float(ctx.f6.f64 * ctx.f3.f64));
	// fmsubs f30,f2,f2,f30
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f2.f64 - ctx.f30.f64));
	// fmsubs f1,f26,f3,f1
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f3.f64 - ctx.f1.f64));
	// fmadds f11,f24,f3,f11
	ctx.f11.f64 = double(float(ctx.f24.f64 * ctx.f3.f64 + ctx.f11.f64));
	// fmuls f25,f8,f4
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// fmadds f28,f22,f2,f28
	ctx.f28.f64 = double(float(ctx.f22.f64 * ctx.f2.f64 + ctx.f28.f64));
	// fmadds f27,f6,f2,f27
	ctx.f27.f64 = double(float(ctx.f6.f64 * ctx.f2.f64 + ctx.f27.f64));
	// fmsubs f13,f24,f4,f13
	ctx.f13.f64 = double(float(ctx.f24.f64 * ctx.f4.f64 - ctx.f13.f64));
	// fmsubs f0,f23,f7,f0
	ctx.f0.f64 = double(float(ctx.f23.f64 * ctx.f7.f64 - ctx.f0.f64));
	// fmadds f29,f8,f2,f29
	ctx.f29.f64 = double(float(ctx.f8.f64 * ctx.f2.f64 + ctx.f29.f64));
	// fmuls f21,f23,f30
	ctx.f21.f64 = double(float(ctx.f23.f64 * ctx.f30.f64));
	// fmuls f24,f24,f30
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f30.f64));
	// fmuls f30,f26,f30
	ctx.f30.f64 = double(float(ctx.f26.f64 * ctx.f30.f64));
	// fmadds f11,f23,f4,f11
	ctx.f11.f64 = double(float(ctx.f23.f64 * ctx.f4.f64 + ctx.f11.f64));
	// fmuls f1,f2,f1
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// fmuls f13,f13,f2
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f2.f64));
	// fmuls f0,f0,f2
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// fmadds f29,f10,f4,f29
	ctx.f29.f64 = double(float(ctx.f10.f64 * ctx.f4.f64 + ctx.f29.f64));
	// fmsubs f2,f10,f2,f25
	ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f2.f64 - ctx.f25.f64));
	// fmadds f10,f10,f3,f28
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f3.f64 + ctx.f28.f64));
	// fmadds f28,f22,f4,f27
	ctx.f28.f64 = double(float(ctx.f22.f64 * ctx.f4.f64 + ctx.f27.f64));
	// fmuls f27,f3,f11
	ctx.f27.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// fadds f1,f21,f1
	ctx.f1.f64 = double(float(ctx.f21.f64 + ctx.f1.f64));
	// fadds f13,f30,f13
	ctx.f13.f64 = double(float(ctx.f30.f64 + ctx.f13.f64));
	// fmuls f30,f11,f4
	ctx.f30.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// fmuls f11,f7,f11
	ctx.f11.f64 = double(float(ctx.f7.f64 * ctx.f11.f64));
	// fadds f0,f24,f0
	ctx.f0.f64 = double(float(ctx.f24.f64 + ctx.f0.f64));
	// fnmsubs f10,f6,f4,f10
	ctx.f10.f64 = double(float(-(ctx.f6.f64 * ctx.f4.f64 - ctx.f10.f64)));
	// stfs f10,84(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f2,f22,f3,f2
	ctx.f2.f64 = double(float(-(ctx.f22.f64 * ctx.f3.f64 - ctx.f2.f64)));
	// fnmsubs f8,f8,f3,f28
	ctx.f8.f64 = double(float(-(ctx.f8.f64 * ctx.f3.f64 - ctx.f28.f64)));
	// fnmsubs f29,f22,f7,f29
	ctx.f29.f64 = double(float(-(ctx.f22.f64 * ctx.f7.f64 - ctx.f29.f64)));
	// stfs f29,80(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fadds f4,f1,f30
	ctx.f4.f64 = double(float(ctx.f1.f64 + ctx.f30.f64));
	// fadds f1,f13,f11
	ctx.f1.f64 = double(float(ctx.f13.f64 + ctx.f11.f64));
	// fadds f3,f0,f27
	ctx.f3.f64 = double(float(ctx.f0.f64 + ctx.f27.f64));
	// fnmsubs f0,f6,f7,f2
	ctx.f0.f64 = double(float(-(ctx.f6.f64 * ctx.f7.f64 - ctx.f2.f64)));
	// fmuls f13,f4,f31
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// fmuls f7,f1,f31
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// fmuls f11,f3,f31
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// fadds f6,f13,f12
	ctx.f6.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// stfs f6,96(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f3,f7,f5
	ctx.f3.f64 = double(float(ctx.f7.f64 + ctx.f5.f64));
	// stfs f3,104(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fadds f4,f11,f9
	ctx.f4.f64 = double(float(ctx.f11.f64 + ctx.f9.f64));
	// stfs f4,100(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stfs f8,88(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lwz r7,96(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stfs f0,92(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lwz r5,100(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r4,104(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stw r8,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r8.u32);
	// lwz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r3,12(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// lwz r11,4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// stw r6,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r6.u32);
	// stw r3,12(r9)
	PPC_STORE_U32(ctx.r9.u32 + 12, ctx.r3.u32);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r11.u32);
	// stw r10,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r10.u32);
	// stw r7,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r7.u32);
	// stw r5,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r5.u32);
	// stw r4,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r4.u32);
	// b 0x830be420
	goto loc_830BE420;
loc_830BE2C0:
	// lfs f13,4(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lfs f11,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// lfs f5,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f5.f64 = double(temp.f32);
	// lfs f0,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fmsubs f4,f5,f5,f30
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f5.f64 - ctx.f30.f64));
	// lfs f6,16(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f8,24(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 24);
	ctx.f8.f64 = double(temp.f32);
	// addi r10,r1,172
	ctx.r10.s64 = ctx.r1.s64 + 172;
	// lfs f7,20(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	ctx.f7.f64 = double(temp.f32);
	// lfs f3,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,12(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f3,f11
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// fmuls f28,f2,f11
	ctx.f28.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// lfs f27,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f30,f2,f0
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// lfs f29,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f24,f6,f10
	ctx.f24.f64 = double(float(ctx.f6.f64 * ctx.f10.f64));
	// lfs f25,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f22,f7,f9
	ctx.f22.f64 = double(float(ctx.f7.f64 * ctx.f9.f64));
	// lfs f23,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f20,f8,f9
	ctx.f20.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// lfs f21,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f26,f8,f12
	ctx.f26.f64 = double(float(ctx.f8.f64 * ctx.f12.f64));
	// stw r27,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r27.u32);
	// fmuls f17,f7,f4
	ctx.f17.f64 = double(float(ctx.f7.f64 * ctx.f4.f64));
	// fmuls f18,f6,f4
	ctx.f18.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// fmuls f4,f8,f4
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// fmadds f1,f27,f5,f1
	ctx.f1.f64 = double(float(ctx.f27.f64 * ctx.f5.f64 + ctx.f1.f64));
	// fmuls f19,f3,f0
	ctx.f19.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmadds f28,f29,f5,f28
	ctx.f28.f64 = double(float(ctx.f29.f64 * ctx.f5.f64 + ctx.f28.f64));
	// fmsubs f24,f7,f12,f24
	ctx.f24.f64 = double(float(ctx.f7.f64 * ctx.f12.f64 - ctx.f24.f64));
	// fmsubs f8,f8,f10,f22
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f10.f64 - ctx.f22.f64));
	// fmadds f7,f7,f10,f20
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f10.f64 + ctx.f20.f64));
	// fmsubs f26,f6,f9,f26
	ctx.f26.f64 = double(float(ctx.f6.f64 * ctx.f9.f64 - ctx.f26.f64));
	// fmadds f30,f3,f5,f30
	ctx.f30.f64 = double(float(ctx.f3.f64 * ctx.f5.f64 + ctx.f30.f64));
	// fmsubs f22,f2,f5,f19
	ctx.f22.f64 = double(float(ctx.f2.f64 * ctx.f5.f64 - ctx.f19.f64));
	// fmuls f24,f24,f5
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f5.f64));
	// fmuls f8,f8,f5
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// fmadds f7,f6,f12,f7
	ctx.f7.f64 = double(float(ctx.f6.f64 * ctx.f12.f64 + ctx.f7.f64));
	// fmadds f6,f2,f13,f1
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f13.f64 + ctx.f1.f64));
	// fmuls f26,f26,f5
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f5.f64));
	// fmadds f5,f27,f0,f28
	ctx.f5.f64 = double(float(ctx.f27.f64 * ctx.f0.f64 + ctx.f28.f64));
	// fmadds f30,f29,f13,f30
	ctx.f30.f64 = double(float(ctx.f29.f64 * ctx.f13.f64 + ctx.f30.f64));
	// fnmsubs f2,f27,f13,f22
	ctx.f2.f64 = double(float(-(ctx.f27.f64 * ctx.f13.f64 - ctx.f22.f64)));
	// fadds f4,f4,f24
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f24.f64));
	// fadds f8,f18,f8
	ctx.f8.f64 = double(float(ctx.f18.f64 + ctx.f8.f64));
	// fmuls f12,f7,f12
	ctx.f12.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// fmuls f10,f10,f7
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f7.f64));
	// fmuls f9,f9,f7
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// fnmsubs f7,f29,f0,f6
	ctx.f7.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f6.f64)));
	// stfs f7,84(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f6,f3,f13,f5
	ctx.f6.f64 = double(float(-(ctx.f3.f64 * ctx.f13.f64 - ctx.f5.f64)));
	// stfs f6,88(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fnmsubs f1,f27,f11,f30
	ctx.f1.f64 = double(float(-(ctx.f27.f64 * ctx.f11.f64 - ctx.f30.f64)));
	// stfs f1,80(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fadds f1,f17,f26
	ctx.f1.f64 = double(float(ctx.f17.f64 + ctx.f26.f64));
	// fnmsubs f5,f29,f11,f2
	ctx.f5.f64 = double(float(-(ctx.f29.f64 * ctx.f11.f64 - ctx.f2.f64)));
	// stfs f5,92(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fadds f3,f8,f12
	ctx.f3.f64 = double(float(ctx.f8.f64 + ctx.f12.f64));
	// fadds f2,f1,f10
	ctx.f2.f64 = double(float(ctx.f1.f64 + ctx.f10.f64));
	// fadds f1,f4,f9
	ctx.f1.f64 = double(float(ctx.f4.f64 + ctx.f9.f64));
	// fmuls f0,f3,f31
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// fmuls f13,f2,f31
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f12,f1,f31
	ctx.f12.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// fadds f11,f25,f0
	ctx.f11.f64 = double(float(ctx.f25.f64 + ctx.f0.f64));
	// stfs f11,96(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lwz r5,96(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// fadds f10,f23,f13
	ctx.f10.f64 = double(float(ctx.f23.f64 + ctx.f13.f64));
	// stfs f10,100(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f9,f21,f12
	ctx.f9.f64 = double(float(ctx.f21.f64 + ctx.f12.f64));
	// stfs f9,104(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// lwz r6,12(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r7,8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r4,100(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r3,104(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stw r7,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r7.u32);
	// stw r6,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r6.u32);
	// stw r5,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r5.u32);
	// stw r4,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r4.u32);
	// stw r3,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r3.u32);
loc_830BE420:
	// lwz r11,152(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 152);
	// addi r5,r1,192
	ctx.r5.s64 = ctx.r1.s64 + 192;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r11.u32);
	// lwz r3,336(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 336);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,76(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 76);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830BE444;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// stw r3,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r3.u32);
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// li r9,7
	ctx.r9.s64 = 7;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_830BE458:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830be458
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830BE458;
	// lwz r11,268(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 268);
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// lwz r31,348(r29)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// lwz r3,1412(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1412);
	// bl 0x8315fb80
	ctx.lr = 0x830BE480;
	sub_8315FB80(ctx, base);
	// stwx r3,r31,r28
	PPC_STORE_U32(ctx.r31.u32 + ctx.r28.u32, ctx.r3.u32);
	// lwz r11,348(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// add r11,r11,r28
	ctx.r11.u64 = ctx.r11.u64 + ctx.r28.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830be4ec
	if (ctx.cr6.eq) goto loc_830BE4EC;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830be4ec
	if (ctx.cr6.eq) goto loc_830BE4EC;
	// lwz r29,20(r11)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// lwz r31,16(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// cmplw cr6,r31,r29
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r29.u32, ctx.xer);
	// beq cr6,0x830be4ec
	if (ctx.cr6.eq) goto loc_830BE4EC;
loc_830BE4B4:
	// lwz r30,0(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830BE4CC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x830be4e0
	if (ctx.cr6.eq) goto loc_830BE4E0;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x830bdd70
	ctx.lr = 0x830BE4E0;
	sub_830BDD70(ctx, base);
loc_830BE4E0:
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// cmplw cr6,r31,r29
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r29.u32, ctx.xer);
	// bne cr6,0x830be4b4
	if (!ctx.cr6.eq) goto loc_830BE4B4;
loc_830BE4EC:
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// bl 0x830b95f8
	ctx.lr = 0x830BE4F4;
	sub_830B95F8(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lwz r11,40(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 40);
	// beq cr6,0x830be510
	if (ctx.cr6.eq) goto loc_830BE510;
	// ori r10,r11,1
	ctx.r10.u64 = ctx.r11.u64 | 1;
	// stw r10,40(r25)
	PPC_STORE_U32(ctx.r25.u32 + 40, ctx.r10.u32);
	// b 0x830becdc
	goto loc_830BECDC;
loc_830BE510:
	// rlwinm r10,r11,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r10,40(r25)
	PPC_STORE_U32(ctx.r25.u32 + 40, ctx.r10.u32);
	// b 0x830becdc
	goto loc_830BECDC;
loc_830BE51C:
	// lwz r11,40(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 40);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x830bec34
	if (!ctx.cr6.eq) goto loc_830BEC34;
	// ori r11,r11,1
	ctx.r11.u64 = ctx.r11.u64 | 1;
	// lwz r10,36(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 36);
	// stw r11,40(r25)
	PPC_STORE_U32(ctx.r25.u32 + 40, ctx.r11.u32);
	// lwz r29,104(r10)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + 104);
	// lwz r26,100(r10)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r10.u32 + 100);
	// rlwinm r28,r26,4,0,27
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r11,348(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// add r11,r28,r11
	ctx.r11.u64 = ctx.r28.u64 + ctx.r11.u64;
	// addi r10,r11,8
	ctx.r10.s64 = ctx.r11.s64 + 8;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// stw r9,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r9.u32);
	// lwz r8,344(r29)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r29.u32 + 344);
	// cmpwi cr6,r8,2
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 2, ctx.xer);
	// bne cr6,0x830bec34
	if (!ctx.cr6.eq) goto loc_830BEC34;
	// lwz r11,348(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// add r11,r28,r11
	ctx.r11.u64 = ctx.r28.u64 + ctx.r11.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x830bec34
	if (!ctx.cr6.gt) goto loc_830BEC34;
	// lwz r11,0(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x830bec34
	if (!ctx.cr6.eq) goto loc_830BEC34;
	// lwz r11,348(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// li r27,0
	ctx.r27.s64 = 0;
	// add r11,r28,r11
	ctx.r11.u64 = ctx.r28.u64 + ctx.r11.u64;
	// stb r27,12(r11)
	PPC_STORE_U8(ctx.r11.u32 + 12, ctx.r27.u8);
	// lwz r10,348(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// lwzx r9,r28,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r10.u32);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x830bec34
	if (!ctx.cr6.eq) goto loc_830BEC34;
	// lfs f13,116(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 116);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f0,112(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 112);
	ctx.f0.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f5,124(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 124);
	ctx.f5.f64 = double(temp.f32);
	// lfs f11,120(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 120);
	ctx.f11.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f6,176(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 176);
	ctx.f6.f64 = double(temp.f32);
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// lfs f30,6380(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6380);
	ctx.f30.f64 = double(temp.f32);
	// addi r31,r29,112
	ctx.r31.s64 = ctx.r29.s64 + 112;
	// lfs f8,180(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 180);
	ctx.f8.f64 = double(temp.f32);
	// fmsubs f4,f5,f5,f30
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f5.f64 - ctx.f30.f64));
	// lfs f7,184(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 184);
	ctx.f7.f64 = double(temp.f32);
	// addi r30,r29,160
	ctx.r30.s64 = ctx.r29.s64 + 160;
	// lfs f3,160(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 160);
	ctx.f3.f64 = double(temp.f32);
	// addi r9,r1,124
	ctx.r9.s64 = ctx.r1.s64 + 124;
	// lfs f2,172(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 172);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f3,f11
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// fmuls f20,f2,f11
	ctx.f20.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// lfs f26,164(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 164);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f25,f10,f6
	ctx.f25.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// lfs f28,168(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 168);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f24,f10,f8
	ctx.f24.f64 = double(float(ctx.f10.f64 * ctx.f8.f64));
	// lfs f31,7676(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 7676);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f27,f12,f7
	ctx.f27.f64 = double(float(ctx.f12.f64 * ctx.f7.f64));
	// lfs f23,132(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 132);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f22,f9,f8
	ctx.f22.f64 = double(float(ctx.f9.f64 * ctx.f8.f64));
	// lfs f21,136(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 136);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f18,f3,f0
	ctx.f18.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// lfs f19,128(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 128);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f17,f4,f8
	ctx.f17.f64 = double(float(ctx.f4.f64 * ctx.f8.f64));
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// fmuls f16,f4,f7
	ctx.f16.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// fmuls f29,f2,f0
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmadds f1,f26,f5,f1
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f5.f64 + ctx.f1.f64));
	// fmuls f4,f4,f6
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f6.f64));
	// fmsubs f8,f12,f8,f25
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f8.f64 - ctx.f25.f64));
	// fmadds f25,f9,f7,f24
	ctx.f25.f64 = double(float(ctx.f9.f64 * ctx.f7.f64 + ctx.f24.f64));
	// fmsubs f27,f9,f6,f27
	ctx.f27.f64 = double(float(ctx.f9.f64 * ctx.f6.f64 - ctx.f27.f64));
	// fmadds f24,f28,f5,f20
	ctx.f24.f64 = double(float(ctx.f28.f64 * ctx.f5.f64 + ctx.f20.f64));
	// fmsubs f7,f10,f7,f22
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f7.f64 - ctx.f22.f64));
	// fmsubs f22,f2,f5,f18
	ctx.f22.f64 = double(float(ctx.f2.f64 * ctx.f5.f64 - ctx.f18.f64));
	// fmadds f29,f3,f5,f29
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f5.f64 + ctx.f29.f64));
	// fmadds f2,f2,f13,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f13.f64 + ctx.f1.f64));
	// fmuls f8,f8,f5
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// fmadds f6,f12,f6,f25
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f6.f64 + ctx.f25.f64));
	// fmuls f27,f27,f5
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f5.f64));
	// fmadds f1,f26,f0,f24
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f0.f64 + ctx.f24.f64));
	// fmuls f5,f7,f5
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// fmadds f7,f28,f13,f29
	ctx.f7.f64 = double(float(ctx.f28.f64 * ctx.f13.f64 + ctx.f29.f64));
	// fnmsubs f29,f26,f13,f22
	ctx.f29.f64 = double(float(-(ctx.f26.f64 * ctx.f13.f64 - ctx.f22.f64)));
	// fnmsubs f2,f28,f0,f2
	ctx.f2.f64 = double(float(-(ctx.f28.f64 * ctx.f0.f64 - ctx.f2.f64)));
	// stfs f2,84(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fadds f8,f16,f8
	ctx.f8.f64 = double(float(ctx.f16.f64 + ctx.f8.f64));
	// fmuls f10,f10,f6
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// fmuls f9,f9,f6
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f6.f64));
	// fadds f27,f17,f27
	ctx.f27.f64 = double(float(ctx.f17.f64 + ctx.f27.f64));
	// fmuls f6,f6,f12
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// fnmsubs f1,f3,f13,f1
	ctx.f1.f64 = double(float(-(ctx.f3.f64 * ctx.f13.f64 - ctx.f1.f64)));
	// stfs f1,88(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fadds f5,f4,f5
	ctx.f5.f64 = double(float(ctx.f4.f64 + ctx.f5.f64));
	// fnmsubs f4,f26,f11,f7
	ctx.f4.f64 = double(float(-(ctx.f26.f64 * ctx.f11.f64 - ctx.f7.f64)));
	// stfs f4,80(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fnmsubs f0,f28,f11,f29
	ctx.f0.f64 = double(float(-(ctx.f28.f64 * ctx.f11.f64 - ctx.f29.f64)));
	// stfs f0,92(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fadds f12,f9,f8
	ctx.f12.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
	// fadds f13,f10,f27
	ctx.f13.f64 = double(float(ctx.f10.f64 + ctx.f27.f64));
	// fadds f11,f5,f6
	ctx.f11.f64 = double(float(ctx.f5.f64 + ctx.f6.f64));
	// fmuls f9,f12,f31
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// fmuls f10,f13,f31
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// fmuls f8,f11,f31
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// fadds f6,f21,f9
	ctx.f6.f64 = double(float(ctx.f21.f64 + ctx.f9.f64));
	// stfs f6,104(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fadds f7,f23,f10
	ctx.f7.f64 = double(float(ctx.f23.f64 + ctx.f10.f64));
	// stfs f7,100(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f5,f19,f8
	ctx.f5.f64 = double(float(ctx.f19.f64 + ctx.f8.f64));
	// stfs f5,96(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lwz r6,104(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r7,100(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r5,0(r8)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// lwz r4,4(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r11,8(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// lwz r10,12(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lwz r8,96(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r5,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r5.u32);
	// stw r4,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r4.u32);
	// stw r11,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r11.u32);
	// stw r10,12(r9)
	PPC_STORE_U32(ctx.r9.u32 + 12, ctx.r10.u32);
	// stw r8,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r8.u32);
	// stw r7,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r7.u32);
	// stw r6,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r6.u32);
	// bl 0x8315fb70
	ctx.lr = 0x830BE724;
	sub_8315FB70(ctx, base);
	// li r7,10
	ctx.r7.s64 = 10;
	// stw r7,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r7.u32);
	// lwz r11,264(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 264);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830bea08
	if (ctx.cr6.eq) goto loc_830BEA08;
	// lfs f0,216(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 216);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r11,216
	ctx.r10.s64 = ctx.r11.s64 + 216;
	// lfs f5,240(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 240);
	ctx.f5.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f13,220(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 220);
	ctx.f13.f64 = double(temp.f32);
	// fneg f3,f5
	ctx.f3.u64 = ctx.f5.u64 ^ 0x8000000000000000;
	// lfs f11,224(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 224);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// lfs f9,232(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 232);
	ctx.f9.f64 = double(temp.f32);
	// fmr f8,f11
	ctx.f8.f64 = ctx.f11.f64;
	// lfs f7,236(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 236);
	ctx.f7.f64 = double(temp.f32);
	// fneg f6,f9
	ctx.f6.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// fneg f4,f7
	ctx.f4.u64 = ctx.f7.u64 ^ 0x8000000000000000;
	// lfs f2,228(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 228);
	ctx.f2.f64 = double(temp.f32);
	// fneg f1,f0
	ctx.f1.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lfs f0,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f0.f64 = double(temp.f32);
	// fneg f11,f11
	ctx.f11.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// lfs f9,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f9.f64 = double(temp.f32);
	// fneg f7,f13
	ctx.f7.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// lfs f5,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f5.f64 = double(temp.f32);
	// fmsubs f28,f2,f2,f30
	ctx.f28.f64 = double(float(ctx.f2.f64 * ctx.f2.f64 - ctx.f30.f64));
	// lfs f29,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f29.f64 = double(temp.f32);
	// fmr f13,f2
	ctx.f13.f64 = ctx.f2.f64;
	// lfs f27,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f24,f3,f12
	ctx.f24.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// fmuls f21,f8,f3
	ctx.f21.f64 = double(float(ctx.f8.f64 * ctx.f3.f64));
	// fmuls f26,f10,f6
	ctx.f26.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// fmuls f25,f8,f4
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// fmuls f23,f1,f0
	ctx.f23.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f20,f11,f0
	ctx.f20.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f19,f7,f5
	ctx.f19.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// fmuls f22,f11,f9
	ctx.f22.f64 = double(float(ctx.f11.f64 * ctx.f9.f64));
	// fmuls f17,f28,f6
	ctx.f17.f64 = double(float(ctx.f28.f64 * ctx.f6.f64));
	// fmsubs f24,f8,f6,f24
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f6.f64 - ctx.f24.f64));
	// fmsubs f18,f13,f13,f30
	ctx.f18.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 - ctx.f30.f64));
	// fmadds f6,f12,f6,f21
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f6.f64 + ctx.f21.f64));
	// fmsubs f26,f4,f12,f26
	ctx.f26.f64 = double(float(ctx.f4.f64 * ctx.f12.f64 - ctx.f26.f64));
	// fmsubs f25,f10,f3,f25
	ctx.f25.f64 = double(float(ctx.f10.f64 * ctx.f3.f64 - ctx.f25.f64));
	// fmsubs f23,f11,f5,f23
	ctx.f23.f64 = double(float(ctx.f11.f64 * ctx.f5.f64 - ctx.f23.f64));
	// fmadds f21,f7,f9,f20
	ctx.f21.f64 = double(float(ctx.f7.f64 * ctx.f9.f64 + ctx.f20.f64));
	// fmsubs f20,f1,f9,f19
	ctx.f20.f64 = double(float(ctx.f1.f64 * ctx.f9.f64 - ctx.f19.f64));
	// fmsubs f22,f7,f0,f22
	ctx.f22.f64 = double(float(ctx.f7.f64 * ctx.f0.f64 - ctx.f22.f64));
	// fmuls f19,f4,f28
	ctx.f19.f64 = double(float(ctx.f4.f64 * ctx.f28.f64));
	// fmuls f3,f3,f28
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f28.f64));
	// fmuls f28,f18,f5
	ctx.f28.f64 = double(float(ctx.f18.f64 * ctx.f5.f64));
	// fmadds f6,f10,f4,f6
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f4.f64 + ctx.f6.f64));
	// fmuls f26,f26,f2
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f2.f64));
	// fmuls f25,f25,f2
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f2.f64));
	// fmuls f4,f24,f2
	ctx.f4.f64 = double(float(ctx.f24.f64 * ctx.f2.f64));
	// fmadds f5,f1,f5,f21
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f5.f64 + ctx.f21.f64));
	// fmuls f9,f18,f9
	ctx.f9.f64 = double(float(ctx.f18.f64 * ctx.f9.f64));
	// fmuls f2,f23,f13
	ctx.f2.f64 = double(float(ctx.f23.f64 * ctx.f13.f64));
	// fmuls f0,f18,f0
	ctx.f0.f64 = double(float(ctx.f18.f64 * ctx.f0.f64));
	// fmuls f24,f13,f22
	ctx.f24.f64 = double(float(ctx.f13.f64 * ctx.f22.f64));
	// fmuls f23,f20,f13
	ctx.f23.f64 = double(float(ctx.f20.f64 * ctx.f13.f64));
	// fmuls f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// fsubs f3,f3,f26
	ctx.f3.f64 = double(float(ctx.f3.f64 - ctx.f26.f64));
	// fmuls f10,f10,f6
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// fmuls f8,f8,f6
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f6.f64));
	// fsubs f26,f17,f25
	ctx.f26.f64 = double(float(ctx.f17.f64 - ctx.f25.f64));
	// fmuls f6,f5,f1
	ctx.f6.f64 = double(float(ctx.f5.f64 * ctx.f1.f64));
	// fadds f2,f9,f2
	ctx.f2.f64 = double(float(ctx.f9.f64 + ctx.f2.f64));
	// fmuls f25,f7,f5
	ctx.f25.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// fsubs f4,f19,f4
	ctx.f4.f64 = double(float(ctx.f19.f64 - ctx.f4.f64));
	// fmuls f5,f11,f5
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f5.f64));
	// fadds f9,f28,f24
	ctx.f9.f64 = double(float(ctx.f28.f64 + ctx.f24.f64));
	// fadds f0,f0,f23
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f23.f64));
	// fadds f8,f3,f8
	ctx.f8.f64 = double(float(ctx.f3.f64 + ctx.f8.f64));
	// fadds f12,f26,f12
	ctx.f12.f64 = double(float(ctx.f26.f64 + ctx.f12.f64));
	// fadds f3,f2,f25
	ctx.f3.f64 = double(float(ctx.f2.f64 + ctx.f25.f64));
	// fadds f4,f4,f10
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f10.f64));
	// fadds f2,f9,f6
	ctx.f2.f64 = double(float(ctx.f9.f64 + ctx.f6.f64));
	// fadds f0,f0,f5
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f5.f64));
	// fmuls f10,f8,f31
	ctx.f10.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fmuls f12,f12,f31
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// fmuls f8,f3,f31
	ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// lfs f3,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f9,f4,f31
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// fmuls f4,f11,f29
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f29.f64));
	// fmuls f6,f2,f31
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f5,f0,f31
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fmuls f2,f13,f27
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f27.f64));
	// lfs f0,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f28,f11,f27
	ctx.f28.f64 = double(float(ctx.f11.f64 * ctx.f27.f64));
	// lfs f23,16(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f25,f1,f27
	ctx.f25.f64 = double(float(ctx.f1.f64 * ctx.f27.f64));
	// lfs f24,20(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f4,f1,f3,f4
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f3.f64 + ctx.f4.f64));
	// lfs f26,24(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 24);
	ctx.f26.f64 = double(temp.f32);
	// fadds f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 + ctx.f12.f64));
	// lfs f6,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// fadds f9,f8,f9
	ctx.f9.f64 = double(float(ctx.f8.f64 + ctx.f9.f64));
	// lfs f8,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f22,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f22.f64 = double(temp.f32);
	// fadds f5,f5,f10
	ctx.f5.f64 = double(float(ctx.f5.f64 + ctx.f10.f64));
	// lfs f10,12(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// lwz r8,184(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 184);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r9,r1,236
	ctx.r9.s64 = ctx.r1.s64 + 236;
	// fmadds f2,f1,f29,f2
	ctx.f2.f64 = double(float(ctx.f1.f64 * ctx.f29.f64 + ctx.f2.f64));
	// fmadds f28,f13,f3,f28
	ctx.f28.f64 = double(float(ctx.f13.f64 * ctx.f3.f64 + ctx.f28.f64));
	// fmsubs f25,f13,f29,f25
	ctx.f25.f64 = double(float(ctx.f13.f64 * ctx.f29.f64 - ctx.f25.f64));
	// fmadds f13,f13,f0,f4
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f0.f64 + ctx.f4.f64));
	// fmadds f4,f7,f0,f2
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f0.f64 + ctx.f2.f64));
	// fmadds f2,f7,f29,f28
	ctx.f2.f64 = double(float(ctx.f7.f64 * ctx.f29.f64 + ctx.f28.f64));
	// fnmsubs f29,f7,f3,f25
	ctx.f29.f64 = double(float(-(ctx.f7.f64 * ctx.f3.f64 - ctx.f25.f64)));
	// fnmsubs f13,f7,f27,f13
	ctx.f13.f64 = double(float(-(ctx.f7.f64 * ctx.f27.f64 - ctx.f13.f64)));
	// fnmsubs f7,f11,f3,f4
	ctx.f7.f64 = double(float(-(ctx.f11.f64 * ctx.f3.f64 - ctx.f4.f64)));
	// fnmsubs f4,f1,f0,f2
	ctx.f4.f64 = double(float(-(ctx.f1.f64 * ctx.f0.f64 - ctx.f2.f64)));
	// fnmsubs f3,f11,f0,f29
	ctx.f3.f64 = double(float(-(ctx.f11.f64 * ctx.f0.f64 - ctx.f29.f64)));
	// fmuls f2,f13,f24
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f24.f64));
	// fmuls f28,f13,f6
	ctx.f28.f64 = double(float(ctx.f13.f64 * ctx.f6.f64));
	// fmuls f1,f7,f26
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f26.f64));
	// fmuls f0,f4,f23
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f23.f64));
	// fmuls f11,f4,f24
	ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f24.f64));
	// fmsubs f30,f3,f3,f30
	ctx.f30.f64 = double(float(ctx.f3.f64 * ctx.f3.f64 - ctx.f30.f64));
	// fmuls f27,f3,f8
	ctx.f27.f64 = double(float(ctx.f3.f64 * ctx.f8.f64));
	// fmsubs f2,f4,f26,f2
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f26.f64 - ctx.f2.f64));
	// fmuls f29,f3,f6
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f6.f64));
	// fmadds f28,f3,f22,f28
	ctx.f28.f64 = double(float(ctx.f3.f64 * ctx.f22.f64 + ctx.f28.f64));
	// fmuls f25,f7,f6
	ctx.f25.f64 = double(float(ctx.f7.f64 * ctx.f6.f64));
	// fmsubs f1,f13,f23,f1
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f23.f64 - ctx.f1.f64));
	// fmsubs f0,f7,f24,f0
	ctx.f0.f64 = double(float(ctx.f7.f64 * ctx.f24.f64 - ctx.f0.f64));
	// fmadds f11,f7,f23,f11
	ctx.f11.f64 = double(float(ctx.f7.f64 * ctx.f23.f64 + ctx.f11.f64));
	// fmuls f23,f30,f23
	ctx.f23.f64 = double(float(ctx.f30.f64 * ctx.f23.f64));
	// fmuls f24,f30,f24
	ctx.f24.f64 = double(float(ctx.f30.f64 * ctx.f24.f64));
	// fmadds f27,f7,f22,f27
	ctx.f27.f64 = double(float(ctx.f7.f64 * ctx.f22.f64 + ctx.f27.f64));
	// fmuls f30,f30,f26
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f26.f64));
	// fmuls f2,f2,f3
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f3.f64));
	// fmadds f29,f7,f10,f29
	ctx.f29.f64 = double(float(ctx.f7.f64 * ctx.f10.f64 + ctx.f29.f64));
	// fmadds f28,f4,f10,f28
	ctx.f28.f64 = double(float(ctx.f4.f64 * ctx.f10.f64 + ctx.f28.f64));
	// fmuls f1,f1,f3
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f3.f64));
	// fmuls f0,f0,f3
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f3.f64));
	// fmadds f11,f13,f26,f11
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f26.f64 + ctx.f11.f64));
	// fmsubs f3,f3,f10,f25
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f10.f64 - ctx.f25.f64));
	// fmadds f10,f13,f10,f27
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f10.f64 + ctx.f27.f64));
	// fadds f2,f23,f2
	ctx.f2.f64 = double(float(ctx.f23.f64 + ctx.f2.f64));
	// fmadds f29,f4,f8,f29
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f29.f64));
	// fadds f1,f24,f1
	ctx.f1.f64 = double(float(ctx.f24.f64 + ctx.f1.f64));
	// fadds f0,f30,f0
	ctx.f0.f64 = double(float(ctx.f30.f64 + ctx.f0.f64));
	// fmuls f30,f11,f7
	ctx.f30.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// fmuls f27,f11,f4
	ctx.f27.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// fmuls f11,f11,f13
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fnmsubs f3,f4,f22,f3
	ctx.f3.f64 = double(float(-(ctx.f4.f64 * ctx.f22.f64 - ctx.f3.f64)));
	// fnmsubs f6,f4,f6,f10
	ctx.f6.f64 = double(float(-(ctx.f4.f64 * ctx.f6.f64 - ctx.f10.f64)));
	// fnmsubs f7,f7,f8,f28
	ctx.f7.f64 = double(float(-(ctx.f7.f64 * ctx.f8.f64 - ctx.f28.f64)));
	// stfs f7,84(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f29,f13,f22,f29
	ctx.f29.f64 = double(float(-(ctx.f13.f64 * ctx.f22.f64 - ctx.f29.f64)));
	// stfs f29,80(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fadds f4,f2,f30
	ctx.f4.f64 = double(float(ctx.f2.f64 + ctx.f30.f64));
	// fadds f2,f1,f27
	ctx.f2.f64 = double(float(ctx.f1.f64 + ctx.f27.f64));
	// fadds f1,f0,f11
	ctx.f1.f64 = double(float(ctx.f0.f64 + ctx.f11.f64));
	// fnmsubs f0,f13,f8,f3
	ctx.f0.f64 = double(float(-(ctx.f13.f64 * ctx.f8.f64 - ctx.f3.f64)));
	// fmuls f13,f4,f31
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// fmuls f11,f2,f31
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f10,f1,f31
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// fadds f8,f13,f12
	ctx.f8.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// stfs f8,96(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f4,f11,f9
	ctx.f4.f64 = double(float(ctx.f11.f64 + ctx.f9.f64));
	// stfs f4,100(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f3,f10,f5
	ctx.f3.f64 = double(float(ctx.f10.f64 + ctx.f5.f64));
	// stfs f3,104(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// stfs f6,88(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lwz r7,96(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stfs f0,92(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lwz r6,100(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r5,104(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stw r8,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r8.u32);
	// lwz r4,12(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// lwz r3,8(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lwz r11,4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r4,12(r9)
	PPC_STORE_U32(ctx.r9.u32 + 12, ctx.r4.u32);
	// stw r3,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r3.u32);
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r11.u32);
	// stw r7,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r7.u32);
	// stw r6,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, ctx.r6.u32);
	// stw r5,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, ctx.r5.u32);
	// b 0x830beb68
	goto loc_830BEB68;
loc_830BEA08:
	// lfs f0,0(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lfs f13,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f11,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// lfs f5,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f5.f64 = double(temp.f32);
	// lfs f8,24(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 24);
	ctx.f8.f64 = double(temp.f32);
	// fmsubs f4,f5,f5,f30
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f5.f64 - ctx.f30.f64));
	// lfs f2,12(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f2.f64 = double(temp.f32);
	// addi r10,r1,236
	ctx.r10.s64 = ctx.r1.s64 + 236;
	// lfs f7,20(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f30,f2,f0
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// lfs f6,16(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f28,f2,f11
	ctx.f28.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// lfs f3,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f1,f3,f11
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// lfs f29,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f19,f3,f0
	ctx.f19.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// lfs f27,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f26,f12,f8
	ctx.f26.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// lfs f25,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f22,f10,f7
	ctx.f22.f64 = double(float(ctx.f10.f64 * ctx.f7.f64));
	// lfs f23,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f24,f10,f6
	ctx.f24.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// lfs f21,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f20,f9,f7
	ctx.f20.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// stw r27,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r27.u32);
	// fmuls f18,f4,f7
	ctx.f18.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// fmuls f17,f4,f8
	ctx.f17.f64 = double(float(ctx.f4.f64 * ctx.f8.f64));
	// fmadds f30,f3,f5,f30
	ctx.f30.f64 = double(float(ctx.f3.f64 * ctx.f5.f64 + ctx.f30.f64));
	// fmuls f4,f4,f6
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f6.f64));
	// fmadds f1,f27,f5,f1
	ctx.f1.f64 = double(float(ctx.f27.f64 * ctx.f5.f64 + ctx.f1.f64));
	// fmadds f28,f29,f5,f28
	ctx.f28.f64 = double(float(ctx.f29.f64 * ctx.f5.f64 + ctx.f28.f64));
	// fmsubs f26,f9,f6,f26
	ctx.f26.f64 = double(float(ctx.f9.f64 * ctx.f6.f64 - ctx.f26.f64));
	// fmadds f6,f12,f6,f22
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f6.f64 + ctx.f22.f64));
	// fmsubs f7,f12,f7,f24
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f7.f64 - ctx.f24.f64));
	// fmsubs f24,f10,f8,f20
	ctx.f24.f64 = double(float(ctx.f10.f64 * ctx.f8.f64 - ctx.f20.f64));
	// fmsubs f22,f2,f5,f19
	ctx.f22.f64 = double(float(ctx.f2.f64 * ctx.f5.f64 - ctx.f19.f64));
	// fmadds f30,f29,f13,f30
	ctx.f30.f64 = double(float(ctx.f29.f64 * ctx.f13.f64 + ctx.f30.f64));
	// fmadds f2,f2,f13,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f13.f64 + ctx.f1.f64));
	// fmadds f1,f27,f0,f28
	ctx.f1.f64 = double(float(ctx.f27.f64 * ctx.f0.f64 + ctx.f28.f64));
	// fmuls f26,f5,f26
	ctx.f26.f64 = double(float(ctx.f5.f64 * ctx.f26.f64));
	// fmadds f6,f9,f8,f6
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f8.f64 + ctx.f6.f64));
	// fmuls f7,f5,f7
	ctx.f7.f64 = double(float(ctx.f5.f64 * ctx.f7.f64));
	// fmuls f5,f24,f5
	ctx.f5.f64 = double(float(ctx.f24.f64 * ctx.f5.f64));
	// fnmsubs f8,f27,f13,f22
	ctx.f8.f64 = double(float(-(ctx.f27.f64 * ctx.f13.f64 - ctx.f22.f64)));
	// fnmsubs f30,f27,f11,f30
	ctx.f30.f64 = double(float(-(ctx.f27.f64 * ctx.f11.f64 - ctx.f30.f64)));
	// stfs f30,80(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fnmsubs f3,f3,f13,f1
	ctx.f3.f64 = double(float(-(ctx.f3.f64 * ctx.f13.f64 - ctx.f1.f64)));
	// stfs f3,88(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fadds f30,f18,f26
	ctx.f30.f64 = double(float(ctx.f18.f64 + ctx.f26.f64));
	// fmuls f10,f6,f10
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f10.f64));
	// fmuls f9,f6,f9
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f9.f64));
	// fadds f7,f17,f7
	ctx.f7.f64 = double(float(ctx.f17.f64 + ctx.f7.f64));
	// fmuls f6,f6,f12
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// fadds f5,f4,f5
	ctx.f5.f64 = double(float(ctx.f4.f64 + ctx.f5.f64));
	// fnmsubs f4,f29,f0,f2
	ctx.f4.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f2.f64)));
	// stfs f4,84(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f2,f29,f11,f8
	ctx.f2.f64 = double(float(-(ctx.f29.f64 * ctx.f11.f64 - ctx.f8.f64)));
	// stfs f2,92(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fadds f1,f30,f10
	ctx.f1.f64 = double(float(ctx.f30.f64 + ctx.f10.f64));
	// fadds f0,f7,f9
	ctx.f0.f64 = double(float(ctx.f7.f64 + ctx.f9.f64));
	// fadds f13,f5,f6
	ctx.f13.f64 = double(float(ctx.f5.f64 + ctx.f6.f64));
	// fmuls f12,f1,f31
	ctx.f12.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// fmuls f11,f0,f31
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fmuls f10,f13,f31
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// fadds f9,f25,f12
	ctx.f9.f64 = double(float(ctx.f25.f64 + ctx.f12.f64));
	// stfs f9,100(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f8,f23,f11
	ctx.f8.f64 = double(float(ctx.f23.f64 + ctx.f11.f64));
	// stfs f8,104(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fadds f7,f21,f10
	ctx.f7.f64 = double(float(ctx.f21.f64 + ctx.f10.f64));
	// stfs f7,96(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// lwz r6,12(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r5,96(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r9,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r9.u32);
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// lwz r4,100(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r3,104(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stw r8,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r8.u32);
	// stw r6,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r6.u32);
	// stw r5,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r5.u32);
	// stw r4,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, ctx.r4.u32);
	// stw r3,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, ctx.r3.u32);
loc_830BEB68:
	// lwz r11,152(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 152);
	// addi r5,r1,256
	ctx.r5.s64 = ctx.r1.s64 + 256;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// stw r11,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, ctx.r11.u32);
	// lwz r3,336(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 336);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,76(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 76);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830BEB8C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// stw r3,252(r1)
	PPC_STORE_U32(ctx.r1.u32 + 252, ctx.r3.u32);
	// addi r10,r1,224
	ctx.r10.s64 = ctx.r1.s64 + 224;
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// li r9,7
	ctx.r9.s64 = 7;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_830BEBA0:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830beba0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830BEBA0;
	// lwz r11,268(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 268);
	// addi r4,r1,208
	ctx.r4.s64 = ctx.r1.s64 + 208;
	// lwz r31,348(r29)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// lwz r3,1412(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1412);
	// bl 0x8315fb80
	ctx.lr = 0x830BEBC8;
	sub_8315FB80(ctx, base);
	// stwx r3,r28,r31
	PPC_STORE_U32(ctx.r28.u32 + ctx.r31.u32, ctx.r3.u32);
	// lwz r11,348(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// add r11,r28,r11
	ctx.r11.u64 = ctx.r28.u64 + ctx.r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830bec34
	if (ctx.cr6.eq) goto loc_830BEC34;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830bec34
	if (ctx.cr6.eq) goto loc_830BEC34;
	// lwz r29,20(r11)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// lwz r31,16(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// cmplw cr6,r31,r29
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r29.u32, ctx.xer);
	// beq cr6,0x830bec34
	if (ctx.cr6.eq) goto loc_830BEC34;
loc_830BEBFC:
	// lwz r30,0(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830BEC14;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x830bec28
	if (ctx.cr6.eq) goto loc_830BEC28;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x830bdd70
	ctx.lr = 0x830BEC28;
	sub_830BDD70(ctx, base);
loc_830BEC28:
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// cmplw cr6,r31,r29
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r29.u32, ctx.xer);
	// bne cr6,0x830bebfc
	if (!ctx.cr6.eq) goto loc_830BEBFC;
loc_830BEC34:
	// lwz r11,40(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 40);
	// rlwinm r10,r11,30,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 30) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x830becdc
	if (!ctx.cr6.eq) goto loc_830BECDC;
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// bl 0x830b98b0
	ctx.lr = 0x830BEC4C;
	sub_830B98B0(ctx, base);
	// clrlwi r31,r3,24
	ctx.r31.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x830bec8c
	if (!ctx.cr6.eq) goto loc_830BEC8C;
	// lwz r11,36(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 36);
	// lwz r3,44(r25)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r25.u32 + 44);
	// lwz r4,104(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 104);
	// bl 0x83097de8
	ctx.lr = 0x830BEC68;
	sub_83097DE8(ctx, base);
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x830bec84
	if (!ctx.cr6.eq) goto loc_830BEC84;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// bl 0x830b9950
	ctx.lr = 0x830BEC80;
	sub_830B9950(ctx, base);
	// b 0x830becdc
	goto loc_830BECDC;
loc_830BEC84:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x830becb8
	if (ctx.cr6.eq) goto loc_830BECB8;
loc_830BEC8C:
	// lwz r11,36(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 36);
	// lwz r3,44(r25)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r25.u32 + 44);
	// lwz r4,104(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 104);
	// bl 0x83097de8
	ctx.lr = 0x830BEC9C;
	sub_83097DE8(ctx, base);
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830becb8
	if (ctx.cr6.eq) goto loc_830BECB8;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// bl 0x830b9950
	ctx.lr = 0x830BECB4;
	sub_830B9950(ctx, base);
	// b 0x830becdc
	goto loc_830BECDC;
loc_830BECB8:
	// lwz r11,36(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 36);
	// lwz r3,44(r25)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r25.u32 + 44);
	// lwz r4,104(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 104);
	// bl 0x83097de8
	ctx.lr = 0x830BECC8;
	sub_83097DE8(ctx, base);
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x830becdc
	if (!ctx.cr6.eq) goto loc_830BECDC;
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// bl 0x830b95f8
	ctx.lr = 0x830BECDC;
	sub_830B95F8(ctx, base);
loc_830BECDC:
	// lwz r11,40(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 40);
	// clrlwi r3,r11,31
	ctx.r3.u64 = ctx.r11.u32 & 0x1;
	// addi r1,r1,464
	ctx.r1.s64 = ctx.r1.s64 + 464;
	// addi r12,r1,-64
	ctx.r12.s64 = ctx.r1.s64 + -64;
	// bl 0x82cb6b04
	ctx.lr = 0x830BECF0;
	__restfpr_16(ctx, base);
	// b 0x82cb112c
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830BECF4"))) PPC_WEAK_FUNC(sub_830BECF4);
PPC_FUNC_IMPL(__imp__sub_830BECF4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830BECF8"))) PPC_WEAK_FUNC(sub_830BECF8);
PPC_FUNC_IMPL(__imp__sub_830BECF8) {
	PPC_FUNC_PROLOGUE();
	// b 0x830bdd70
	sub_830BDD70(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830BECFC"))) PPC_WEAK_FUNC(sub_830BECFC);
PPC_FUNC_IMPL(__imp__sub_830BECFC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830BED00"))) PPC_WEAK_FUNC(sub_830BED00);
PPC_FUNC_IMPL(__imp__sub_830BED00) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// addi r31,r30,348
	ctx.r31.s64 = ctx.r30.s64 + 348;
	// addi r10,r11,4888
	ctx.r10.s64 = ctx.r11.s64 + 4888;
	// stw r10,0(r30)
	PPC_STORE_U32(ctx.r30.u32 + 0, ctx.r10.u32);
	// lwz r4,348(r30)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r30.u32 + 348);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x830bed4c
	if (ctx.cr6.eq) goto loc_830BED4C;
	// lis r11,-31901
	ctx.r11.s64 = -2090663936;
	// lwz r3,-32308(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -32308);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,20(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830BED4C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830BED4C:
	// li r11,0
	ctx.r11.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// stw r11,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r11.u32);
	// stw r11,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r11.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// bl 0x83088de0
	ctx.lr = 0x830BED64;
	sub_83088DE0(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830BED7C"))) PPC_WEAK_FUNC(sub_830BED7C);
PPC_FUNC_IMPL(__imp__sub_830BED7C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830BED80"))) PPC_WEAK_FUNC(sub_830BED80);
PPC_FUNC_IMPL(__imp__sub_830BED80) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e0
	ctx.lr = 0x830BED88;
	__savegprlr_26(ctx, base);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// mr r26,r5
	ctx.r26.u64 = ctx.r5.u64;
	// mr r29,r6
	ctx.r29.u64 = ctx.r6.u64;
	// lwz r30,268(r28)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r28.u32 + 268);
	// lwz r3,308(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 308);
	// bl 0x83073d18
	ctx.lr = 0x830BEDA8;
	sub_83073D18(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// lfs f0,0(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lfs f12,8(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// lfs f11,12(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// lfs f10,16(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// lfs f9,20(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 20);
	ctx.f9.f64 = double(temp.f32);
	// mr r5,r26
	ctx.r5.u64 = ctx.r26.u64;
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// stfs f13,100(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stfs f12,104(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// addi r9,r31,1168
	ctx.r9.s64 = ctx.r31.s64 + 1168;
	// stfs f11,108(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// addi r8,r31,1016
	ctx.r8.s64 = ctx.r31.s64 + 1016;
	// stfs f10,112(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// stfs f9,116(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// bl 0x830bb548
	ctx.lr = 0x830BEE08;
	sub_830BB548(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830bee48
	if (ctx.cr6.eq) goto loc_830BEE48;
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830BEE30;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lwz r3,308(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 308);
	// bl 0x83073de8
	ctx.lr = 0x830BEE3C;
	sub_83073DE8(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82cb1130
	__restgprlr_26(ctx, base);
	return;
loc_830BEE48:
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lwz r3,308(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 308);
	// bl 0x83073de8
	ctx.lr = 0x830BEE54;
	sub_83073DE8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82cb1130
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830BEE60"))) PPC_WEAK_FUNC(sub_830BEE60);
PPC_FUNC_IMPL(__imp__sub_830BEE60) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e8
	ctx.lr = 0x830BEE68;
	__savegprlr_28(ctx, base);
	// stfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.f31.u64);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// bl 0x830891c0
	ctx.lr = 0x830BEE78;
	sub_830891C0(ctx, base);
	// lwz r11,352(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 352);
	// addi r30,r31,348
	ctx.r30.s64 = ctx.r31.s64 + 348;
	// lwz r10,348(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 348);
	// li r28,0
	ctx.r28.s64 = 0;
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi. r8,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 4;
	ctx.cr0.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// beq 0x830bef94
	if (ctx.cr0.eq) goto loc_830BEF94;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// li r29,0
	ctx.r29.s64 = 0;
	// lfs f31,6380(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6380);
	ctx.f31.f64 = double(temp.f32);
loc_830BEEA0:
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// add r11,r29,r11
	ctx.r11.u64 = ctx.r29.u64 + ctx.r11.u64;
	// lwz r31,4(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r5,100(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 100);
	// lwz r4,104(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 104);
	// bl 0x830bd6d0
	ctx.lr = 0x830BEEBC;
	sub_830BD6D0(ctx, base);
	// lfs f0,96(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// lwz r3,72(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	// lfs f13,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// lfs f10,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f10.f64 = double(temp.f32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// lfs f9,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f9.f64 = double(temp.f32);
	// fadds f8,f10,f12
	ctx.f8.f64 = double(float(ctx.f10.f64 + ctx.f12.f64));
	// lfs f7,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f7.f64 = double(temp.f32);
	// fadds f6,f7,f9
	ctx.f6.f64 = double(float(ctx.f7.f64 + ctx.f9.f64));
	// fmuls f5,f11,f31
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// stfs f5,80(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmuls f4,f8,f31
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// stfs f4,84(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmuls f3,f6,f31
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// stfs f3,88(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// beq cr6,0x830bef18
	if (ctx.cr6.eq) goto loc_830BEF18;
	// lwz r11,88(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// li r4,0
	ctx.r4.s64 = 0;
	// ld r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// rldicr r6,r11,32,63
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// bl 0x8315f360
	ctx.lr = 0x830BEF18;
	sub_8315F360(ctx, base);
loc_830BEF18:
	// lfs f0,108(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f0.f64 = double(temp.f32);
	// lwz r3,72(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	// lfs f13,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f11,f0,f13
	ctx.f11.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// lfs f10,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f10.f64 = double(temp.f32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// lfs f9,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f9.f64 = double(temp.f32);
	// fsubs f8,f12,f10
	ctx.f8.f64 = double(float(ctx.f12.f64 - ctx.f10.f64));
	// lfs f7,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f7.f64 = double(temp.f32);
	// fsubs f6,f9,f7
	ctx.f6.f64 = double(float(ctx.f9.f64 - ctx.f7.f64));
	// fmuls f5,f11,f31
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// stfs f5,80(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmuls f4,f8,f31
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// stfs f4,84(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmuls f3,f6,f31
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// stfs f3,88(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// beq cr6,0x830bef74
	if (ctx.cr6.eq) goto loc_830BEF74;
	// lwz r11,88(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// li r4,1
	ctx.r4.s64 = 1;
	// ld r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// rldicr r6,r11,32,63
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// bl 0x8315f360
	ctx.lr = 0x830BEF74;
	sub_8315F360(ctx, base);
loc_830BEF74:
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// addi r29,r29,16
	ctx.r29.s64 = ctx.r29.s64 + 16;
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi r8,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 4;
	// cmplw cr6,r28,r8
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r8.u32, ctx.xer);
	// blt cr6,0x830beea0
	if (ctx.cr6.lt) goto loc_830BEEA0;
loc_830BEF94:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// lfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x82cb1138
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830BEFA0"))) PPC_WEAK_FUNC(sub_830BEFA0);
PPC_FUNC_IMPL(__imp__sub_830BEFA0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// addi r12,r1,-16
	ctx.r12.s64 = ctx.r1.s64 + -16;
	// bl 0x82cb6abc
	ctx.lr = 0x830BEFB4;
	__savefpr_17(ctx, base);
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// bl 0x83088760
	ctx.lr = 0x830BEFC0;
	sub_83088760(ctx, base);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x830bf27c
	if (ctx.cr6.eq) goto loc_830BF27C;
	// lwz r10,156(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 156);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830bf27c
	if (ctx.cr6.eq) goto loc_830BF27C;
	// lfs f0,220(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 220);
	ctx.f0.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f13,224(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 224);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f11,232(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 232);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// lfs f7,240(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 240);
	ctx.f7.f64 = double(temp.f32);
	// fneg f8,f11
	ctx.f8.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// lfs f9,236(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 236);
	ctx.f9.f64 = double(temp.f32);
	// fneg f5,f7
	ctx.f5.u64 = ctx.f7.u64 ^ 0x8000000000000000;
	// lfs f4,216(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 216);
	ctx.f4.f64 = double(temp.f32);
	// fneg f6,f9
	ctx.f6.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// fmr f3,f4
	ctx.f3.f64 = ctx.f4.f64;
	// lfs f2,228(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 228);
	ctx.f2.f64 = double(temp.f32);
	// fneg f1,f4
	ctx.f1.u64 = ctx.f4.u64 ^ 0x8000000000000000;
	// lfs f11,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f11.f64 = double(temp.f32);
	// fneg f9,f13
	ctx.f9.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// lfs f13,6380(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6380);
	ctx.f13.f64 = double(temp.f32);
	// fneg f4,f0
	ctx.f4.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lfs f7,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f7.f64 = double(temp.f32);
	// fmsubs f29,f2,f2,f13
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f2.f64 - ctx.f13.f64));
	// lfs f31,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f31.f64 = double(temp.f32);
	// fmr f30,f2
	ctx.f30.f64 = ctx.f2.f64;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// addi r11,r11,216
	ctx.r11.s64 = ctx.r11.s64 + 216;
	// fmuls f28,f12,f8
	ctx.f28.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// addi r11,r31,112
	ctx.r11.s64 = ctx.r31.s64 + 112;
	// fmuls f26,f10,f5
	ctx.f26.f64 = double(float(ctx.f10.f64 * ctx.f5.f64));
	// fmuls f27,f10,f6
	ctx.f27.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// fmuls f25,f5,f3
	ctx.f25.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// lfs f0,7676(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f24,f11,f1
	ctx.f24.f64 = double(float(ctx.f11.f64 * ctx.f1.f64));
	// fmuls f23,f9,f7
	ctx.f23.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// fmuls f22,f9,f11
	ctx.f22.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// fmuls f21,f4,f31
	ctx.f21.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// fmuls f19,f29,f8
	ctx.f19.f64 = double(float(ctx.f29.f64 * ctx.f8.f64));
	// fmsubs f20,f30,f30,f13
	ctx.f20.f64 = double(float(ctx.f30.f64 * ctx.f30.f64 - ctx.f13.f64));
	// fmsubs f28,f6,f3,f28
	ctx.f28.f64 = double(float(ctx.f6.f64 * ctx.f3.f64 - ctx.f28.f64));
	// fmadds f26,f12,f6,f26
	ctx.f26.f64 = double(float(ctx.f12.f64 * ctx.f6.f64 + ctx.f26.f64));
	// fmsubs f27,f12,f5,f27
	ctx.f27.f64 = double(float(ctx.f12.f64 * ctx.f5.f64 - ctx.f27.f64));
	// fmsubs f25,f10,f8,f25
	ctx.f25.f64 = double(float(ctx.f10.f64 * ctx.f8.f64 - ctx.f25.f64));
	// fmuls f5,f5,f29
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f29.f64));
	// fmuls f6,f6,f29
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f29.f64));
	// fmsubs f24,f9,f31,f24
	ctx.f24.f64 = double(float(ctx.f9.f64 * ctx.f31.f64 - ctx.f24.f64));
	// fmsubs f23,f4,f11,f23
	ctx.f23.f64 = double(float(ctx.f4.f64 * ctx.f11.f64 - ctx.f23.f64));
	// fmadds f22,f4,f7,f22
	ctx.f22.f64 = double(float(ctx.f4.f64 * ctx.f7.f64 + ctx.f22.f64));
	// fmsubs f21,f7,f1,f21
	ctx.f21.f64 = double(float(ctx.f7.f64 * ctx.f1.f64 - ctx.f21.f64));
	// fmuls f29,f28,f2
	ctx.f29.f64 = double(float(ctx.f28.f64 * ctx.f2.f64));
	// fmadds f8,f3,f8,f26
	ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f8.f64 + ctx.f26.f64));
	// fmuls f28,f27,f2
	ctx.f28.f64 = double(float(ctx.f27.f64 * ctx.f2.f64));
	// fmuls f2,f25,f2
	ctx.f2.f64 = double(float(ctx.f25.f64 * ctx.f2.f64));
	// fmuls f27,f31,f20
	ctx.f27.f64 = double(float(ctx.f31.f64 * ctx.f20.f64));
	// fmuls f7,f7,f20
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f20.f64));
	// fmuls f26,f24,f30
	ctx.f26.f64 = double(float(ctx.f24.f64 * ctx.f30.f64));
	// fmuls f25,f30,f23
	ctx.f25.f64 = double(float(ctx.f30.f64 * ctx.f23.f64));
	// fmadds f31,f31,f1,f22
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f1.f64 + ctx.f22.f64));
	// fmuls f11,f11,f20
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f20.f64));
	// fsubs f5,f5,f29
	ctx.f5.f64 = double(float(ctx.f5.f64 - ctx.f29.f64));
	// fmuls f10,f10,f8
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f8.f64));
	// fmuls f3,f8,f3
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f3.f64));
	// fsubs f29,f19,f28
	ctx.f29.f64 = double(float(ctx.f19.f64 - ctx.f28.f64));
	// fmuls f8,f12,f8
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// fsubs f6,f6,f2
	ctx.f6.f64 = double(float(ctx.f6.f64 - ctx.f2.f64));
	// fmuls f24,f21,f30
	ctx.f24.f64 = double(float(ctx.f21.f64 * ctx.f30.f64));
	// fadds f2,f7,f26
	ctx.f2.f64 = double(float(ctx.f7.f64 + ctx.f26.f64));
	// fmuls f12,f31,f1
	ctx.f12.f64 = double(float(ctx.f31.f64 * ctx.f1.f64));
	// fmuls f28,f4,f31
	ctx.f28.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// fadds f7,f27,f25
	ctx.f7.f64 = double(float(ctx.f27.f64 + ctx.f25.f64));
	// fadds f10,f5,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 + ctx.f10.f64));
	// fmuls f31,f9,f31
	ctx.f31.f64 = double(float(ctx.f9.f64 * ctx.f31.f64));
	// fadds f5,f29,f3
	ctx.f5.f64 = double(float(ctx.f29.f64 + ctx.f3.f64));
	// fadds f3,f6,f8
	ctx.f3.f64 = double(float(ctx.f6.f64 + ctx.f8.f64));
	// fadds f11,f11,f24
	ctx.f11.f64 = double(float(ctx.f11.f64 + ctx.f24.f64));
	// fadds f2,f2,f28
	ctx.f2.f64 = double(float(ctx.f2.f64 + ctx.f28.f64));
	// fadds f12,f7,f12
	ctx.f12.f64 = double(float(ctx.f7.f64 + ctx.f12.f64));
	// lfs f7,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f8,f5,f0
	ctx.f8.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f5,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f6,f3,f0
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fadds f11,f11,f31
	ctx.f11.f64 = double(float(ctx.f11.f64 + ctx.f31.f64));
	// fmuls f3,f4,f7
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// lfs f29,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f31,f4,f5
	ctx.f31.f64 = double(float(ctx.f4.f64 * ctx.f5.f64));
	// lfs f27,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f28,f9,f7
	ctx.f28.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// lfs f23,176(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 176);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f26,f29,f1
	ctx.f26.f64 = double(float(ctx.f29.f64 * ctx.f1.f64));
	// lfs f24,180(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f25,184(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 184);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// lfs f22,160(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f2,f2,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// lfs f21,168(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 168);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,172(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 172);
	ctx.f20.f64 = double(temp.f32);
	// addi r11,r31,160
	ctx.r11.s64 = ctx.r31.s64 + 160;
	// lfs f19,164(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 164);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f3,f9,f29,f3
	ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f29.f64 + ctx.f3.f64));
	// fmadds f31,f30,f29,f31
	ctx.f31.f64 = double(float(ctx.f30.f64 * ctx.f29.f64 + ctx.f31.f64));
	// fmadds f28,f30,f5,f28
	ctx.f28.f64 = double(float(ctx.f30.f64 * ctx.f5.f64 + ctx.f28.f64));
	// fmsubs f26,f30,f7,f26
	ctx.f26.f64 = double(float(ctx.f30.f64 * ctx.f7.f64 - ctx.f26.f64));
	// fadds f12,f12,f8
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f8.f64));
	// fadds f11,f11,f10
	ctx.f11.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// fadds f2,f2,f6
	ctx.f2.f64 = double(float(ctx.f2.f64 + ctx.f6.f64));
	// fmadds f10,f30,f27,f3
	ctx.f10.f64 = double(float(ctx.f30.f64 * ctx.f27.f64 + ctx.f3.f64));
	// fmadds f8,f7,f1,f31
	ctx.f8.f64 = double(float(ctx.f7.f64 * ctx.f1.f64 + ctx.f31.f64));
	// fmadds f7,f27,f1,f28
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f1.f64 + ctx.f28.f64));
	// fnmsubs f6,f4,f27,f26
	ctx.f6.f64 = double(float(-(ctx.f4.f64 * ctx.f27.f64 - ctx.f26.f64)));
	// fnmsubs f3,f5,f1,f10
	ctx.f3.f64 = double(float(-(ctx.f5.f64 * ctx.f1.f64 - ctx.f10.f64)));
	// fnmsubs f1,f9,f27,f8
	ctx.f1.f64 = double(float(-(ctx.f9.f64 * ctx.f27.f64 - ctx.f8.f64)));
	// fnmsubs f10,f4,f29,f7
	ctx.f10.f64 = double(float(-(ctx.f4.f64 * ctx.f29.f64 - ctx.f7.f64)));
	// fnmsubs f9,f9,f5,f6
	ctx.f9.f64 = double(float(-(ctx.f9.f64 * ctx.f5.f64 - ctx.f6.f64)));
	// fmuls f8,f3,f23
	ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f23.f64));
	// fmuls f5,f3,f24
	ctx.f5.f64 = double(float(ctx.f3.f64 * ctx.f24.f64));
	// fmuls f6,f10,f24
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f24.f64));
	// fmuls f7,f25,f1
	ctx.f7.f64 = double(float(ctx.f25.f64 * ctx.f1.f64));
	// fmsubs f4,f9,f9,f13
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f9.f64 - ctx.f13.f64));
	// fmuls f13,f9,f22
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f22.f64));
	// fmuls f30,f9,f21
	ctx.f30.f64 = double(float(ctx.f9.f64 * ctx.f21.f64));
	// fmuls f31,f10,f22
	ctx.f31.f64 = double(float(ctx.f10.f64 * ctx.f22.f64));
	// fmuls f29,f22,f1
	ctx.f29.f64 = double(float(ctx.f22.f64 * ctx.f1.f64));
	// fmsubs f8,f24,f1,f8
	ctx.f8.f64 = double(float(ctx.f24.f64 * ctx.f1.f64 - ctx.f8.f64));
	// fmadds f5,f23,f1,f5
	ctx.f5.f64 = double(float(ctx.f23.f64 * ctx.f1.f64 + ctx.f5.f64));
	// fmsubs f6,f3,f25,f6
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f25.f64 - ctx.f6.f64));
	// fmsubs f7,f10,f23,f7
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f23.f64 - ctx.f7.f64));
	// fmuls f28,f23,f4
	ctx.f28.f64 = double(float(ctx.f23.f64 * ctx.f4.f64));
	// fmuls f27,f24,f4
	ctx.f27.f64 = double(float(ctx.f24.f64 * ctx.f4.f64));
	// fmadds f13,f20,f1,f13
	ctx.f13.f64 = double(float(ctx.f20.f64 * ctx.f1.f64 + ctx.f13.f64));
	// fmuls f4,f25,f4
	ctx.f4.f64 = double(float(ctx.f25.f64 * ctx.f4.f64));
	// fmadds f30,f19,f1,f30
	ctx.f30.f64 = double(float(ctx.f19.f64 * ctx.f1.f64 + ctx.f30.f64));
	// fmadds f31,f9,f19,f31
	ctx.f31.f64 = double(float(ctx.f9.f64 * ctx.f19.f64 + ctx.f31.f64));
	// fmuls f8,f8,f9
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// fmadds f5,f10,f25,f5
	ctx.f5.f64 = double(float(ctx.f10.f64 * ctx.f25.f64 + ctx.f5.f64));
	// fmuls f6,f9,f6
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f6.f64));
	// fmuls f7,f7,f9
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f9.f64));
	// fmsubs f9,f9,f20,f29
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f20.f64 - ctx.f29.f64));
	// fmadds f13,f3,f21,f13
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f21.f64 + ctx.f13.f64));
	// fmadds f30,f10,f20,f30
	ctx.f30.f64 = double(float(ctx.f10.f64 * ctx.f20.f64 + ctx.f30.f64));
	// fmadds f31,f3,f20,f31
	ctx.f31.f64 = double(float(ctx.f3.f64 * ctx.f20.f64 + ctx.f31.f64));
	// fadds f8,f4,f8
	ctx.f8.f64 = double(float(ctx.f4.f64 + ctx.f8.f64));
	// fmuls f4,f5,f1
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f1.f64));
	// fmuls f29,f3,f5
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f5.f64));
	// fadds f6,f28,f6
	ctx.f6.f64 = double(float(ctx.f28.f64 + ctx.f6.f64));
	// fmuls f5,f10,f5
	ctx.f5.f64 = double(float(ctx.f10.f64 * ctx.f5.f64));
	// fadds f7,f27,f7
	ctx.f7.f64 = double(float(ctx.f27.f64 + ctx.f7.f64));
	// fnmsubs f13,f10,f19,f13
	ctx.f13.f64 = double(float(-(ctx.f10.f64 * ctx.f19.f64 - ctx.f13.f64)));
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fnmsubs f9,f3,f19,f9
	ctx.f9.f64 = double(float(-(ctx.f3.f64 * ctx.f19.f64 - ctx.f9.f64)));
	// fnmsubs f13,f3,f22,f30
	ctx.f13.f64 = double(float(-(ctx.f3.f64 * ctx.f22.f64 - ctx.f30.f64)));
	// stfs f13,88(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fnmsubs f1,f21,f1,f31
	ctx.f1.f64 = double(float(-(ctx.f21.f64 * ctx.f1.f64 - ctx.f31.f64)));
	// stfs f1,84(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fadds f6,f6,f4
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f4.f64));
	// fadds f3,f8,f5
	ctx.f3.f64 = double(float(ctx.f8.f64 + ctx.f5.f64));
	// fadds f4,f7,f29
	ctx.f4.f64 = double(float(ctx.f7.f64 + ctx.f29.f64));
	// fnmsubs f10,f10,f21,f9
	ctx.f10.f64 = double(float(-(ctx.f10.f64 * ctx.f21.f64 - ctx.f9.f64)));
	// fmuls f9,f6,f0
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f7,f3,f0
	ctx.f7.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmuls f8,f4,f0
	ctx.f8.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fadds f6,f7,f11
	ctx.f6.f64 = double(float(ctx.f7.f64 + ctx.f11.f64));
	// fadds f5,f8,f2
	ctx.f5.f64 = double(float(ctx.f8.f64 + ctx.f2.f64));
	// fadds f4,f9,f12
	ctx.f4.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
	// stfs f10,92(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// stfs f4,96(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stfs f5,100(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// b 0x830bf3c0
	goto loc_830BF3C0;
loc_830BF27C:
	// lwz r11,156(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 156);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830bf3c8
	if (ctx.cr6.eq) goto loc_830BF3C8;
	// lfs f12,116(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f12.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f10,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f10.f64 = double(temp.f32);
	// fmr f9,f12
	ctx.f9.f64 = ctx.f12.f64;
	// lfs f13,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// fmr f8,f10
	ctx.f8.f64 = ctx.f10.f64;
	// fmr f11,f13
	ctx.f11.f64 = ctx.f13.f64;
	// lfs f4,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f4.f64 = double(temp.f32);
	// lfs f6,176(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 176);
	ctx.f6.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f0,6380(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6380);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r31,160
	ctx.r11.s64 = ctx.r31.s64 + 160;
	// lfs f7,184(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 184);
	ctx.f7.f64 = double(temp.f32);
	// fmsubs f3,f4,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f4.f64 - ctx.f0.f64));
	// lfs f5,180(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	ctx.f5.f64 = double(temp.f32);
	// addi r11,r31,112
	ctx.r11.s64 = ctx.r31.s64 + 112;
	// lfs f2,160(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	ctx.f2.f64 = double(temp.f32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lfs f1,172(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 172);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f31,f2,f10
	ctx.f31.f64 = double(float(ctx.f2.f64 * ctx.f10.f64));
	// fmuls f30,f1,f13
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
	// lfs f27,164(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 164);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f26,f9,f6
	ctx.f26.f64 = double(float(ctx.f9.f64 * ctx.f6.f64));
	// lfs f29,168(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 168);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f25,f8,f7
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f7.f64));
	// lfs f0,7676(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f23,f8,f5
	ctx.f23.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// lfs f24,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f28,f11,f7
	ctx.f28.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// lfs f20,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f21,f1,f10
	ctx.f21.f64 = double(float(ctx.f1.f64 * ctx.f10.f64));
	// lfs f22,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f18,f3,f5
	ctx.f18.f64 = double(float(ctx.f3.f64 * ctx.f5.f64));
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// fmuls f17,f3,f7
	ctx.f17.f64 = double(float(ctx.f3.f64 * ctx.f7.f64));
	// fmuls f3,f3,f6
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f6.f64));
	// fmuls f19,f2,f13
	ctx.f19.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// fmadds f30,f2,f4,f30
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f4.f64 + ctx.f30.f64));
	// fmsubs f26,f11,f5,f26
	ctx.f26.f64 = double(float(ctx.f11.f64 * ctx.f5.f64 - ctx.f26.f64));
	// fmadds f5,f9,f5,f25
	ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f5.f64 + ctx.f25.f64));
	// fmsubs f7,f9,f7,f23
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f7.f64 - ctx.f23.f64));
	// fmsubs f28,f8,f6,f28
	ctx.f28.f64 = double(float(ctx.f8.f64 * ctx.f6.f64 - ctx.f28.f64));
	// fmadds f31,f27,f4,f31
	ctx.f31.f64 = double(float(ctx.f27.f64 * ctx.f4.f64 + ctx.f31.f64));
	// fmadds f25,f29,f4,f21
	ctx.f25.f64 = double(float(ctx.f29.f64 * ctx.f4.f64 + ctx.f21.f64));
	// fmsubs f23,f1,f4,f19
	ctx.f23.f64 = double(float(ctx.f1.f64 * ctx.f4.f64 - ctx.f19.f64));
	// fmuls f26,f26,f4
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f4.f64));
	// fmadds f6,f11,f6,f5
	ctx.f6.f64 = double(float(ctx.f11.f64 * ctx.f6.f64 + ctx.f5.f64));
	// fmuls f5,f7,f4
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f4.f64));
	// fmuls f28,f28,f4
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f4.f64));
	// fmadds f1,f1,f12,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f12.f64 + ctx.f31.f64));
	// fmadds f4,f29,f12,f30
	ctx.f4.f64 = double(float(ctx.f29.f64 * ctx.f12.f64 + ctx.f30.f64));
	// fmadds f7,f27,f13,f25
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f13.f64 + ctx.f25.f64));
	// fnmsubs f31,f27,f12,f23
	ctx.f31.f64 = double(float(-(ctx.f27.f64 * ctx.f12.f64 - ctx.f23.f64)));
	// fmuls f9,f9,f6
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f6.f64));
	// fmuls f8,f8,f6
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f6.f64));
	// fadds f5,f3,f5
	ctx.f5.f64 = double(float(ctx.f3.f64 + ctx.f5.f64));
	// fadds f30,f18,f28
	ctx.f30.f64 = double(float(ctx.f18.f64 + ctx.f28.f64));
	// fmuls f6,f6,f11
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f11.f64));
	// fadds f28,f17,f26
	ctx.f28.f64 = double(float(ctx.f17.f64 + ctx.f26.f64));
	// fnmsubs f3,f29,f13,f1
	ctx.f3.f64 = double(float(-(ctx.f29.f64 * ctx.f13.f64 - ctx.f1.f64)));
	// stfs f3,84(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f2,f2,f12,f7
	ctx.f2.f64 = double(float(-(ctx.f2.f64 * ctx.f12.f64 - ctx.f7.f64)));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fnmsubs f4,f27,f10,f4
	ctx.f4.f64 = double(float(-(ctx.f27.f64 * ctx.f10.f64 - ctx.f4.f64)));
	// stfs f4,80(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fnmsubs f1,f29,f10,f31
	ctx.f1.f64 = double(float(-(ctx.f29.f64 * ctx.f10.f64 - ctx.f31.f64)));
	// stfs f1,92(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fadds f13,f30,f9
	ctx.f13.f64 = double(float(ctx.f30.f64 + ctx.f9.f64));
	// fadds f11,f5,f6
	ctx.f11.f64 = double(float(ctx.f5.f64 + ctx.f6.f64));
	// fadds f12,f28,f8
	ctx.f12.f64 = double(float(ctx.f28.f64 + ctx.f8.f64));
	// fmuls f10,f13,f0
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fmuls f8,f11,f0
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fadds f7,f24,f10
	ctx.f7.f64 = double(float(ctx.f24.f64 + ctx.f10.f64));
	// stfs f7,100(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f5,f20,f8
	ctx.f5.f64 = double(float(ctx.f20.f64 + ctx.f8.f64));
	// stfs f5,96(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f6,f22,f9
	ctx.f6.f64 = double(float(ctx.f22.f64 + ctx.f9.f64));
loc_830BF3C0:
	// stfs f6,104(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// bl 0x830bd608
	ctx.lr = 0x830BF3C8;
	sub_830BD608(ctx, base);
loc_830BF3C8:
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// addi r12,r1,-16
	ctx.r12.s64 = ctx.r1.s64 + -16;
	// bl 0x82cb6b08
	ctx.lr = 0x830BF3D4;
	__restfpr_17(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830BF3E4"))) PPC_WEAK_FUNC(sub_830BF3E4);
PPC_FUNC_IMPL(__imp__sub_830BF3E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830BF3E8"))) PPC_WEAK_FUNC(sub_830BF3E8);
PPC_FUNC_IMPL(__imp__sub_830BF3E8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10ec
	ctx.lr = 0x830BF3F0;
	__savegprlr_29(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// mr r31,r5
	ctx.r31.u64 = ctx.r5.u64;
	// bl 0x831f3a18
	ctx.lr = 0x830BF408;
	sub_831F3A18(ctx, base);
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// stw r29,36(r30)
	PPC_STORE_U32(ctx.r30.u32 + 36, ctx.r29.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r10,r11,5432
	ctx.r10.s64 = ctx.r11.s64 + 5432;
	// stw r6,40(r30)
	PPC_STORE_U32(ctx.r30.u32 + 40, ctx.r6.u32);
	// stw r10,0(r30)
	PPC_STORE_U32(ctx.r30.u32 + 0, ctx.r10.u32);
	// stw r6,44(r30)
	PPC_STORE_U32(ctx.r30.u32 + 44, ctx.r6.u32);
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x830bf438
	if (!ctx.cr6.eq) goto loc_830BF438;
	// lwz r11,280(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 280);
	// b 0x830bf444
	goto loc_830BF444;
loc_830BF438:
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bne cr6,0x830bf448
	if (!ctx.cr6.eq) goto loc_830BF448;
	// lwz r11,100(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 100);
loc_830BF444:
	// stw r11,32(r30)
	PPC_STORE_U32(ctx.r30.u32 + 32, ctx.r11.u32);
loc_830BF448:
	// lwz r11,32(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 32);
	// lwz r10,264(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x830bf460
	if (ctx.cr6.eq) goto loc_830BF460;
	// addi r8,r10,4
	ctx.r8.s64 = ctx.r10.s64 + 4;
	// b 0x830bf478
	goto loc_830BF478;
loc_830BF460:
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x830bf474
	if (ctx.cr6.eq) goto loc_830BF474;
	// lwz r8,332(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 332);
	// b 0x830bf478
	goto loc_830BF478;
loc_830BF474:
	// lwz r8,332(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 332);
loc_830BF478:
	// lwz r7,20(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 20);
	// lwz r9,16(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	// cmplw cr6,r9,r7
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, ctx.xer);
	// beq cr6,0x830bf4e0
	if (ctx.cr6.eq) goto loc_830BF4E0;
loc_830BF488:
	// lwz r11,0(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmplw cr6,r10,r8
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, ctx.xer);
	// bne cr6,0x830bf4a8
	if (!ctx.cr6.eq) goto loc_830BF4A8;
	// lwz r5,12(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r4,108(r29)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r29.u32 + 108);
	// cmplw cr6,r5,r4
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, ctx.r4.u32, ctx.xer);
	// beq cr6,0x830bf4c0
	if (ctx.cr6.eq) goto loc_830BF4C0;
loc_830BF4A8:
	// lwz r5,108(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 108);
	// cmplw cr6,r10,r5
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, ctx.xer);
	// bne cr6,0x830bf4d4
	if (!ctx.cr6.eq) goto loc_830BF4D4;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// cmplw cr6,r10,r8
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, ctx.xer);
	// bne cr6,0x830bf4d4
	if (!ctx.cr6.eq) goto loc_830BF4D4;
loc_830BF4C0:
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x830bf524
	if (ctx.cr6.eq) goto loc_830BF524;
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// beq cr6,0x830bf524
	if (ctx.cr6.eq) goto loc_830BF524;
loc_830BF4D4:
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmplw cr6,r9,r7
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, ctx.xer);
	// bne cr6,0x830bf488
	if (!ctx.cr6.eq) goto loc_830BF488;
loc_830BF4E0:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
loc_830BF4E4:
	// stw r11,44(r30)
	PPC_STORE_U32(ctx.r30.u32 + 44, ctx.r11.u32);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x831f3a60
	ctx.lr = 0x830BF4F0;
	sub_831F3A60(ctx, base);
	// lwz r11,40(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 40);
	// lwz r3,44(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 44);
	// ori r10,r11,2
	ctx.r10.u64 = ctx.r11.u64 | 2;
	// stw r10,40(r30)
	PPC_STORE_U32(ctx.r30.u32 + 40, ctx.r10.u32);
	// bl 0x83097208
	ctx.lr = 0x830BF504;
	sub_83097208(ctx, base);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x830bf51c
	if (ctx.cr6.eq) goto loc_830BF51C;
	// bl 0x830bdd70
	ctx.lr = 0x830BF518;
	sub_830BDD70(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
loc_830BF51C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82cb113c
	__restgprlr_29(ctx, base);
	return;
loc_830BF524:
	// lwz r11,0(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// b 0x830bf4e4
	goto loc_830BF4E4;
}

__attribute__((alias("__imp__sub_830BF52C"))) PPC_WEAK_FUNC(sub_830BF52C);
PPC_FUNC_IMPL(__imp__sub_830BF52C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830BF530"))) PPC_WEAK_FUNC(sub_830BF530);
PPC_FUNC_IMPL(__imp__sub_830BF530) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e0
	ctx.lr = 0x830BF538;
	__savegprlr_26(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r11,40(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	// ori r10,r11,2
	ctx.r10.u64 = ctx.r11.u64 | 2;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// stw r10,40(r31)
	PPC_STORE_U32(ctx.r31.u32 + 40, ctx.r10.u32);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x830bf738
	if (ctx.cr6.eq) goto loc_830BF738;
	// lwz r11,36(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// lwz r3,104(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 104);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,352(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 352);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830BF570;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmpwi cr6,r3,1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 1, ctx.xer);
	// beq cr6,0x830bf598
	if (ctx.cr6.eq) goto loc_830BF598;
	// lwz r11,36(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// lwz r3,104(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 104);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,352(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 352);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830BF590;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmpwi cr6,r3,2
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 2, ctx.xer);
	// bne cr6,0x830bf5a8
	if (!ctx.cr6.eq) goto loc_830BF5A8;
loc_830BF598:
	// lwz r11,40(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	// rlwinm r10,r11,30,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 30) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830bf5c8
	if (ctx.cr6.eq) goto loc_830BF5C8;
loc_830BF5A8:
	// lwz r11,36(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lwz r10,104(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 104);
	// lwz r3,268(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 268);
	// bl 0x83047c78
	ctx.lr = 0x830BF5BC;
	sub_83047C78(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82cb1130
	__restgprlr_26(ctx, base);
	return;
loc_830BF5C8:
	// lwz r10,36(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// rlwinm r9,r11,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,40(r31)
	PPC_STORE_U32(ctx.r31.u32 + 40, ctx.r9.u32);
	// lwz r27,104(r10)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r10.u32 + 104);
	// lwz r11,348(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 348);
	// lwz r8,100(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 100);
	// rlwinm r10,r8,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r9,r11,8
	ctx.r9.s64 = ctx.r11.s64 + 8;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// addi r7,r9,-1
	ctx.r7.s64 = ctx.r9.s64 + -1;
	// stw r7,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r7.u32);
	// lwz r6,344(r27)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r27.u32 + 344);
	// cmpwi cr6,r6,2
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 2, ctx.xer);
	// bne cr6,0x830bf72c
	if (!ctx.cr6.eq) goto loc_830BF72C;
	// lwz r11,348(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 348);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x830bf72c
	if (!ctx.cr6.eq) goto loc_830BF72C;
	// lbz r10,368(r27)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r27.u32 + 368);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x830bf72c
	if (!ctx.cr6.eq) goto loc_830BF72C;
	// li r26,1
	ctx.r26.s64 = 1;
	// stb r26,12(r11)
	PPC_STORE_U8(ctx.r11.u32 + 12, ctx.r26.u8);
	// lwz r11,268(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 268);
	// addi r31,r11,1532
	ctx.r31.s64 = ctx.r11.s64 + 1532;
	// lwz r9,1540(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1540);
	// lwz r11,1536(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1536);
	// cmplw cr6,r9,r11
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x830bf714
	if (ctx.cr6.gt) goto loc_830BF714;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// srawi r11,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 2;
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// rlwinm r11,r8,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// bne cr6,0x830bf668
	if (!ctx.cr6.eq) goto loc_830BF668;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x830bf670
	goto loc_830BF670;
loc_830BF668:
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// srawi r10,r10,2
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
loc_830BF670:
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x830bf714
	if (!ctx.cr6.lt) goto loc_830BF714;
	// lis r29,-31901
	ctx.r29.s64 = -2090663936;
	// rlwinm r28,r11,2,0,29
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// li r5,282
	ctx.r5.s64 = 282;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lwz r3,-32308(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830BF69C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830bf6cc
	if (ctx.cr6.eq) goto loc_830BF6CC;
loc_830BF6B4:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x830bf6b4
	if (!ctx.cr6.eq) goto loc_830BF6B4;
loc_830BF6CC:
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x830bf6ec
	if (ctx.cr6.eq) goto loc_830BF6EC;
	// lwz r3,-32308(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830BF6EC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830BF6EC:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// add r10,r28,r30
	ctx.r10.u64 = ctx.r28.u64 + ctx.r30.u64;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// subf r8,r9,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r9.s64;
	// stw r30,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r30.u32);
	// srawi r7,r8,2
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r11,r30
	ctx.r6.u64 = ctx.r11.u64 + ctx.r30.u64;
	// stw r6,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r6.u32);
loc_830BF714:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// stw r27,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r27.u32);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// addi r10,r11,4
	ctx.r10.s64 = ctx.r11.s64 + 4;
	// stw r10,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r10.u32);
	// stb r26,368(r27)
	PPC_STORE_U8(ctx.r27.u32 + 368, ctx.r26.u8);
loc_830BF72C:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82cb1130
	__restgprlr_26(ctx, base);
	return;
loc_830BF738:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82cb1130
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830BF744"))) PPC_WEAK_FUNC(sub_830BF744);
PPC_FUNC_IMPL(__imp__sub_830BF744) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830BF748"))) PPC_WEAK_FUNC(sub_830BF748);
PPC_FUNC_IMPL(__imp__sub_830BF748) {
	PPC_FUNC_PROLOGUE();
	// b 0x830bf530
	sub_830BF530(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830BF74C"))) PPC_WEAK_FUNC(sub_830BF74C);
PPC_FUNC_IMPL(__imp__sub_830BF74C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830BF750"))) PPC_WEAK_FUNC(sub_830BF750);
PPC_FUNC_IMPL(__imp__sub_830BF750) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e0
	ctx.lr = 0x830BF758;
	__savegprlr_26(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r11,40(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830bf980
	if (ctx.cr6.eq) goto loc_830BF980;
	// rlwinm r10,r11,0,30,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x2;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x830bf788
	if (!ctx.cr6.eq) goto loc_830BF788;
	// clrlwi r10,r4,24
	ctx.r10.u64 = ctx.r4.u32 & 0xFF;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830bf980
	if (ctx.cr6.eq) goto loc_830BF980;
loc_830BF788:
	// lwz r10,36(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// rlwinm r9,r11,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,40(r31)
	PPC_STORE_U32(ctx.r31.u32 + 40, ctx.r9.u32);
	// lwz r3,104(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 104);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r7,352(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 352);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x830BF7A8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmpwi cr6,r3,1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 1, ctx.xer);
	// beq cr6,0x830bf7d0
	if (ctx.cr6.eq) goto loc_830BF7D0;
	// lwz r11,36(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// lwz r3,104(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 104);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,352(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 352);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830BF7C8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmpwi cr6,r3,2
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 2, ctx.xer);
	// bne cr6,0x830bf80c
	if (!ctx.cr6.eq) goto loc_830BF80C;
loc_830BF7D0:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x830b98b0
	ctx.lr = 0x830BF7D8;
	sub_830B98B0(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830bf80c
	if (ctx.cr6.eq) goto loc_830BF80C;
	// lwz r11,36(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	// lwz r4,104(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 104);
	// bl 0x83097de8
	ctx.lr = 0x830BF7F4;
	sub_83097DE8(ctx, base);
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830bf80c
	if (ctx.cr6.eq) goto loc_830BF80C;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x830b9950
	ctx.lr = 0x830BF80C;
	sub_830B9950(ctx, base);
loc_830BF80C:
	// lwz r11,40(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	// rlwinm r10,r11,30,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 30) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830bf824
	if (ctx.cr6.eq) goto loc_830BF824;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x830b97e8
	ctx.lr = 0x830BF824;
	sub_830B97E8(ctx, base);
loc_830BF824:
	// lwz r11,36(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// lwz r27,104(r11)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r11.u32 + 104);
	// lwz r10,100(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 100);
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r11,348(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 348);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// addi r9,r11,8
	ctx.r9.s64 = ctx.r11.s64 + 8;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stw r9,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r9.u32);
	// lwz r8,344(r27)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r27.u32 + 344);
	// cmpwi cr6,r8,2
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 2, ctx.xer);
	// bne cr6,0x830bf980
	if (!ctx.cr6.eq) goto loc_830BF980;
	// lwz r11,348(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 348);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x830bf980
	if (!ctx.cr6.eq) goto loc_830BF980;
	// lbz r10,368(r27)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r27.u32 + 368);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x830bf980
	if (!ctx.cr6.eq) goto loc_830BF980;
	// li r26,1
	ctx.r26.s64 = 1;
	// stb r26,12(r11)
	PPC_STORE_U8(ctx.r11.u32 + 12, ctx.r26.u8);
	// lwz r11,268(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 268);
	// addi r31,r11,1532
	ctx.r31.s64 = ctx.r11.s64 + 1532;
	// lwz r9,1540(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1540);
	// lwz r11,1536(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1536);
	// cmplw cr6,r9,r11
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x830bf968
	if (ctx.cr6.gt) goto loc_830BF968;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// srawi r11,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 2;
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// rlwinm r11,r8,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// bne cr6,0x830bf8bc
	if (!ctx.cr6.eq) goto loc_830BF8BC;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x830bf8c4
	goto loc_830BF8C4;
loc_830BF8BC:
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// srawi r10,r10,2
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
loc_830BF8C4:
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x830bf968
	if (!ctx.cr6.lt) goto loc_830BF968;
	// lis r29,-31901
	ctx.r29.s64 = -2090663936;
	// rlwinm r28,r11,2,0,29
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// li r5,282
	ctx.r5.s64 = 282;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lwz r3,-32308(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830BF8F0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830bf920
	if (ctx.cr6.eq) goto loc_830BF920;
loc_830BF908:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x830bf908
	if (!ctx.cr6.eq) goto loc_830BF908;
loc_830BF920:
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x830bf940
	if (ctx.cr6.eq) goto loc_830BF940;
	// lwz r3,-32308(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830BF940;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830BF940:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// add r10,r28,r30
	ctx.r10.u64 = ctx.r28.u64 + ctx.r30.u64;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// subf r8,r9,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r9.s64;
	// stw r30,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r30.u32);
	// srawi r7,r8,2
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r11,r30
	ctx.r6.u64 = ctx.r11.u64 + ctx.r30.u64;
	// stw r6,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r6.u32);
loc_830BF968:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// stw r27,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r27.u32);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// addi r10,r11,4
	ctx.r10.s64 = ctx.r11.s64 + 4;
	// stw r10,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r10.u32);
	// stb r26,368(r27)
	PPC_STORE_U8(ctx.r27.u32 + 368, ctx.r26.u8);
loc_830BF980:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82cb1130
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830BF988"))) PPC_WEAK_FUNC(sub_830BF988);
PPC_FUNC_IMPL(__imp__sub_830BF988) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lis r10,-32248
	ctx.r10.s64 = -2113404928;
	// addi r9,r10,5432
	ctx.r9.s64 = ctx.r10.s64 + 5432;
	// lwz r11,40(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	// stw r9,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r9.u32);
	// rlwinm r8,r11,30,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 30) & 0x1;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x830bf9c0
	if (ctx.cr6.eq) goto loc_830BF9C0;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x830bf750
	ctx.lr = 0x830BF9C0;
	sub_830BF750(ctx, base);
loc_830BF9C0:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x831f3ab0
	ctx.lr = 0x830BF9C8;
	sub_831F3AB0(ctx, base);
	// lwz r11,40(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	// rlwinm r10,r11,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x8;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830bf9ec
	if (ctx.cr6.eq) goto loc_830BF9EC;
	// lwz r11,36(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lwz r10,104(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 104);
	// lwz r3,268(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 268);
	// bl 0x83045d48
	ctx.lr = 0x830BF9EC;
	sub_83045D48(ctx, base);
loc_830BF9EC:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x831f3a50
	ctx.lr = 0x830BF9F4;
	sub_831F3A50(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830BFA08"))) PPC_WEAK_FUNC(sub_830BFA08);
PPC_FUNC_IMPL(__imp__sub_830BFA08) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10dc
	ctx.lr = 0x830BFA10;
	__savegprlr_25(ctx, base);
	// addi r12,r1,-64
	ctx.r12.s64 = ctx.r1.s64 + -64;
	// bl 0x82cb6ab8
	ctx.lr = 0x830BFA18;
	__savefpr_16(ctx, base);
	// stwu r1,-464(r1)
	ea = -464 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// mr r25,r4
	ctx.r25.u64 = ctx.r4.u64;
	// addi r27,r28,348
	ctx.r27.s64 = ctx.r28.s64 + 348;
	// lwz r11,352(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 352);
	// lwz r10,348(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 348);
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi r8,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 4;
	// cmplw cr6,r25,r8
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, ctx.r8.u32, ctx.xer);
	// blt cr6,0x830bfa54
	if (ctx.cr6.lt) goto loc_830BFA54;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,464
	ctx.r1.s64 = ctx.r1.s64 + 464;
	// addi r12,r1,-64
	ctx.r12.s64 = ctx.r1.s64 + -64;
	// bl 0x82cb6b04
	ctx.lr = 0x830BFA50;
	__restfpr_16(ctx, base);
	// b 0x82cb112c
	__restgprlr_25(ctx, base);
	return;
loc_830BFA54:
	// lwz r11,344(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 344);
	// rlwinm r26,r25,4,0,27
	ctx.r26.u64 = __builtin_rotateleft64(ctx.r25.u32 | (ctx.r25.u64 << 32), 4) & 0xFFFFFFF0;
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// add r11,r26,r11
	ctx.r11.u64 = ctx.r26.u64 + ctx.r11.u64;
	// bne cr6,0x830c0174
	if (!ctx.cr6.eq) goto loc_830C0174;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// rlwinm r9,r10,0,1,1
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x40000000;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne cr6,0x830c0818
	if (!ctx.cr6.eq) goto loc_830C0818;
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// oris r9,r10,16384
	ctx.r9.u64 = ctx.r10.u64 | 1073741824;
	// stw r9,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r9.u32);
	// lwz r8,344(r28)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r28.u32 + 344);
	// cmpwi cr6,r8,2
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 2, ctx.xer);
	// bne cr6,0x830c0150
	if (!ctx.cr6.eq) goto loc_830C0150;
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// add r11,r26,r11
	ctx.r11.u64 = ctx.r26.u64 + ctx.r11.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x830c0150
	if (!ctx.cr6.gt) goto loc_830C0150;
	// lwz r11,0(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x830c0150
	if (!ctx.cr6.eq) goto loc_830C0150;
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// li r29,0
	ctx.r29.s64 = 0;
	// add r11,r26,r11
	ctx.r11.u64 = ctx.r26.u64 + ctx.r11.u64;
	// stb r29,12(r11)
	PPC_STORE_U8(ctx.r11.u32 + 12, ctx.r29.u8);
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// lwzx r9,r26,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r10.u32);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x830c0150
	if (!ctx.cr6.eq) goto loc_830C0150;
	// lfs f0,112(r28)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 112);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f13,116(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 116);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f11,120(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 120);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// lfs f5,124(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 124);
	ctx.f5.f64 = double(temp.f32);
	// lfs f8,184(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 184);
	ctx.f8.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f30,6380(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6380);
	ctx.f30.f64 = double(temp.f32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lfs f7,180(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 180);
	ctx.f7.f64 = double(temp.f32);
	// fmsubs f4,f5,f5,f30
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f5.f64 - ctx.f30.f64));
	// lfs f6,176(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 176);
	ctx.f6.f64 = double(temp.f32);
	// addi r31,r28,112
	ctx.r31.s64 = ctx.r28.s64 + 112;
	// lfs f3,160(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 160);
	ctx.f3.f64 = double(temp.f32);
	// addi r30,r28,160
	ctx.r30.s64 = ctx.r28.s64 + 160;
	// lfs f2,172(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 172);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f3,f11
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// fmuls f29,f2,f0
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// lfs f26,164(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 164);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f27,f12,f8
	ctx.f27.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// lfs f28,168(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 168);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f25,f6,f10
	ctx.f25.f64 = double(float(ctx.f6.f64 * ctx.f10.f64));
	// lfs f31,7676(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 7676);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f24,f7,f10
	ctx.f24.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
	// lfs f23,132(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 132);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f22,f7,f9
	ctx.f22.f64 = double(float(ctx.f7.f64 * ctx.f9.f64));
	// lfs f21,136(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 136);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f20,f2,f11
	ctx.f20.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// lfs f19,128(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 128);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f17,f4,f7
	ctx.f17.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// addi r9,r1,124
	ctx.r9.s64 = ctx.r1.s64 + 124;
	// fmuls f16,f4,f8
	ctx.f16.f64 = double(float(ctx.f4.f64 * ctx.f8.f64));
	// fmuls f4,f6,f4
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// fmuls f18,f3,f0
	ctx.f18.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmadds f1,f26,f5,f1
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f5.f64 + ctx.f1.f64));
	// fmsubs f27,f6,f9,f27
	ctx.f27.f64 = double(float(ctx.f6.f64 * ctx.f9.f64 - ctx.f27.f64));
	// fmsubs f7,f12,f7,f25
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f7.f64 - ctx.f25.f64));
	// fmadds f6,f6,f12,f24
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f12.f64 + ctx.f24.f64));
	// fmsubs f25,f8,f10,f22
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f10.f64 - ctx.f22.f64));
	// fmadds f29,f3,f5,f29
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f5.f64 + ctx.f29.f64));
	// fmadds f24,f28,f5,f20
	ctx.f24.f64 = double(float(ctx.f28.f64 * ctx.f5.f64 + ctx.f20.f64));
	// fmsubs f22,f2,f5,f18
	ctx.f22.f64 = double(float(ctx.f2.f64 * ctx.f5.f64 - ctx.f18.f64));
	// fmadds f2,f2,f13,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f13.f64 + ctx.f1.f64));
	// fmuls f27,f27,f5
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f5.f64));
	// fmuls f7,f7,f5
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// fmadds f6,f8,f9,f6
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f9.f64 + ctx.f6.f64));
	// fmuls f5,f25,f5
	ctx.f5.f64 = double(float(ctx.f25.f64 * ctx.f5.f64));
	// fmadds f8,f28,f13,f29
	ctx.f8.f64 = double(float(ctx.f28.f64 * ctx.f13.f64 + ctx.f29.f64));
	// fmadds f1,f26,f0,f24
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f0.f64 + ctx.f24.f64));
	// fnmsubs f29,f26,f13,f22
	ctx.f29.f64 = double(float(-(ctx.f26.f64 * ctx.f13.f64 - ctx.f22.f64)));
	// fnmsubs f2,f28,f0,f2
	ctx.f2.f64 = double(float(-(ctx.f28.f64 * ctx.f0.f64 - ctx.f2.f64)));
	// stfs f2,84(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fadds f27,f17,f27
	ctx.f27.f64 = double(float(ctx.f17.f64 + ctx.f27.f64));
	// fadds f7,f16,f7
	ctx.f7.f64 = double(float(ctx.f16.f64 + ctx.f7.f64));
	// fmuls f10,f6,f10
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f10.f64));
	// fmuls f9,f6,f9
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f9.f64));
	// fmuls f6,f6,f12
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// fadds f5,f4,f5
	ctx.f5.f64 = double(float(ctx.f4.f64 + ctx.f5.f64));
	// fnmsubs f4,f26,f11,f8
	ctx.f4.f64 = double(float(-(ctx.f26.f64 * ctx.f11.f64 - ctx.f8.f64)));
	// stfs f4,80(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fnmsubs f1,f3,f13,f1
	ctx.f1.f64 = double(float(-(ctx.f3.f64 * ctx.f13.f64 - ctx.f1.f64)));
	// stfs f1,88(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fnmsubs f0,f28,f11,f29
	ctx.f0.f64 = double(float(-(ctx.f28.f64 * ctx.f11.f64 - ctx.f29.f64)));
	// stfs f0,92(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fadds f13,f27,f10
	ctx.f13.f64 = double(float(ctx.f27.f64 + ctx.f10.f64));
	// fadds f12,f7,f9
	ctx.f12.f64 = double(float(ctx.f7.f64 + ctx.f9.f64));
	// fadds f11,f5,f6
	ctx.f11.f64 = double(float(ctx.f5.f64 + ctx.f6.f64));
	// fmuls f10,f13,f31
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// fmuls f9,f12,f31
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// fmuls f8,f11,f31
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// fadds f7,f23,f10
	ctx.f7.f64 = double(float(ctx.f23.f64 + ctx.f10.f64));
	// stfs f7,100(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f6,f21,f9
	ctx.f6.f64 = double(float(ctx.f21.f64 + ctx.f9.f64));
	// stfs f6,104(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fadds f5,f19,f8
	ctx.f5.f64 = double(float(ctx.f19.f64 + ctx.f8.f64));
	// stfs f5,96(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lwz r7,8(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// lwz r6,12(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lwz r11,4(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r5,100(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r4,104(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r8,96(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r11.u32);
	// stw r7,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r7.u32);
	// stw r6,12(r9)
	PPC_STORE_U32(ctx.r9.u32 + 12, ctx.r6.u32);
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// stw r5,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r5.u32);
	// stw r8,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r8.u32);
	// stw r4,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r4.u32);
	// bl 0x8315fb70
	ctx.lr = 0x830BFC50;
	sub_8315FB70(ctx, base);
	// lwz r11,264(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 264);
	// li r7,10
	ctx.r7.s64 = 10;
	// stw r7,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r7.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830bff24
	if (ctx.cr6.eq) goto loc_830BFF24;
	// lfs f0,216(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 216);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r11,216
	ctx.r10.s64 = ctx.r11.s64 + 216;
	// lfs f5,240(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 240);
	ctx.f5.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f13,220(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 220);
	ctx.f13.f64 = double(temp.f32);
	// fneg f3,f5
	ctx.f3.u64 = ctx.f5.u64 ^ 0x8000000000000000;
	// lfs f11,224(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 224);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// lfs f9,232(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 232);
	ctx.f9.f64 = double(temp.f32);
	// fmr f8,f11
	ctx.f8.f64 = ctx.f11.f64;
	// lfs f7,236(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 236);
	ctx.f7.f64 = double(temp.f32);
	// fneg f6,f9
	ctx.f6.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// fneg f4,f7
	ctx.f4.u64 = ctx.f7.u64 ^ 0x8000000000000000;
	// lfs f2,228(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 228);
	ctx.f2.f64 = double(temp.f32);
	// fneg f1,f0
	ctx.f1.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lfs f0,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f0.f64 = double(temp.f32);
	// fneg f11,f11
	ctx.f11.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// lfs f9,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f9.f64 = double(temp.f32);
	// fneg f7,f13
	ctx.f7.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// lfs f5,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f5.f64 = double(temp.f32);
	// fmsubs f28,f2,f2,f30
	ctx.f28.f64 = double(float(ctx.f2.f64 * ctx.f2.f64 - ctx.f30.f64));
	// lfs f29,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f29.f64 = double(temp.f32);
	// fmr f13,f2
	ctx.f13.f64 = ctx.f2.f64;
	// lfs f27,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f24,f3,f12
	ctx.f24.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// fmuls f21,f8,f3
	ctx.f21.f64 = double(float(ctx.f8.f64 * ctx.f3.f64));
	// fmuls f26,f10,f6
	ctx.f26.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// fmuls f25,f8,f4
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// fmuls f23,f1,f0
	ctx.f23.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f20,f11,f0
	ctx.f20.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f19,f7,f5
	ctx.f19.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// fmuls f22,f11,f9
	ctx.f22.f64 = double(float(ctx.f11.f64 * ctx.f9.f64));
	// fmuls f17,f28,f6
	ctx.f17.f64 = double(float(ctx.f28.f64 * ctx.f6.f64));
	// fmsubs f24,f8,f6,f24
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f6.f64 - ctx.f24.f64));
	// fmsubs f18,f13,f13,f30
	ctx.f18.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 - ctx.f30.f64));
	// fmadds f6,f12,f6,f21
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f6.f64 + ctx.f21.f64));
	// fmsubs f26,f4,f12,f26
	ctx.f26.f64 = double(float(ctx.f4.f64 * ctx.f12.f64 - ctx.f26.f64));
	// fmsubs f25,f10,f3,f25
	ctx.f25.f64 = double(float(ctx.f10.f64 * ctx.f3.f64 - ctx.f25.f64));
	// fmsubs f23,f11,f5,f23
	ctx.f23.f64 = double(float(ctx.f11.f64 * ctx.f5.f64 - ctx.f23.f64));
	// fmadds f21,f7,f9,f20
	ctx.f21.f64 = double(float(ctx.f7.f64 * ctx.f9.f64 + ctx.f20.f64));
	// fmsubs f20,f1,f9,f19
	ctx.f20.f64 = double(float(ctx.f1.f64 * ctx.f9.f64 - ctx.f19.f64));
	// fmsubs f22,f7,f0,f22
	ctx.f22.f64 = double(float(ctx.f7.f64 * ctx.f0.f64 - ctx.f22.f64));
	// fmuls f19,f4,f28
	ctx.f19.f64 = double(float(ctx.f4.f64 * ctx.f28.f64));
	// fmuls f3,f3,f28
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f28.f64));
	// fmuls f28,f18,f5
	ctx.f28.f64 = double(float(ctx.f18.f64 * ctx.f5.f64));
	// fmadds f6,f10,f4,f6
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f4.f64 + ctx.f6.f64));
	// fmuls f26,f26,f2
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f2.f64));
	// fmuls f25,f25,f2
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f2.f64));
	// fmuls f4,f24,f2
	ctx.f4.f64 = double(float(ctx.f24.f64 * ctx.f2.f64));
	// fmadds f5,f1,f5,f21
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f5.f64 + ctx.f21.f64));
	// fmuls f9,f18,f9
	ctx.f9.f64 = double(float(ctx.f18.f64 * ctx.f9.f64));
	// fmuls f2,f23,f13
	ctx.f2.f64 = double(float(ctx.f23.f64 * ctx.f13.f64));
	// fmuls f0,f18,f0
	ctx.f0.f64 = double(float(ctx.f18.f64 * ctx.f0.f64));
	// fmuls f24,f22,f13
	ctx.f24.f64 = double(float(ctx.f22.f64 * ctx.f13.f64));
	// fmuls f23,f20,f13
	ctx.f23.f64 = double(float(ctx.f20.f64 * ctx.f13.f64));
	// fmuls f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// fsubs f3,f3,f26
	ctx.f3.f64 = double(float(ctx.f3.f64 - ctx.f26.f64));
	// fmuls f10,f10,f6
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// fmuls f8,f8,f6
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f6.f64));
	// fsubs f26,f17,f25
	ctx.f26.f64 = double(float(ctx.f17.f64 - ctx.f25.f64));
	// fmuls f6,f5,f1
	ctx.f6.f64 = double(float(ctx.f5.f64 * ctx.f1.f64));
	// fadds f2,f9,f2
	ctx.f2.f64 = double(float(ctx.f9.f64 + ctx.f2.f64));
	// fmuls f25,f7,f5
	ctx.f25.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// fsubs f4,f19,f4
	ctx.f4.f64 = double(float(ctx.f19.f64 - ctx.f4.f64));
	// fmuls f5,f11,f5
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f5.f64));
	// fadds f9,f28,f24
	ctx.f9.f64 = double(float(ctx.f28.f64 + ctx.f24.f64));
	// fadds f0,f0,f23
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f23.f64));
	// fadds f8,f3,f8
	ctx.f8.f64 = double(float(ctx.f3.f64 + ctx.f8.f64));
	// fadds f12,f26,f12
	ctx.f12.f64 = double(float(ctx.f26.f64 + ctx.f12.f64));
	// fadds f3,f2,f25
	ctx.f3.f64 = double(float(ctx.f2.f64 + ctx.f25.f64));
	// fadds f4,f4,f10
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f10.f64));
	// fadds f2,f9,f6
	ctx.f2.f64 = double(float(ctx.f9.f64 + ctx.f6.f64));
	// fadds f0,f0,f5
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f5.f64));
	// fmuls f10,f8,f31
	ctx.f10.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fmuls f12,f12,f31
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// fmuls f8,f3,f31
	ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// fmuls f9,f4,f31
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// lfs f4,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f3,f11,f29
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f29.f64));
	// fmuls f6,f2,f31
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f5,f0,f31
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fmuls f2,f7,f27
	ctx.f2.f64 = double(float(ctx.f7.f64 * ctx.f27.f64));
	// lfs f0,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f28,f11,f4
	ctx.f28.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// lfs f23,16(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f25,f4,f1
	ctx.f25.f64 = double(float(ctx.f4.f64 * ctx.f1.f64));
	// lfs f24,20(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f3,f13,f27,f3
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f27.f64 + ctx.f3.f64));
	// lfs f26,24(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 24);
	ctx.f26.f64 = double(temp.f32);
	// fadds f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 + ctx.f12.f64));
	// lfs f6,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// fadds f9,f8,f9
	ctx.f9.f64 = double(float(ctx.f8.f64 + ctx.f9.f64));
	// lfs f8,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f22,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f22.f64 = double(temp.f32);
	// fadds f5,f5,f10
	ctx.f5.f64 = double(float(ctx.f5.f64 + ctx.f10.f64));
	// lfs f10,12(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// lwz r8,184(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 184);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r9,r1,172
	ctx.r9.s64 = ctx.r1.s64 + 172;
	// fmadds f2,f13,f4,f2
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f4.f64 + ctx.f2.f64));
	// fmadds f28,f0,f13,f28
	ctx.f28.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 + ctx.f28.f64));
	// fmsubs f13,f13,f29,f25
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f29.f64 - ctx.f25.f64));
	// fmadds f3,f0,f1,f3
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f1.f64 + ctx.f3.f64));
	// fmadds f2,f29,f1,f2
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f1.f64 + ctx.f2.f64));
	// fmadds f29,f7,f29,f28
	ctx.f29.f64 = double(float(ctx.f7.f64 * ctx.f29.f64 + ctx.f28.f64));
	// fnmsubs f13,f0,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f0.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// fnmsubs f7,f7,f4,f3
	ctx.f7.f64 = double(float(-(ctx.f7.f64 * ctx.f4.f64 - ctx.f3.f64)));
	// fnmsubs f4,f0,f11,f2
	ctx.f4.f64 = double(float(-(ctx.f0.f64 * ctx.f11.f64 - ctx.f2.f64)));
	// fnmsubs f3,f27,f1,f29
	ctx.f3.f64 = double(float(-(ctx.f27.f64 * ctx.f1.f64 - ctx.f29.f64)));
	// fnmsubs f2,f11,f27,f13
	ctx.f2.f64 = double(float(-(ctx.f11.f64 * ctx.f27.f64 - ctx.f13.f64)));
	// fmuls f1,f24,f7
	ctx.f1.f64 = double(float(ctx.f24.f64 * ctx.f7.f64));
	// fmuls f28,f6,f7
	ctx.f28.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// fmuls f0,f4,f26
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f26.f64));
	// fmuls f13,f23,f3
	ctx.f13.f64 = double(float(ctx.f23.f64 * ctx.f3.f64));
	// fmuls f11,f24,f3
	ctx.f11.f64 = double(float(ctx.f24.f64 * ctx.f3.f64));
	// fmsubs f30,f2,f2,f30
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f2.f64 - ctx.f30.f64));
	// fmuls f27,f8,f2
	ctx.f27.f64 = double(float(ctx.f8.f64 * ctx.f2.f64));
	// fmsubs f1,f26,f3,f1
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f3.f64 - ctx.f1.f64));
	// fmuls f29,f6,f2
	ctx.f29.f64 = double(float(ctx.f6.f64 * ctx.f2.f64));
	// fmadds f28,f22,f2,f28
	ctx.f28.f64 = double(float(ctx.f22.f64 * ctx.f2.f64 + ctx.f28.f64));
	// fmuls f25,f6,f4
	ctx.f25.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// fmsubs f0,f23,f7,f0
	ctx.f0.f64 = double(float(ctx.f23.f64 * ctx.f7.f64 - ctx.f0.f64));
	// fmsubs f13,f4,f24,f13
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f24.f64 - ctx.f13.f64));
	// fmadds f11,f4,f23,f11
	ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f23.f64 + ctx.f11.f64));
	// fmuls f23,f30,f23
	ctx.f23.f64 = double(float(ctx.f30.f64 * ctx.f23.f64));
	// fmuls f24,f30,f24
	ctx.f24.f64 = double(float(ctx.f30.f64 * ctx.f24.f64));
	// fmadds f27,f22,f4,f27
	ctx.f27.f64 = double(float(ctx.f22.f64 * ctx.f4.f64 + ctx.f27.f64));
	// fmuls f30,f30,f26
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f26.f64));
	// fmuls f1,f1,f2
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f2.f64));
	// fmadds f29,f10,f4,f29
	ctx.f29.f64 = double(float(ctx.f10.f64 * ctx.f4.f64 + ctx.f29.f64));
	// fmadds f28,f10,f3,f28
	ctx.f28.f64 = double(float(ctx.f10.f64 * ctx.f3.f64 + ctx.f28.f64));
	// fmuls f0,f0,f2
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// fmuls f13,f13,f2
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f2.f64));
	// fmadds f11,f26,f7,f11
	ctx.f11.f64 = double(float(ctx.f26.f64 * ctx.f7.f64 + ctx.f11.f64));
	// fmsubs f2,f10,f2,f25
	ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f2.f64 - ctx.f25.f64));
	// fmadds f10,f10,f7,f27
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f7.f64 + ctx.f27.f64));
	// fadds f1,f23,f1
	ctx.f1.f64 = double(float(ctx.f23.f64 + ctx.f1.f64));
	// fmadds f29,f8,f3,f29
	ctx.f29.f64 = double(float(ctx.f8.f64 * ctx.f3.f64 + ctx.f29.f64));
	// fadds f0,f24,f0
	ctx.f0.f64 = double(float(ctx.f24.f64 + ctx.f0.f64));
	// fadds f13,f30,f13
	ctx.f13.f64 = double(float(ctx.f30.f64 + ctx.f13.f64));
	// fmuls f30,f11,f4
	ctx.f30.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// fmuls f27,f11,f3
	ctx.f27.f64 = double(float(ctx.f11.f64 * ctx.f3.f64));
	// fmuls f11,f11,f7
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// fnmsubs f2,f22,f3,f2
	ctx.f2.f64 = double(float(-(ctx.f22.f64 * ctx.f3.f64 - ctx.f2.f64)));
	// fnmsubs f4,f8,f4,f28
	ctx.f4.f64 = double(float(-(ctx.f8.f64 * ctx.f4.f64 - ctx.f28.f64)));
	// stfs f4,84(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f3,f6,f3,f10
	ctx.f3.f64 = double(float(-(ctx.f6.f64 * ctx.f3.f64 - ctx.f10.f64)));
	// fnmsubs f29,f22,f7,f29
	ctx.f29.f64 = double(float(-(ctx.f22.f64 * ctx.f7.f64 - ctx.f29.f64)));
	// stfs f29,80(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fadds f1,f1,f30
	ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f30.f64));
	// fadds f0,f0,f27
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f27.f64));
	// fadds f13,f13,f11
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f11.f64));
	// fnmsubs f11,f8,f7,f2
	ctx.f11.f64 = double(float(-(ctx.f8.f64 * ctx.f7.f64 - ctx.f2.f64)));
	// fmuls f10,f1,f31
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// fmuls f8,f0,f31
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fmuls f7,f13,f31
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// fadds f6,f10,f12
	ctx.f6.f64 = double(float(ctx.f10.f64 + ctx.f12.f64));
	// stfs f6,96(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f2,f8,f9
	ctx.f2.f64 = double(float(ctx.f8.f64 + ctx.f9.f64));
	// stfs f2,100(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f1,f7,f5
	ctx.f1.f64 = double(float(ctx.f7.f64 + ctx.f5.f64));
	// stfs f1,104(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// stfs f11,92(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lwz r7,96(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stfs f3,88(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lwz r6,4(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r8,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r8.u32);
	// stw r6,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r6.u32);
	// lwz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r11,8(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lwz r10,12(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// stw r5,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r5.u32);
	// stw r11,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r11.u32);
	// stw r10,12(r9)
	PPC_STORE_U32(ctx.r9.u32 + 12, ctx.r10.u32);
	// stw r7,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r7.u32);
	// b 0x830c0074
	goto loc_830C0074;
loc_830BFF24:
	// lfs f0,0(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lfs f13,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f11,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// lfs f5,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f5.f64 = double(temp.f32);
	// lfs f3,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// fmsubs f4,f5,f5,f30
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f5.f64 - ctx.f30.f64));
	// lfs f8,24(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 24);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f1,f3,f13
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// lfs f6,16(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// addi r10,r1,172
	ctx.r10.s64 = ctx.r1.s64 + 172;
	// lfs f7,20(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	ctx.f7.f64 = double(temp.f32);
	// stw r29,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r29.u32);
	// lfs f2,12(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f2.f64 = double(temp.f32);
	// lfs f30,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f29,f2,f11
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// fmuls f28,f30,f11
	ctx.f28.f64 = double(float(ctx.f30.f64 * ctx.f11.f64));
	// lfs f27,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f26,f12,f8
	ctx.f26.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// lfs f25,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f24,f6,f10
	ctx.f24.f64 = double(float(ctx.f6.f64 * ctx.f10.f64));
	// lfs f23,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f22,f7,f9
	ctx.f22.f64 = double(float(ctx.f7.f64 * ctx.f9.f64));
	// lfs f21,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f20,f7,f10
	ctx.f20.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
	// fmuls f17,f4,f7
	ctx.f17.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// fmuls f18,f4,f6
	ctx.f18.f64 = double(float(ctx.f4.f64 * ctx.f6.f64));
	// fmadds f1,f30,f5,f1
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f5.f64 + ctx.f1.f64));
	// fmuls f19,f30,f0
	ctx.f19.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmadds f29,f3,f5,f29
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f5.f64 + ctx.f29.f64));
	// fmuls f4,f4,f8
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f8.f64));
	// fmsubs f26,f6,f9,f26
	ctx.f26.f64 = double(float(ctx.f6.f64 * ctx.f9.f64 - ctx.f26.f64));
	// fmsubs f7,f12,f7,f24
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f7.f64 - ctx.f24.f64));
	// fmsubs f24,f8,f10,f22
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f10.f64 - ctx.f22.f64));
	// fmadds f6,f12,f6,f20
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f6.f64 + ctx.f20.f64));
	// fmadds f28,f27,f5,f28
	ctx.f28.f64 = double(float(ctx.f27.f64 * ctx.f5.f64 + ctx.f28.f64));
	// fmadds f1,f2,f0,f1
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64 + ctx.f1.f64));
	// fmsubs f22,f2,f5,f19
	ctx.f22.f64 = double(float(ctx.f2.f64 * ctx.f5.f64 - ctx.f19.f64));
	// fmuls f26,f5,f26
	ctx.f26.f64 = double(float(ctx.f5.f64 * ctx.f26.f64));
	// fmuls f7,f5,f7
	ctx.f7.f64 = double(float(ctx.f5.f64 * ctx.f7.f64));
	// fmuls f5,f24,f5
	ctx.f5.f64 = double(float(ctx.f24.f64 * ctx.f5.f64));
	// fmadds f8,f8,f9,f6
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f9.f64 + ctx.f6.f64));
	// fmadds f6,f2,f13,f28
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f13.f64 + ctx.f28.f64));
	// fmadds f2,f27,f0,f29
	ctx.f2.f64 = double(float(ctx.f27.f64 * ctx.f0.f64 + ctx.f29.f64));
	// fnmsubs f1,f27,f11,f1
	ctx.f1.f64 = double(float(-(ctx.f27.f64 * ctx.f11.f64 - ctx.f1.f64)));
	// stfs f1,80(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fnmsubs f29,f27,f13,f22
	ctx.f29.f64 = double(float(-(ctx.f27.f64 * ctx.f13.f64 - ctx.f22.f64)));
	// fadds f1,f17,f26
	ctx.f1.f64 = double(float(ctx.f17.f64 + ctx.f26.f64));
	// fadds f7,f4,f7
	ctx.f7.f64 = double(float(ctx.f4.f64 + ctx.f7.f64));
	// fadds f5,f18,f5
	ctx.f5.f64 = double(float(ctx.f18.f64 + ctx.f5.f64));
	// fmuls f4,f8,f12
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f12.f64));
	// fmuls f12,f8,f10
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f10.f64));
	// fmuls f10,f8,f9
	ctx.f10.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// fnmsubs f8,f30,f13,f2
	ctx.f8.f64 = double(float(-(ctx.f30.f64 * ctx.f13.f64 - ctx.f2.f64)));
	// stfs f8,88(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fnmsubs f9,f3,f0,f6
	ctx.f9.f64 = double(float(-(ctx.f3.f64 * ctx.f0.f64 - ctx.f6.f64)));
	// stfs f9,84(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f6,f3,f11,f29
	ctx.f6.f64 = double(float(-(ctx.f3.f64 * ctx.f11.f64 - ctx.f29.f64)));
	// stfs f6,92(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fadds f5,f5,f4
	ctx.f5.f64 = double(float(ctx.f5.f64 + ctx.f4.f64));
	// fadds f4,f1,f12
	ctx.f4.f64 = double(float(ctx.f1.f64 + ctx.f12.f64));
	// fadds f3,f7,f10
	ctx.f3.f64 = double(float(ctx.f7.f64 + ctx.f10.f64));
	// fmuls f2,f5,f31
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// fmuls f1,f4,f31
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// fmuls f0,f3,f31
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// fadds f13,f2,f25
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f25.f64));
	// stfs f13,96(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f12,f23,f1
	ctx.f12.f64 = double(float(ctx.f23.f64 + ctx.f1.f64));
	// stfs f12,100(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f11,f21,f0
	ctx.f11.f64 = double(float(ctx.f21.f64 + ctx.f0.f64));
	// stfs f11,104(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r6,12(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r5,96(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r7,8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// stw r7,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r7.u32);
	// stw r6,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r6.u32);
	// stw r5,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r5.u32);
loc_830C0074:
	// lwz r3,104(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// addi r5,r1,192
	ctx.r5.s64 = ctx.r1.s64 + 192;
	// lwz r4,100(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r11,152(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 152);
	// stw r3,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r3.u32);
	// lwz r3,336(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 336);
	// stw r4,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r4.u32);
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r11.u32);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,76(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 76);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830C00A8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// stw r3,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r3.u32);
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// li r9,7
	ctx.r9.s64 = 7;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_830C00BC:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c00bc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C00BC;
	// lwz r11,268(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 268);
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// lwz r31,0(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// lwz r3,1412(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1412);
	// bl 0x8315fb80
	ctx.lr = 0x830C00E4;
	sub_8315FB80(ctx, base);
	// stwx r3,r26,r31
	PPC_STORE_U32(ctx.r26.u32 + ctx.r31.u32, ctx.r3.u32);
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// add r11,r26,r11
	ctx.r11.u64 = ctx.r26.u64 + ctx.r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830c0150
	if (ctx.cr6.eq) goto loc_830C0150;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c0150
	if (ctx.cr6.eq) goto loc_830C0150;
	// lwz r29,20(r11)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// lwz r31,16(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// cmplw cr6,r31,r29
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r29.u32, ctx.xer);
	// beq cr6,0x830c0150
	if (ctx.cr6.eq) goto loc_830C0150;
loc_830C0118:
	// lwz r30,0(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C0130;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x830c0144
	if (ctx.cr6.eq) goto loc_830C0144;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x830bdd70
	ctx.lr = 0x830C0144;
	sub_830BDD70(ctx, base);
loc_830C0144:
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// cmplw cr6,r31,r29
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r29.u32, ctx.xer);
	// bne cr6,0x830c0118
	if (!ctx.cr6.eq) goto loc_830C0118;
loc_830C0150:
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// add r11,r26,r11
	ctx.r11.u64 = ctx.r26.u64 + ctx.r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x830c0818
	if (!ctx.cr6.eq) goto loc_830C0818;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// rlwinm r9,r10,0,2,0
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFBFFFFFFF;
	// stw r9,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r9.u32);
	// b 0x830c0818
	goto loc_830C0818;
loc_830C0174:
	// li r29,0
	ctx.r29.s64 = 0;
	// stb r29,12(r11)
	PPC_STORE_U8(ctx.r11.u32 + 12, ctx.r29.u8);
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// lwzx r9,r26,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r10.u32);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x830c0818
	if (!ctx.cr6.eq) goto loc_830C0818;
	// lfs f13,116(r28)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 116);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f0,112(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 112);
	ctx.f0.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// lfs f11,120(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 120);
	ctx.f11.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// lfs f5,124(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 124);
	ctx.f5.f64 = double(temp.f32);
	// lfs f7,176(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 176);
	ctx.f7.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f30,6380(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6380);
	ctx.f30.f64 = double(temp.f32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lfs f8,184(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 184);
	ctx.f8.f64 = double(temp.f32);
	// fmsubs f4,f5,f5,f30
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f5.f64 - ctx.f30.f64));
	// lfs f6,180(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 180);
	ctx.f6.f64 = double(temp.f32);
	// addi r31,r28,112
	ctx.r31.s64 = ctx.r28.s64 + 112;
	// lfs f3,160(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 160);
	ctx.f3.f64 = double(temp.f32);
	// addi r30,r28,160
	ctx.r30.s64 = ctx.r28.s64 + 160;
	// lfs f2,172(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 172);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f3,f11
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// fmuls f29,f2,f0
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// lfs f26,164(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 164);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f25,f7,f10
	ctx.f25.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
	// lfs f28,168(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 168);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f27,f12,f8
	ctx.f27.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// lfs f31,7676(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 7676);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f24,f8,f9
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// lfs f23,132(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 132);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f22,f6,f9
	ctx.f22.f64 = double(float(ctx.f6.f64 * ctx.f9.f64));
	// lfs f21,136(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 136);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f20,f2,f11
	ctx.f20.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// lfs f19,128(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 128);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f17,f4,f6
	ctx.f17.f64 = double(float(ctx.f4.f64 * ctx.f6.f64));
	// addi r9,r1,124
	ctx.r9.s64 = ctx.r1.s64 + 124;
	// fmuls f16,f4,f8
	ctx.f16.f64 = double(float(ctx.f4.f64 * ctx.f8.f64));
	// fmuls f18,f3,f0
	ctx.f18.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmuls f4,f4,f7
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// fmadds f1,f26,f5,f1
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f5.f64 + ctx.f1.f64));
	// fmsubs f25,f12,f6,f25
	ctx.f25.f64 = double(float(ctx.f12.f64 * ctx.f6.f64 - ctx.f25.f64));
	// fmsubs f27,f7,f9,f27
	ctx.f27.f64 = double(float(ctx.f7.f64 * ctx.f9.f64 - ctx.f27.f64));
	// fmadds f6,f6,f10,f24
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f10.f64 + ctx.f24.f64));
	// fmsubs f8,f8,f10,f22
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f10.f64 - ctx.f22.f64));
	// fmadds f29,f3,f5,f29
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f5.f64 + ctx.f29.f64));
	// fmadds f24,f28,f5,f20
	ctx.f24.f64 = double(float(ctx.f28.f64 * ctx.f5.f64 + ctx.f20.f64));
	// fmsubs f22,f2,f5,f18
	ctx.f22.f64 = double(float(ctx.f2.f64 * ctx.f5.f64 - ctx.f18.f64));
	// fmadds f2,f2,f13,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f13.f64 + ctx.f1.f64));
	// fmuls f25,f5,f25
	ctx.f25.f64 = double(float(ctx.f5.f64 * ctx.f25.f64));
	// fmuls f27,f5,f27
	ctx.f27.f64 = double(float(ctx.f5.f64 * ctx.f27.f64));
	// fmadds f7,f12,f7,f6
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f7.f64 + ctx.f6.f64));
	// fmuls f6,f8,f5
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// fmadds f5,f28,f13,f29
	ctx.f5.f64 = double(float(ctx.f28.f64 * ctx.f13.f64 + ctx.f29.f64));
	// fmadds f1,f26,f0,f24
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f0.f64 + ctx.f24.f64));
	// fnmsubs f8,f26,f13,f22
	ctx.f8.f64 = double(float(-(ctx.f26.f64 * ctx.f13.f64 - ctx.f22.f64)));
	// fadds f29,f17,f27
	ctx.f29.f64 = double(float(ctx.f17.f64 + ctx.f27.f64));
	// fmuls f10,f7,f10
	ctx.f10.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
	// fmuls f9,f7,f9
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f9.f64));
	// fmuls f7,f7,f12
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// fadds f6,f4,f6
	ctx.f6.f64 = double(float(ctx.f4.f64 + ctx.f6.f64));
	// fadds f27,f16,f25
	ctx.f27.f64 = double(float(ctx.f16.f64 + ctx.f25.f64));
	// fnmsubs f4,f28,f0,f2
	ctx.f4.f64 = double(float(-(ctx.f28.f64 * ctx.f0.f64 - ctx.f2.f64)));
	// stfs f4,84(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f3,f3,f13,f1
	ctx.f3.f64 = double(float(-(ctx.f3.f64 * ctx.f13.f64 - ctx.f1.f64)));
	// stfs f3,88(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fnmsubs f2,f28,f11,f8
	ctx.f2.f64 = double(float(-(ctx.f28.f64 * ctx.f11.f64 - ctx.f8.f64)));
	// stfs f2,92(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fnmsubs f5,f26,f11,f5
	ctx.f5.f64 = double(float(-(ctx.f26.f64 * ctx.f11.f64 - ctx.f5.f64)));
	// stfs f5,80(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fadds f1,f29,f10
	ctx.f1.f64 = double(float(ctx.f29.f64 + ctx.f10.f64));
	// fadds f13,f6,f7
	ctx.f13.f64 = double(float(ctx.f6.f64 + ctx.f7.f64));
	// fadds f0,f27,f9
	ctx.f0.f64 = double(float(ctx.f27.f64 + ctx.f9.f64));
	// fmuls f12,f1,f31
	ctx.f12.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// fmuls f10,f13,f31
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// fmuls f11,f0,f31
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fadds f9,f23,f12
	ctx.f9.f64 = double(float(ctx.f23.f64 + ctx.f12.f64));
	// stfs f9,100(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f7,f10,f19
	ctx.f7.f64 = double(float(ctx.f10.f64 + ctx.f19.f64));
	// stfs f7,96(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f8,f21,f11
	ctx.f8.f64 = double(float(ctx.f21.f64 + ctx.f11.f64));
	// stfs f8,104(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// lwz r6,96(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r7,0(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r11,4(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// lwz r10,8(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// lwz r8,12(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lwz r5,100(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r4,104(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r11.u32);
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// stw r10,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r10.u32);
	// stw r8,12(r9)
	PPC_STORE_U32(ctx.r9.u32 + 12, ctx.r8.u32);
	// stw r6,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r6.u32);
	// stw r5,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r5.u32);
	// stw r4,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r4.u32);
	// bl 0x8315fb70
	ctx.lr = 0x830C0308;
	sub_8315FB70(ctx, base);
	// lwz r11,264(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 264);
	// li r7,10
	ctx.r7.s64 = 10;
	// stw r7,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r7.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c05ec
	if (ctx.cr6.eq) goto loc_830C05EC;
	// lfs f0,216(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 216);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r11,216
	ctx.r10.s64 = ctx.r11.s64 + 216;
	// lfs f5,240(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 240);
	ctx.f5.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f13,220(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 220);
	ctx.f13.f64 = double(temp.f32);
	// fneg f3,f5
	ctx.f3.u64 = ctx.f5.u64 ^ 0x8000000000000000;
	// lfs f11,224(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 224);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// lfs f9,232(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 232);
	ctx.f9.f64 = double(temp.f32);
	// fmr f8,f11
	ctx.f8.f64 = ctx.f11.f64;
	// lfs f7,236(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 236);
	ctx.f7.f64 = double(temp.f32);
	// fneg f6,f9
	ctx.f6.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// fneg f4,f7
	ctx.f4.u64 = ctx.f7.u64 ^ 0x8000000000000000;
	// lfs f2,228(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 228);
	ctx.f2.f64 = double(temp.f32);
	// fneg f1,f0
	ctx.f1.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lfs f0,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f0.f64 = double(temp.f32);
	// fneg f11,f11
	ctx.f11.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// lfs f9,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f9.f64 = double(temp.f32);
	// fneg f7,f13
	ctx.f7.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// lfs f5,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f5.f64 = double(temp.f32);
	// fmsubs f28,f2,f2,f30
	ctx.f28.f64 = double(float(ctx.f2.f64 * ctx.f2.f64 - ctx.f30.f64));
	// lfs f29,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f29.f64 = double(temp.f32);
	// fmr f13,f2
	ctx.f13.f64 = ctx.f2.f64;
	// lfs f27,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f24,f12,f3
	ctx.f24.f64 = double(float(ctx.f12.f64 * ctx.f3.f64));
	// fmuls f21,f8,f3
	ctx.f21.f64 = double(float(ctx.f8.f64 * ctx.f3.f64));
	// fmuls f26,f6,f10
	ctx.f26.f64 = double(float(ctx.f6.f64 * ctx.f10.f64));
	// fmuls f25,f8,f4
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// fmuls f23,f1,f0
	ctx.f23.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f20,f0,f11
	ctx.f20.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// fmuls f19,f5,f7
	ctx.f19.f64 = double(float(ctx.f5.f64 * ctx.f7.f64));
	// fmuls f22,f9,f11
	ctx.f22.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// fmuls f17,f28,f6
	ctx.f17.f64 = double(float(ctx.f28.f64 * ctx.f6.f64));
	// fmsubs f24,f6,f8,f24
	ctx.f24.f64 = double(float(ctx.f6.f64 * ctx.f8.f64 - ctx.f24.f64));
	// fmsubs f18,f13,f13,f30
	ctx.f18.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 - ctx.f30.f64));
	// fmadds f6,f12,f6,f21
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f6.f64 + ctx.f21.f64));
	// fmsubs f26,f12,f4,f26
	ctx.f26.f64 = double(float(ctx.f12.f64 * ctx.f4.f64 - ctx.f26.f64));
	// fmsubs f25,f10,f3,f25
	ctx.f25.f64 = double(float(ctx.f10.f64 * ctx.f3.f64 - ctx.f25.f64));
	// fmsubs f23,f5,f11,f23
	ctx.f23.f64 = double(float(ctx.f5.f64 * ctx.f11.f64 - ctx.f23.f64));
	// fmadds f21,f9,f7,f20
	ctx.f21.f64 = double(float(ctx.f9.f64 * ctx.f7.f64 + ctx.f20.f64));
	// fmsubs f20,f1,f9,f19
	ctx.f20.f64 = double(float(ctx.f1.f64 * ctx.f9.f64 - ctx.f19.f64));
	// fmsubs f22,f0,f7,f22
	ctx.f22.f64 = double(float(ctx.f0.f64 * ctx.f7.f64 - ctx.f22.f64));
	// fmuls f19,f28,f4
	ctx.f19.f64 = double(float(ctx.f28.f64 * ctx.f4.f64));
	// fmuls f3,f28,f3
	ctx.f3.f64 = double(float(ctx.f28.f64 * ctx.f3.f64));
	// fmuls f28,f5,f18
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f18.f64));
	// fmadds f6,f10,f4,f6
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f4.f64 + ctx.f6.f64));
	// fmuls f26,f2,f26
	ctx.f26.f64 = double(float(ctx.f2.f64 * ctx.f26.f64));
	// fmuls f25,f25,f2
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f2.f64));
	// fmuls f4,f2,f24
	ctx.f4.f64 = double(float(ctx.f2.f64 * ctx.f24.f64));
	// fmadds f5,f1,f5,f21
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f5.f64 + ctx.f21.f64));
	// fmuls f9,f9,f18
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f18.f64));
	// fmuls f2,f23,f13
	ctx.f2.f64 = double(float(ctx.f23.f64 * ctx.f13.f64));
	// fmuls f0,f0,f18
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f18.f64));
	// fmuls f24,f13,f22
	ctx.f24.f64 = double(float(ctx.f13.f64 * ctx.f22.f64));
	// fmuls f23,f20,f13
	ctx.f23.f64 = double(float(ctx.f20.f64 * ctx.f13.f64));
	// fmuls f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// fsubs f3,f3,f26
	ctx.f3.f64 = double(float(ctx.f3.f64 - ctx.f26.f64));
	// fmuls f10,f6,f10
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f10.f64));
	// fmuls f8,f6,f8
	ctx.f8.f64 = double(float(ctx.f6.f64 * ctx.f8.f64));
	// fsubs f26,f17,f25
	ctx.f26.f64 = double(float(ctx.f17.f64 - ctx.f25.f64));
	// fmuls f6,f1,f5
	ctx.f6.f64 = double(float(ctx.f1.f64 * ctx.f5.f64));
	// fadds f2,f9,f2
	ctx.f2.f64 = double(float(ctx.f9.f64 + ctx.f2.f64));
	// fmuls f25,f7,f5
	ctx.f25.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// fsubs f4,f19,f4
	ctx.f4.f64 = double(float(ctx.f19.f64 - ctx.f4.f64));
	// fmuls f5,f11,f5
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f5.f64));
	// fadds f9,f28,f24
	ctx.f9.f64 = double(float(ctx.f28.f64 + ctx.f24.f64));
	// fadds f0,f0,f23
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f23.f64));
	// fadds f8,f3,f8
	ctx.f8.f64 = double(float(ctx.f3.f64 + ctx.f8.f64));
	// fadds f12,f26,f12
	ctx.f12.f64 = double(float(ctx.f26.f64 + ctx.f12.f64));
	// fadds f3,f2,f25
	ctx.f3.f64 = double(float(ctx.f2.f64 + ctx.f25.f64));
	// fadds f4,f4,f10
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f10.f64));
	// fadds f2,f9,f6
	ctx.f2.f64 = double(float(ctx.f9.f64 + ctx.f6.f64));
	// fadds f0,f0,f5
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f5.f64));
	// fmuls f10,f8,f31
	ctx.f10.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fmuls f12,f12,f31
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// fmuls f8,f3,f31
	ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// lfs f3,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f9,f4,f31
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// fmuls f4,f29,f13
	ctx.f4.f64 = double(float(ctx.f29.f64 * ctx.f13.f64));
	// fmuls f6,f2,f31
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f5,f0,f31
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fmuls f28,f27,f11
	ctx.f28.f64 = double(float(ctx.f27.f64 * ctx.f11.f64));
	// lfs f0,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f2,f27,f13
	ctx.f2.f64 = double(float(ctx.f27.f64 * ctx.f13.f64));
	// lfs f23,16(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f4,f1,f0,f4
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 + ctx.f4.f64));
	// lfs f26,24(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 24);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f25,f1,f27
	ctx.f25.f64 = double(float(ctx.f1.f64 * ctx.f27.f64));
	// lfs f24,20(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	ctx.f24.f64 = double(temp.f32);
	// fadds f12,f12,f6
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f6.f64));
	// lfs f6,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// fadds f9,f8,f9
	ctx.f9.f64 = double(float(ctx.f8.f64 + ctx.f9.f64));
	// lfs f8,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lfs f22,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f22.f64 = double(temp.f32);
	// fadds f5,f5,f10
	ctx.f5.f64 = double(float(ctx.f5.f64 + ctx.f10.f64));
	// lfs f10,12(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// lwz r8,184(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 184);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r9,r1,236
	ctx.r9.s64 = ctx.r1.s64 + 236;
	// fmadds f28,f0,f13,f28
	ctx.f28.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 + ctx.f28.f64));
	// fmadds f2,f1,f3,f2
	ctx.f2.f64 = double(float(ctx.f1.f64 * ctx.f3.f64 + ctx.f2.f64));
	// fmadds f4,f3,f11,f4
	ctx.f4.f64 = double(float(ctx.f3.f64 * ctx.f11.f64 + ctx.f4.f64));
	// fmsubs f13,f3,f13,f25
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f13.f64 - ctx.f25.f64));
	// fmadds f3,f3,f7,f28
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f7.f64 + ctx.f28.f64));
	// fmadds f2,f29,f7,f2
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f7.f64 + ctx.f2.f64));
	// fnmsubs f13,f0,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f0.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// fnmsubs f7,f27,f7,f4
	ctx.f7.f64 = double(float(-(ctx.f27.f64 * ctx.f7.f64 - ctx.f4.f64)));
	// fnmsubs f3,f1,f29,f3
	ctx.f3.f64 = double(float(-(ctx.f1.f64 * ctx.f29.f64 - ctx.f3.f64)));
	// fnmsubs f4,f0,f11,f2
	ctx.f4.f64 = double(float(-(ctx.f0.f64 * ctx.f11.f64 - ctx.f2.f64)));
	// fnmsubs f2,f29,f11,f13
	ctx.f2.f64 = double(float(-(ctx.f29.f64 * ctx.f11.f64 - ctx.f13.f64)));
	// fmuls f1,f24,f7
	ctx.f1.f64 = double(float(ctx.f24.f64 * ctx.f7.f64));
	// fmuls f11,f26,f7
	ctx.f11.f64 = double(float(ctx.f26.f64 * ctx.f7.f64));
	// fmuls f28,f7,f8
	ctx.f28.f64 = double(float(ctx.f7.f64 * ctx.f8.f64));
	// fmuls f13,f23,f3
	ctx.f13.f64 = double(float(ctx.f23.f64 * ctx.f3.f64));
	// fmuls f0,f26,f4
	ctx.f0.f64 = double(float(ctx.f26.f64 * ctx.f4.f64));
	// fmuls f29,f3,f6
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f6.f64));
	// fmsubs f30,f2,f2,f30
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f2.f64 - ctx.f30.f64));
	// fmuls f27,f2,f6
	ctx.f27.f64 = double(float(ctx.f2.f64 * ctx.f6.f64));
	// fmadds f11,f24,f3,f11
	ctx.f11.f64 = double(float(ctx.f24.f64 * ctx.f3.f64 + ctx.f11.f64));
	// fmsubs f1,f26,f3,f1
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f3.f64 - ctx.f1.f64));
	// fmuls f25,f4,f8
	ctx.f25.f64 = double(float(ctx.f4.f64 * ctx.f8.f64));
	// fmadds f28,f2,f22,f28
	ctx.f28.f64 = double(float(ctx.f2.f64 * ctx.f22.f64 + ctx.f28.f64));
	// fmsubs f13,f24,f4,f13
	ctx.f13.f64 = double(float(ctx.f24.f64 * ctx.f4.f64 - ctx.f13.f64));
	// fmsubs f0,f23,f7,f0
	ctx.f0.f64 = double(float(ctx.f23.f64 * ctx.f7.f64 - ctx.f0.f64));
	// fmadds f29,f2,f8,f29
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f8.f64 + ctx.f29.f64));
	// fmuls f21,f23,f30
	ctx.f21.f64 = double(float(ctx.f23.f64 * ctx.f30.f64));
	// fmuls f24,f24,f30
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f30.f64));
	// fmadds f27,f7,f10,f27
	ctx.f27.f64 = double(float(ctx.f7.f64 * ctx.f10.f64 + ctx.f27.f64));
	// fmuls f30,f26,f30
	ctx.f30.f64 = double(float(ctx.f26.f64 * ctx.f30.f64));
	// fmadds f11,f23,f4,f11
	ctx.f11.f64 = double(float(ctx.f23.f64 * ctx.f4.f64 + ctx.f11.f64));
	// fmuls f1,f2,f1
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// fmuls f13,f13,f2
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f2.f64));
	// fmuls f0,f0,f2
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// fmadds f29,f4,f10,f29
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f10.f64 + ctx.f29.f64));
	// fmsubs f2,f2,f10,f25
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f10.f64 - ctx.f25.f64));
	// fmadds f10,f3,f10,f28
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f10.f64 + ctx.f28.f64));
	// fmadds f28,f4,f22,f27
	ctx.f28.f64 = double(float(ctx.f4.f64 * ctx.f22.f64 + ctx.f27.f64));
	// fmuls f27,f3,f11
	ctx.f27.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// fadds f1,f21,f1
	ctx.f1.f64 = double(float(ctx.f21.f64 + ctx.f1.f64));
	// fadds f13,f30,f13
	ctx.f13.f64 = double(float(ctx.f30.f64 + ctx.f13.f64));
	// fmuls f30,f11,f4
	ctx.f30.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// fmuls f11,f7,f11
	ctx.f11.f64 = double(float(ctx.f7.f64 * ctx.f11.f64));
	// fadds f0,f24,f0
	ctx.f0.f64 = double(float(ctx.f24.f64 + ctx.f0.f64));
	// fnmsubs f10,f4,f6,f10
	ctx.f10.f64 = double(float(-(ctx.f4.f64 * ctx.f6.f64 - ctx.f10.f64)));
	// fnmsubs f2,f3,f22,f2
	ctx.f2.f64 = double(float(-(ctx.f3.f64 * ctx.f22.f64 - ctx.f2.f64)));
	// fnmsubs f8,f3,f8,f28
	ctx.f8.f64 = double(float(-(ctx.f3.f64 * ctx.f8.f64 - ctx.f28.f64)));
	// stfs f8,88(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fnmsubs f29,f7,f22,f29
	ctx.f29.f64 = double(float(-(ctx.f7.f64 * ctx.f22.f64 - ctx.f29.f64)));
	// stfs f29,80(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fadds f4,f1,f30
	ctx.f4.f64 = double(float(ctx.f1.f64 + ctx.f30.f64));
	// fadds f1,f13,f11
	ctx.f1.f64 = double(float(ctx.f13.f64 + ctx.f11.f64));
	// fadds f3,f0,f27
	ctx.f3.f64 = double(float(ctx.f0.f64 + ctx.f27.f64));
	// fnmsubs f0,f7,f6,f2
	ctx.f0.f64 = double(float(-(ctx.f7.f64 * ctx.f6.f64 - ctx.f2.f64)));
	// fmuls f13,f4,f31
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// fmuls f7,f1,f31
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// fmuls f11,f3,f31
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// fadds f6,f13,f12
	ctx.f6.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// stfs f6,96(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f3,f7,f5
	ctx.f3.f64 = double(float(ctx.f7.f64 + ctx.f5.f64));
	// stfs f3,104(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fadds f4,f11,f9
	ctx.f4.f64 = double(float(ctx.f11.f64 + ctx.f9.f64));
	// stfs f4,100(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stfs f0,92(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lwz r7,96(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stfs f10,84(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lwz r3,4(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r6,100(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r5,104(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stw r8,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r8.u32);
	// stw r3,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r3.u32);
	// lwz r4,8(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lwz r11,12(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r4,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r4.u32);
	// stw r11,12(r9)
	PPC_STORE_U32(ctx.r9.u32 + 12, ctx.r11.u32);
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// stw r7,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r7.u32);
	// stw r6,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, ctx.r6.u32);
	// stw r5,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, ctx.r5.u32);
	// b 0x830c074c
	goto loc_830C074C;
loc_830C05EC:
	// lfs f0,4(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lfs f13,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f10,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fmr f11,f13
	ctx.f11.f64 = ctx.f13.f64;
	// fmr f8,f10
	ctx.f8.f64 = ctx.f10.f64;
	// lfs f5,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f5.f64 = double(temp.f32);
	// lfs f6,16(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// fmsubs f4,f5,f5,f30
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f5.f64 - ctx.f30.f64));
	// lfs f3,12(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f3.f64 = double(temp.f32);
	// addi r10,r1,236
	ctx.r10.s64 = ctx.r1.s64 + 236;
	// lfs f9,24(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 24);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f1,f3,f13
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// lfs f7,20(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	ctx.f7.f64 = double(temp.f32);
	// stw r29,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r29.u32);
	// lfs f2,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// lfs f30,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f29,f2,f10
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f10.f64));
	// fmuls f28,f30,f0
	ctx.f28.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// lfs f27,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f26,f12,f6
	ctx.f26.f64 = double(float(ctx.f12.f64 * ctx.f6.f64));
	// lfs f25,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f22,f11,f9
	ctx.f22.f64 = double(float(ctx.f11.f64 * ctx.f9.f64));
	// lfs f23,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f24,f11,f7
	ctx.f24.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// lfs f21,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f20,f8,f9
	ctx.f20.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// fmuls f17,f4,f9
	ctx.f17.f64 = double(float(ctx.f4.f64 * ctx.f9.f64));
	// fmuls f18,f4,f6
	ctx.f18.f64 = double(float(ctx.f4.f64 * ctx.f6.f64));
	// fmuls f4,f4,f7
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// fmuls f19,f2,f13
	ctx.f19.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// fmadds f1,f30,f5,f1
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f5.f64 + ctx.f1.f64));
	// fmsubs f29,f3,f5,f29
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f5.f64 - ctx.f29.f64));
	// fmsubs f26,f8,f7,f26
	ctx.f26.f64 = double(float(ctx.f8.f64 * ctx.f7.f64 - ctx.f26.f64));
	// fmadds f7,f12,f7,f22
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f7.f64 + ctx.f22.f64));
	// fmsubs f9,f12,f9,f24
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f9.f64 - ctx.f24.f64));
	// fmsubs f24,f11,f6,f20
	ctx.f24.f64 = double(float(ctx.f11.f64 * ctx.f6.f64 - ctx.f20.f64));
	// fmadds f28,f2,f5,f28
	ctx.f28.f64 = double(float(ctx.f2.f64 * ctx.f5.f64 + ctx.f28.f64));
	// fmadds f22,f27,f5,f19
	ctx.f22.f64 = double(float(ctx.f27.f64 * ctx.f5.f64 + ctx.f19.f64));
	// fmadds f1,f27,f10,f1
	ctx.f1.f64 = double(float(ctx.f27.f64 * ctx.f10.f64 + ctx.f1.f64));
	// fmuls f26,f26,f5
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f5.f64));
	// fmadds f7,f8,f6,f7
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f6.f64 + ctx.f7.f64));
	// fmuls f9,f9,f5
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f5.f64));
	// fmuls f6,f24,f5
	ctx.f6.f64 = double(float(ctx.f24.f64 * ctx.f5.f64));
	// fnmsubs f5,f27,f0,f29
	ctx.f5.f64 = double(float(-(ctx.f27.f64 * ctx.f0.f64 - ctx.f29.f64)));
	// fmadds f29,f3,f10,f28
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f10.f64 + ctx.f28.f64));
	// fmadds f3,f3,f0,f22
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f0.f64 + ctx.f22.f64));
	// fnmsubs f2,f2,f0,f1
	ctx.f2.f64 = double(float(-(ctx.f2.f64 * ctx.f0.f64 - ctx.f1.f64)));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fadds f1,f17,f26
	ctx.f1.f64 = double(float(ctx.f17.f64 + ctx.f26.f64));
	// fadds f0,f18,f9
	ctx.f0.f64 = double(float(ctx.f18.f64 + ctx.f9.f64));
	// fmuls f9,f7,f8
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f8.f64));
	// fadds f6,f4,f6
	ctx.f6.f64 = double(float(ctx.f4.f64 + ctx.f6.f64));
	// fmuls f8,f11,f7
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// fmuls f4,f12,f7
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f7.f64));
	// fnmsubs f12,f30,f10,f3
	ctx.f12.f64 = double(float(-(ctx.f30.f64 * ctx.f10.f64 - ctx.f3.f64)));
	// stfs f12,84(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f2,f30,f13,f5
	ctx.f2.f64 = double(float(-(ctx.f30.f64 * ctx.f13.f64 - ctx.f5.f64)));
	// stfs f2,92(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fnmsubs f13,f27,f13,f29
	ctx.f13.f64 = double(float(-(ctx.f27.f64 * ctx.f13.f64 - ctx.f29.f64)));
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fadds f11,f0,f9
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f9.f64));
	// fadds f10,f1,f8
	ctx.f10.f64 = double(float(ctx.f1.f64 + ctx.f8.f64));
	// fadds f9,f6,f4
	ctx.f9.f64 = double(float(ctx.f6.f64 + ctx.f4.f64));
	// fmuls f8,f11,f31
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// fmuls f7,f10,f31
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f31.f64));
	// fmuls f6,f9,f31
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f31.f64));
	// fadds f5,f8,f25
	ctx.f5.f64 = double(float(ctx.f8.f64 + ctx.f25.f64));
	// stfs f5,96(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f4,f23,f7
	ctx.f4.f64 = double(float(ctx.f23.f64 + ctx.f7.f64));
	// stfs f4,104(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fadds f3,f21,f6
	ctx.f3.f64 = double(float(ctx.f21.f64 + ctx.f6.f64));
	// stfs f3,100(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// lwz r9,96(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r6,8(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r5,12(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// lwz r4,100(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r3,104(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stw r6,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r6.u32);
	// stw r5,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r5.u32);
	// stw r9,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r9.u32);
	// stw r4,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, ctx.r4.u32);
	// stw r3,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, ctx.r3.u32);
loc_830C074C:
	// lwz r11,152(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 152);
	// addi r5,r1,256
	ctx.r5.s64 = ctx.r1.s64 + 256;
	// lwz r3,336(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 336);
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// stw r11,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, ctx.r11.u32);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,76(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 76);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830C0770;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// stw r3,252(r1)
	PPC_STORE_U32(ctx.r1.u32 + 252, ctx.r3.u32);
	// addi r10,r1,224
	ctx.r10.s64 = ctx.r1.s64 + 224;
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// li r9,7
	ctx.r9.s64 = 7;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_830C0784:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c0784
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C0784;
	// lwz r11,268(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 268);
	// addi r4,r1,208
	ctx.r4.s64 = ctx.r1.s64 + 208;
	// lwz r31,0(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// lwz r3,1412(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1412);
	// bl 0x8315fb80
	ctx.lr = 0x830C07AC;
	sub_8315FB80(ctx, base);
	// stwx r3,r26,r31
	PPC_STORE_U32(ctx.r26.u32 + ctx.r31.u32, ctx.r3.u32);
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// add r11,r26,r11
	ctx.r11.u64 = ctx.r26.u64 + ctx.r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830c0818
	if (ctx.cr6.eq) goto loc_830C0818;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c0818
	if (ctx.cr6.eq) goto loc_830C0818;
	// lwz r29,20(r11)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// lwz r31,16(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// cmplw cr6,r31,r29
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r29.u32, ctx.xer);
	// beq cr6,0x830c0818
	if (ctx.cr6.eq) goto loc_830C0818;
loc_830C07E0:
	// lwz r30,0(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C07F8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x830c080c
	if (ctx.cr6.eq) goto loc_830C080C;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x830bdd70
	ctx.lr = 0x830C080C;
	sub_830BDD70(ctx, base);
loc_830C080C:
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// cmplw cr6,r31,r29
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r29.u32, ctx.xer);
	// bne cr6,0x830c07e0
	if (!ctx.cr6.eq) goto loc_830C07E0;
loc_830C0818:
	// lwz r11,344(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 344);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// beq cr6,0x830c082c
	if (ctx.cr6.eq) goto loc_830C082C;
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x830c0838
	if (!ctx.cr6.eq) goto loc_830C0838;
loc_830C082C:
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x830bda80
	ctx.lr = 0x830C0838;
	sub_830BDA80(ctx, base);
loc_830C0838:
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// lwzx r10,r26,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r11.u32);
	// cntlzw r9,r10
	ctx.r9.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r8,r9,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// xori r3,r8,1
	ctx.r3.u64 = ctx.r8.u64 ^ 1;
	// addi r1,r1,464
	ctx.r1.s64 = ctx.r1.s64 + 464;
	// addi r12,r1,-64
	ctx.r12.s64 = ctx.r1.s64 + -64;
	// bl 0x82cb6b04
	ctx.lr = 0x830C0858;
	__restfpr_16(ctx, base);
	// b 0x82cb112c
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C085C"))) PPC_WEAK_FUNC(sub_830C085C);
PPC_FUNC_IMPL(__imp__sub_830C085C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C0860"))) PPC_WEAK_FUNC(sub_830C0860);
PPC_FUNC_IMPL(__imp__sub_830C0860) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10dc
	ctx.lr = 0x830C0868;
	__savegprlr_25(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r25,r3
	ctx.r25.u64 = ctx.r3.u64;
	// li r26,0
	ctx.r26.s64 = 0;
	// addi r30,r25,348
	ctx.r30.s64 = ctx.r25.s64 + 348;
	// mr r27,r26
	ctx.r27.u64 = ctx.r26.u64;
	// lwz r11,352(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 352);
	// lwz r10,348(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 348);
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi. r8,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 4;
	ctx.cr0.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// beq 0x830c0910
	if (ctx.cr0.eq) goto loc_830C0910;
	// mr r28,r26
	ctx.r28.u64 = ctx.r26.u64;
loc_830C0894:
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// add r11,r28,r11
	ctx.r11.u64 = ctx.r28.u64 + ctx.r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830c08f0
	if (ctx.cr6.eq) goto loc_830C08F0;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c08dc
	if (ctx.cr6.eq) goto loc_830C08DC;
	// lwz r29,20(r11)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// lwz r31,16(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// cmplw cr6,r31,r29
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r29.u32, ctx.xer);
	// beq cr6,0x830c08dc
	if (ctx.cr6.eq) goto loc_830C08DC;
loc_830C08C4:
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// bl 0x830bf750
	ctx.lr = 0x830C08D0;
	sub_830BF750(ctx, base);
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// cmplw cr6,r31,r29
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r29.u32, ctx.xer);
	// bne cr6,0x830c08c4
	if (!ctx.cr6.eq) goto loc_830C08C4;
loc_830C08DC:
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwzx r3,r28,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r11.u32);
	// bl 0x8315fca0
	ctx.lr = 0x830C08E8;
	sub_8315FCA0(ctx, base);
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// stwx r26,r28,r10
	PPC_STORE_U32(ctx.r28.u32 + ctx.r10.u32, ctx.r26.u32);
loc_830C08F0:
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// addi r28,r28,16
	ctx.r28.s64 = ctx.r28.s64 + 16;
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi r8,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 4;
	// cmplw cr6,r27,r8
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, ctx.r8.u32, ctx.xer);
	// blt cr6,0x830c0894
	if (ctx.cr6.lt) goto loc_830C0894;
loc_830C0910:
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// lwz r3,268(r25)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r25.u32 + 268);
	// bl 0x83045cb8
	ctx.lr = 0x830C091C;
	sub_83045CB8(ctx, base);
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// bl 0x83088458
	ctx.lr = 0x830C0924;
	sub_83088458(ctx, base);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82cb112c
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C092C"))) PPC_WEAK_FUNC(sub_830C092C);
PPC_FUNC_IMPL(__imp__sub_830C092C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C0930"))) PPC_WEAK_FUNC(sub_830C0930);
PPC_FUNC_IMPL(__imp__sub_830C0930) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10dc
	ctx.lr = 0x830C0938;
	__savegprlr_25(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// mr r25,r4
	ctx.r25.u64 = ctx.r4.u64;
	// addi r30,r26,348
	ctx.r30.s64 = ctx.r26.s64 + 348;
	// lwz r11,352(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 352);
	// lwz r10,348(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 348);
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi r8,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 4;
	// cmplw cr6,r25,r8
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, ctx.r8.u32, ctx.xer);
	// bge cr6,0x830c0b4c
	if (!ctx.cr6.lt) goto loc_830C0B4C;
	// lwz r11,344(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 344);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// bne cr6,0x830c0acc
	if (!ctx.cr6.eq) goto loc_830C0ACC;
	// rlwinm r10,r25,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r25.u32 | (ctx.r25.u64 << 32), 4) & 0xFFFFFFF0;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// rlwinm r8,r9,0,1,1
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x40000000;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// beq cr6,0x830c0b2c
	if (ctx.cr6.eq) goto loc_830C0B2C;
	// rotlwi r9,r9,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// rlwinm r8,r9,0,2,0
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFBFFFFFFF;
	// stw r8,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r8.u32);
	// lwz r7,344(r26)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r26.u32 + 344);
	// cmpwi cr6,r7,2
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 2, ctx.xer);
	// bne cr6,0x830c0b2c
	if (!ctx.cr6.eq) goto loc_830C0B2C;
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x830c0b2c
	if (!ctx.cr6.eq) goto loc_830C0B2C;
	// lbz r10,368(r26)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r26.u32 + 368);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x830c0b2c
	if (!ctx.cr6.eq) goto loc_830C0B2C;
	// li r27,1
	ctx.r27.s64 = 1;
	// stb r27,12(r11)
	PPC_STORE_U8(ctx.r11.u32 + 12, ctx.r27.u8);
	// lwz r11,268(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 268);
	// addi r31,r11,1532
	ctx.r31.s64 = ctx.r11.s64 + 1532;
	// lwz r9,1540(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1540);
	// lwz r11,1536(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1536);
	// cmplw cr6,r9,r11
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x830c0ab0
	if (ctx.cr6.gt) goto loc_830C0AB0;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// srawi r11,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 2;
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// rlwinm r11,r8,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// bne cr6,0x830c0a04
	if (!ctx.cr6.eq) goto loc_830C0A04;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x830c0a0c
	goto loc_830C0A0C;
loc_830C0A04:
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// srawi r10,r10,2
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
loc_830C0A0C:
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x830c0ab0
	if (!ctx.cr6.lt) goto loc_830C0AB0;
	// lis r29,-31901
	ctx.r29.s64 = -2090663936;
	// rlwinm r28,r11,2,0,29
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// li r5,282
	ctx.r5.s64 = 282;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lwz r3,-32308(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C0A38;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c0a68
	if (ctx.cr6.eq) goto loc_830C0A68;
loc_830C0A50:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x830c0a50
	if (!ctx.cr6.eq) goto loc_830C0A50;
loc_830C0A68:
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x830c0a88
	if (ctx.cr6.eq) goto loc_830C0A88;
	// lwz r3,-32308(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C0A88;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830C0A88:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// add r10,r28,r30
	ctx.r10.u64 = ctx.r28.u64 + ctx.r30.u64;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// subf r8,r9,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r9.s64;
	// stw r30,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r30.u32);
	// srawi r7,r8,2
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r11,r30
	ctx.r6.u64 = ctx.r11.u64 + ctx.r30.u64;
	// stw r6,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r6.u32);
loc_830C0AB0:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// stw r26,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r26.u32);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// addi r10,r11,4
	ctx.r10.s64 = ctx.r11.s64 + 4;
	// stw r10,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r10.u32);
	// stb r27,368(r26)
	PPC_STORE_U8(ctx.r26.u32 + 368, ctx.r27.u8);
	// b 0x830c0b2c
	goto loc_830C0B2C;
loc_830C0ACC:
	// rlwinm r28,r25,4,0,27
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r25.u32 | (ctx.r25.u64 << 32), 4) & 0xFFFFFFF0;
	// add r11,r28,r11
	ctx.r11.u64 = ctx.r28.u64 + ctx.r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830c0b2c
	if (ctx.cr6.eq) goto loc_830C0B2C;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c0b14
	if (ctx.cr6.eq) goto loc_830C0B14;
	// lwz r29,20(r11)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// lwz r31,16(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// cmplw cr6,r31,r29
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r29.u32, ctx.xer);
	// beq cr6,0x830c0b14
	if (ctx.cr6.eq) goto loc_830C0B14;
loc_830C0AFC:
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// bl 0x830bf750
	ctx.lr = 0x830C0B08;
	sub_830BF750(ctx, base);
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// cmplw cr6,r31,r29
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r29.u32, ctx.xer);
	// bne cr6,0x830c0afc
	if (!ctx.cr6.eq) goto loc_830C0AFC;
loc_830C0B14:
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwzx r3,r28,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r11.u32);
	// bl 0x8315fca0
	ctx.lr = 0x830C0B20;
	sub_8315FCA0(ctx, base);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// li r10,0
	ctx.r10.s64 = 0;
	// stwx r10,r28,r9
	PPC_STORE_U32(ctx.r28.u32 + ctx.r9.u32, ctx.r10.u32);
loc_830C0B2C:
	// lwz r11,344(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 344);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// beq cr6,0x830c0b40
	if (ctx.cr6.eq) goto loc_830C0B40;
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x830c0b4c
	if (!ctx.cr6.eq) goto loc_830C0B4C;
loc_830C0B40:
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x830bda80
	ctx.lr = 0x830C0B4C;
	sub_830BDA80(ctx, base);
loc_830C0B4C:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82cb112c
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C0B54"))) PPC_WEAK_FUNC(sub_830C0B54);
PPC_FUNC_IMPL(__imp__sub_830C0B54) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C0B58"))) PPC_WEAK_FUNC(sub_830C0B58);
PPC_FUNC_IMPL(__imp__sub_830C0B58) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10dc
	ctx.lr = 0x830C0B60;
	__savegprlr_25(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r25,r3
	ctx.r25.u64 = ctx.r3.u64;
	// li r26,0
	ctx.r26.s64 = 0;
	// addi r30,r25,348
	ctx.r30.s64 = ctx.r25.s64 + 348;
	// mr r27,r26
	ctx.r27.u64 = ctx.r26.u64;
	// lwz r11,352(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 352);
	// lwz r10,348(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 348);
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi. r8,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 4;
	ctx.cr0.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// beq 0x830c0c14
	if (ctx.cr0.eq) goto loc_830C0C14;
	// mr r28,r26
	ctx.r28.u64 = ctx.r26.u64;
loc_830C0B8C:
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// add r11,r28,r11
	ctx.r11.u64 = ctx.r28.u64 + ctx.r11.u64;
	// lbz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 12);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830c0bf4
	if (ctx.cr6.eq) goto loc_830C0BF4;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830c0bf4
	if (ctx.cr6.eq) goto loc_830C0BF4;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c0be0
	if (ctx.cr6.eq) goto loc_830C0BE0;
	// lwz r29,20(r11)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// lwz r31,16(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// cmplw cr6,r31,r29
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r29.u32, ctx.xer);
	// beq cr6,0x830c0be0
	if (ctx.cr6.eq) goto loc_830C0BE0;
loc_830C0BC8:
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// bl 0x830bf750
	ctx.lr = 0x830C0BD4;
	sub_830BF750(ctx, base);
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// cmplw cr6,r31,r29
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r29.u32, ctx.xer);
	// bne cr6,0x830c0bc8
	if (!ctx.cr6.eq) goto loc_830C0BC8;
loc_830C0BE0:
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwzx r3,r28,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r11.u32);
	// bl 0x8315fca0
	ctx.lr = 0x830C0BEC;
	sub_8315FCA0(ctx, base);
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// stwx r26,r28,r10
	PPC_STORE_U32(ctx.r28.u32 + ctx.r10.u32, ctx.r26.u32);
loc_830C0BF4:
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// addi r28,r28,16
	ctx.r28.s64 = ctx.r28.s64 + 16;
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi r8,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 4;
	// cmplw cr6,r27,r8
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, ctx.r8.u32, ctx.xer);
	// blt cr6,0x830c0b8c
	if (ctx.cr6.lt) goto loc_830C0B8C;
loc_830C0C14:
	// stb r26,368(r25)
	PPC_STORE_U8(ctx.r25.u32 + 368, ctx.r26.u8);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82cb112c
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C0C20"))) PPC_WEAK_FUNC(sub_830C0C20);
PPC_FUNC_IMPL(__imp__sub_830C0C20) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10dc
	ctx.lr = 0x830C0C28;
	__savegprlr_25(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r25,r3
	ctx.r25.u64 = ctx.r3.u64;
	// lbz r11,369(r25)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r25.u32 + 369);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c0d10
	if (ctx.cr6.eq) goto loc_830C0D10;
	// lwz r11,352(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 352);
	// li r27,0
	ctx.r27.s64 = 0;
	// lwz r10,348(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 348);
	// addi r31,r25,348
	ctx.r31.s64 = ctx.r25.s64 + 348;
	// mr r26,r27
	ctx.r26.u64 = ctx.r27.u64;
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi. r8,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 4;
	ctx.cr0.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// beq 0x830c0d0c
	if (ctx.cr0.eq) goto loc_830C0D0C;
	// mr r29,r27
	ctx.r29.u64 = ctx.r27.u64;
loc_830C0C60:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// add r11,r29,r11
	ctx.r11.u64 = ctx.r29.u64 + ctx.r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830c0cbc
	if (ctx.cr6.eq) goto loc_830C0CBC;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c0ca8
	if (ctx.cr6.eq) goto loc_830C0CA8;
	// lwz r28,20(r11)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// lwz r30,16(r11)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// cmplw cr6,r30,r28
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r28.u32, ctx.xer);
	// beq cr6,0x830c0ca8
	if (ctx.cr6.eq) goto loc_830C0CA8;
loc_830C0C90:
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// bl 0x830bf750
	ctx.lr = 0x830C0C9C;
	sub_830BF750(ctx, base);
	// addi r30,r30,4
	ctx.r30.s64 = ctx.r30.s64 + 4;
	// cmplw cr6,r30,r28
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r28.u32, ctx.xer);
	// bne cr6,0x830c0c90
	if (!ctx.cr6.eq) goto loc_830C0C90;
loc_830C0CA8:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwzx r3,r29,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + ctx.r11.u32);
	// bl 0x8315fca0
	ctx.lr = 0x830C0CB4;
	sub_8315FCA0(ctx, base);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// stwx r27,r29,r10
	PPC_STORE_U32(ctx.r29.u32 + ctx.r10.u32, ctx.r27.u32);
loc_830C0CBC:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// add r11,r29,r11
	ctx.r11.u64 = ctx.r29.u64 + ctx.r11.u64;
	// lwz r3,4(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x830c0cec
	if (ctx.cr6.eq) goto loc_830C0CEC;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C0CE0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// add r9,r29,r11
	ctx.r9.u64 = ctx.r29.u64 + ctx.r11.u64;
	// stw r27,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r27.u32);
loc_830C0CEC:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// addi r26,r26,1
	ctx.r26.s64 = ctx.r26.s64 + 1;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r29,r29,16
	ctx.r29.s64 = ctx.r29.s64 + 16;
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi r8,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 4;
	// cmplw cr6,r26,r8
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, ctx.r8.u32, ctx.xer);
	// blt cr6,0x830c0c60
	if (ctx.cr6.lt) goto loc_830C0C60;
loc_830C0D0C:
	// stb r27,369(r25)
	PPC_STORE_U8(ctx.r25.u32 + 369, ctx.r27.u8);
loc_830C0D10:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82cb112c
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C0D18"))) PPC_WEAK_FUNC(sub_830C0D18);
PPC_FUNC_IMPL(__imp__sub_830C0D18) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// bl 0x830c0c20
	ctx.lr = 0x830C0D30;
	sub_830C0C20(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x83088518
	ctx.lr = 0x830C0D38;
	sub_83088518(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C0D4C"))) PPC_WEAK_FUNC(sub_830C0D4C);
PPC_FUNC_IMPL(__imp__sub_830C0D4C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C0D50"))) PPC_WEAK_FUNC(sub_830C0D50);
PPC_FUNC_IMPL(__imp__sub_830C0D50) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10d8
	ctx.lr = 0x830C0D58;
	__savegprlr_24(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// bl 0x830c0c20
	ctx.lr = 0x830C0D64;
	sub_830C0C20(ctx, base);
	// lwz r11,264(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 264);
	// li r26,0
	ctx.r26.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x830c0d88
	if (ctx.cr6.eq) goto loc_830C0D88;
	// lwz r11,4(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x830c0e00
	if (ctx.cr6.eq) goto loc_830C0E00;
	// lwz r24,292(r11)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r11.u32 + 292);
	// b 0x830c0e10
	goto loc_830C0E10;
loc_830C0D88:
	// lwz r31,4(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4);
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// beq cr6,0x830c0df0
	if (ctx.cr6.eq) goto loc_830C0DF0;
	// lwz r11,268(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 268);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r24,756(r11)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r11.u32 + 756);
	// lwz r9,460(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 460);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830C0DB0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r10,336(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// mr r11,r26
	ctx.r11.u64 = ctx.r26.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x830c0e10
	if (ctx.cr6.eq) goto loc_830C0E10;
loc_830C0DC0:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r8,312(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 312);
	// clrlwi r7,r8,29
	ctx.r7.u64 = ctx.r8.u32 & 0x7;
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// bne cr6,0x830c0de8
	if (!ctx.cr6.eq) goto loc_830C0DE8;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplw cr6,r11,r3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r3.u32, ctx.xer);
	// blt cr6,0x830c0dc0
	if (ctx.cr6.lt) goto loc_830C0DC0;
	// b 0x830c0e10
	goto loc_830C0E10;
loc_830C0DE8:
	// lwz r24,292(r31)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r31.u32 + 292);
	// b 0x830c0e10
	goto loc_830C0E10;
loc_830C0DF0:
	// lwz r11,312(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 312);
	// clrlwi r10,r11,29
	ctx.r10.u64 = ctx.r11.u32 & 0x7;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x830c0e08
	if (ctx.cr6.eq) goto loc_830C0E08;
loc_830C0E00:
	// lwz r24,292(r28)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r28.u32 + 292);
	// b 0x830c0e10
	goto loc_830C0E10;
loc_830C0E08:
	// lwz r11,268(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 268);
	// lwz r24,756(r11)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r11.u32 + 756);
loc_830C0E10:
	// lwz r11,352(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 352);
	// addi r31,r28,348
	ctx.r31.s64 = ctx.r28.s64 + 348;
	// lwz r10,348(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 348);
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi. r8,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 4;
	ctx.cr0.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// beq 0x830c0ed0
	if (ctx.cr0.eq) goto loc_830C0ED0;
	// mr r30,r26
	ctx.r30.u64 = ctx.r26.u64;
	// li r27,1
	ctx.r27.s64 = 1;
	// lis r25,-31901
	ctx.r25.s64 = -2090663936;
loc_830C0E38:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// li r5,131
	ctx.r5.s64 = 131;
	// li r4,112
	ctx.r4.s64 = 112;
	// stwx r26,r30,r11
	PPC_STORE_U32(ctx.r30.u32 + ctx.r11.u32, ctx.r26.u32);
	// lwz r3,-32308(r25)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r25.u32 + -32308);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830C0E5C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x830c0e7c
	if (ctx.cr6.eq) goto loc_830C0E7C;
	// mr r7,r24
	ctx.r7.u64 = ctx.r24.u64;
	// lwz r4,268(r28)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r28.u32 + 268);
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// bl 0x830bdb48
	ctx.lr = 0x830C0E78;
	sub_830BDB48(ctx, base);
	// b 0x830c0e80
	goto loc_830C0E80;
loc_830C0E7C:
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
loc_830C0E80:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// add r11,r30,r11
	ctx.r11.u64 = ctx.r30.u64 + ctx.r11.u64;
	// stw r3,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r3.u32);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// add r10,r30,r11
	ctx.r10.u64 = ctx.r30.u64 + ctx.r11.u64;
	// stw r26,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r26.u32);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// add r9,r30,r11
	ctx.r9.u64 = ctx.r30.u64 + ctx.r11.u64;
	// stb r27,12(r9)
	PPC_STORE_U8(ctx.r9.u32 + 12, ctx.r27.u8);
	// addi r30,r30,16
	ctx.r30.s64 = ctx.r30.s64 + 16;
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// subf r6,r7,r8
	ctx.r6.s64 = ctx.r8.s64 - ctx.r7.s64;
	// srawi r5,r6,4
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r5.s64 = ctx.r6.s32 >> 4;
	// cmplw cr6,r29,r5
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r5.u32, ctx.xer);
	// blt cr6,0x830c0e38
	if (ctx.cr6.lt) goto loc_830C0E38;
	// stb r27,369(r28)
	PPC_STORE_U8(ctx.r28.u32 + 369, ctx.r27.u8);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82cb1128
	__restgprlr_24(ctx, base);
	return;
loc_830C0ED0:
	// li r11,1
	ctx.r11.s64 = 1;
	// stb r11,369(r28)
	PPC_STORE_U8(ctx.r28.u32 + 369, ctx.r11.u8);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82cb1128
	__restgprlr_24(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C0EE0"))) PPC_WEAK_FUNC(sub_830C0EE0);
PPC_FUNC_IMPL(__imp__sub_830C0EE0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10d8
	ctx.lr = 0x830C0EE8;
	__savegprlr_24(ctx, base);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r3
	ctx.r24.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// stw r4,336(r24)
	PPC_STORE_U32(ctx.r24.u32 + 336, ctx.r4.u32);
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r10,68(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 68);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C0F08;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x830c0f60
	if (!ctx.cr6.eq) goto loc_830C0F60;
	// lwz r11,336(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 336);
	// lwz r10,176(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 176);
	// cmpwi cr6,r10,255
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 255, ctx.xer);
	// bne cr6,0x830c0f60
	if (!ctx.cr6.eq) goto loc_830C0F60;
	// lwz r3,268(r24)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r24.u32 + 268);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C0F38;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// bne cr6,0x830c0f60
	if (!ctx.cr6.eq) goto loc_830C0F60;
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lwz r8,144(r24)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r24.u32 + 144);
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r4,r11,4736
	ctx.r4.s64 = ctx.r11.s64 + 4736;
	// li r5,1190
	ctx.r5.s64 = 1190;
	// addi r7,r4,-152
	ctx.r7.s64 = ctx.r4.s64 + -152;
	// li r3,206
	ctx.r3.s64 = 206;
	// bl 0x82d17988
	ctx.lr = 0x830C0F60;
	sub_82D17988(ctx, base);
loc_830C0F60:
	// lwz r3,268(r24)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r24.u32 + 268);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C0F74;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// li r25,0
	ctx.r25.s64 = 0;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x830c0f8c
	if (ctx.cr6.eq) goto loc_830C0F8C;
	// lwz r11,264(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 264);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x830c0f90
	if (ctx.cr6.eq) goto loc_830C0F90;
loc_830C0F8C:
	// mr r30,r25
	ctx.r30.u64 = ctx.r25.u64;
loc_830C0F90:
	// lwz r10,348(r24)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r24.u32 + 348);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lwz r11,352(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 352);
	// addi r31,r24,348
	ctx.r31.s64 = ctx.r24.s64 + 348;
	// lis r26,-31901
	ctx.r26.s64 = -2090663936;
	// subf r7,r10,r11
	ctx.r7.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi r9,r7,4
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 4;
	// std r25,0(r8)
	PPC_STORE_U64(ctx.r8.u32 + 0, ctx.r25.u64);
	// std r25,8(r8)
	PPC_STORE_U64(ctx.r8.u32 + 8, ctx.r25.u64);
	// cmplw cr6,r30,r9
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r9.u32, ctx.xer);
	// ble cr6,0x830c1174
	if (!ctx.cr6.gt) goto loc_830C1174;
	// subf. r27,r9,r30
	ctx.r27.s64 = ctx.r30.s64 - ctx.r9.s64;
	ctx.cr0.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// mr r4,r11
	ctx.r4.u64 = ctx.r11.u64;
	// beq 0x830c1194
	if (ctx.cr0.eq) goto loc_830C1194;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x830c0fd8
	if (!ctx.cr6.eq) goto loc_830C0FD8;
	// mr r8,r25
	ctx.r8.u64 = ctx.r25.u64;
	// b 0x830c0fe4
	goto loc_830C0FE4;
loc_830C0FD8:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi r8,r8,4
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 4;
loc_830C0FE4:
	// add r11,r9,r27
	ctx.r11.u64 = ctx.r9.u64 + ctx.r27.u64;
	// cmplw cr6,r8,r11
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x830c10dc
	if (!ctx.cr6.lt) goto loc_830C10DC;
	// subf r9,r10,r4
	ctx.r9.s64 = ctx.r4.s64 - ctx.r10.s64;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r28,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r28.s64 = ctx.r9.s32 >> 4;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x830c100c
	if (!ctx.cr6.eq) goto loc_830C100C;
	// mr r10,r25
	ctx.r10.u64 = ctx.r25.u64;
	// b 0x830c1018
	goto loc_830C1018;
loc_830C100C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf r8,r10,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r10.s64;
	// srawi r10,r8,4
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r8.s32 >> 4;
loc_830C1018:
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x830c10d0
	if (!ctx.cr6.lt) goto loc_830C10D0;
	// lwz r3,-32308(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + -32308);
	// rlwinm r29,r11,4,0,27
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// li r5,282
	ctx.r5.s64 = 282;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C1040;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c1088
	if (ctx.cr6.eq) goto loc_830C1088;
loc_830C1058:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r7,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r7.u32);
	// lwz r6,8(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r6,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r6.u32);
	// lwz r5,12(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// stw r5,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r5.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// bne cr6,0x830c1058
	if (!ctx.cr6.eq) goto loc_830C1058;
loc_830C1088:
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x830c10a8
	if (ctx.cr6.eq) goto loc_830C10A8;
	// lwz r3,-32308(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C10A8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830C10A8:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// add r10,r29,r30
	ctx.r10.u64 = ctx.r29.u64 + ctx.r30.u64;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// subf r8,r9,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r9.s64;
	// stw r30,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r30.u32);
	// srawi r7,r8,4
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 4;
	// rlwinm r11,r7,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// add r6,r11,r30
	ctx.r6.u64 = ctx.r11.u64 + ctx.r30.u64;
	// stw r6,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r6.u32);
loc_830C10D0:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// rlwinm r10,r28,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r28.u32 | (ctx.r28.u64 << 32), 4) & 0xFFFFFFF0;
	// add r4,r10,r11
	ctx.r4.u64 = ctx.r10.u64 + ctx.r11.u64;
loc_830C10DC:
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// rlwinm r30,r27,4,0,27
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r27.u32 | (ctx.r27.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// add r10,r30,r4
	ctx.r10.u64 = ctx.r30.u64 + ctx.r4.u64;
	// cmplw cr6,r4,r9
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c1124
	if (ctx.cr6.eq) goto loc_830C1124;
loc_830C10F4:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r7,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r7.u32);
	// lwz r6,8(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r6,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r6.u32);
	// lwz r5,12(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// stw r5,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r5.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// bne cr6,0x830c10f4
	if (!ctx.cr6.eq) goto loc_830C10F4;
loc_830C1124:
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// beq cr6,0x830c1164
	if (ctx.cr6.eq) goto loc_830C1164;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// rlwinm r11,r27,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r27.u32 | (ctx.r27.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r3,r4,16
	ctx.r3.s64 = ctx.r4.s64 + 16;
	// addi r9,r11,-9
	ctx.r9.s64 = ctx.r11.s64 + -9;
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r5,r9,0,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFF8;
	// lwz r7,4(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r6,8(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lwz r11,12(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// stw r8,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r8.u32);
	// stw r7,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r7.u32);
	// stw r6,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r6.u32);
	// stw r11,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r11.u32);
	// bl 0x82cb15e8
	ctx.lr = 0x830C1164;
	sub_82CB15E8(ctx, base);
loc_830C1164:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// add r11,r30,r11
	ctx.r11.u64 = ctx.r30.u64 + ctx.r11.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r11.u32);
	// b 0x830c1194
	goto loc_830C1194;
loc_830C1174:
	// bge cr6,0x830c1194
	if (!ctx.cr6.lt) goto loc_830C1194;
	// rlwinm r9,r30,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 4) & 0xFFFFFFF0;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// subf r10,r10,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi r9,r10,4
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 4;
	// rlwinm r8,r9,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r7,r8,r11
	ctx.r7.s64 = ctx.r11.s64 - ctx.r8.s64;
	// stw r7,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r7.u32);
loc_830C1194:
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmplw cr6,r4,r11
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x830c11cc
	if (!ctx.cr6.eq) goto loc_830C11CC;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x830c11c0
	if (ctx.cr6.eq) goto loc_830C11C0;
	// lwz r3,-32308(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C11C0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830C11C0:
	// stw r25,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r25.u32);
	// stw r25,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r25.u32);
	// stw r25,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r25.u32);
loc_830C11CC:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x830c1214
	if (!ctx.cr6.gt) goto loc_830C1214;
	// lwz r3,-32308(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + -32308);
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// subf r11,r4,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r4.s64;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// srawi r30,r11,4
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r11.s32 >> 4;
	// rlwinm r5,r30,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r9,16(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830C1200;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// rlwinm r11,r30,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r3,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r3.u32);
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r11.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
loc_830C1214:
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x830c0d50
	ctx.lr = 0x830C121C;
	sub_830C0D50(ctx, base);
	// lwz r3,336(r24)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r24.u32 + 336);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 84);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C1230;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,336(r24)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r24.u32 + 336);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lwz r7,80(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 80);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x830C124C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmplw cr6,r3,r31
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r31.u32, ctx.xer);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// blt cr6,0x830c125c
	if (ctx.cr6.lt) goto loc_830C125C;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
loc_830C125C:
	// lwz r3,268(r24)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r24.u32 + 268);
	// bl 0x83042848
	ctx.lr = 0x830C1264;
	sub_83042848(ctx, base);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82cb1128
	__restgprlr_24(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C126C"))) PPC_WEAK_FUNC(sub_830C126C);
PPC_FUNC_IMPL(__imp__sub_830C126C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C1270"))) PPC_WEAK_FUNC(sub_830C1270);
PPC_FUNC_IMPL(__imp__sub_830C1270) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e8
	ctx.lr = 0x830C1278;
	__savegprlr_28(ctx, base);
	// addi r12,r1,-40
	ctx.r12.s64 = ctx.r1.s64 + -40;
	// bl 0x82cb6abc
	ctx.lr = 0x830C1280;
	__savefpr_17(ctx, base);
	// stwu r1,-400(r1)
	ea = -400 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// mr r28,r5
	ctx.r28.u64 = ctx.r5.u64;
	// bl 0x8308a278
	ctx.lr = 0x830C1294;
	sub_8308A278(ctx, base);
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// li r30,0
	ctx.r30.s64 = 0;
	// addi r10,r11,4888
	ctx.r10.s64 = ctx.r11.s64 + 4888;
	// stw r30,336(r31)
	PPC_STORE_U32(ctx.r31.u32 + 336, ctx.r30.u32);
	// li r9,6
	ctx.r9.s64 = 6;
	// stw r10,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r8,100(r29)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r29.u32 + 100);
	// stw r8,344(r31)
	PPC_STORE_U32(ctx.r31.u32 + 344, ctx.r8.u32);
	// stw r30,348(r31)
	PPC_STORE_U32(ctx.r31.u32 + 348, ctx.r30.u32);
	// stw r30,352(r31)
	PPC_STORE_U32(ctx.r31.u32 + 352, ctx.r30.u32);
	// stw r30,356(r31)
	PPC_STORE_U32(ctx.r31.u32 + 356, ctx.r30.u32);
	// stb r30,368(r31)
	PPC_STORE_U8(ctx.r31.u32 + 368, ctx.r30.u8);
	// stb r30,369(r31)
	PPC_STORE_U8(ctx.r31.u32 + 369, ctx.r30.u8);
	// lwz r7,96(r29)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r29.u32 + 96);
	// stw r7,340(r31)
	PPC_STORE_U32(ctx.r31.u32 + 340, ctx.r7.u32);
	// stw r9,288(r31)
	PPC_STORE_U32(ctx.r31.u32 + 288, ctx.r9.u32);
	// lwz r4,92(r29)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r29.u32 + 92);
	// bl 0x830c0ee0
	ctx.lr = 0x830C12E0;
	sub_830C0EE0(ctx, base);
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// bl 0x8315fb70
	ctx.lr = 0x830C12E8;
	sub_8315FB70(ctx, base);
	// lwz r11,336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// li r6,7
	ctx.r6.s64 = 7;
	// lwz r3,1412(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 1412);
	// addi r5,r11,4
	ctx.r5.s64 = ctx.r11.s64 + 4;
	// stw r6,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r6.u32);
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// stw r5,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, ctx.r5.u32);
	// lwz r11,96(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 96);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// stb r10,188(r1)
	PPC_STORE_U8(ctx.r1.u32 + 188, ctx.r10.u8);
	// bl 0x8315fb80
	ctx.lr = 0x830C1314;
	sub_8315FB80(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,152(r31)
	PPC_STORE_U32(ctx.r31.u32 + 152, ctx.r3.u32);
	// beq cr6,0x830c17b0
	if (ctx.cr6.eq) goto loc_830C17B0;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x8315fb70
	ctx.lr = 0x830C1328;
	sub_8315FB70(ctx, base);
	// li r10,10
	ctx.r10.s64 = 10;
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// stw r10,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r10.u32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c1624
	if (ctx.cr6.eq) goto loc_830C1624;
	// lfs f0,220(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 220);
	ctx.f0.f64 = double(temp.f32);
	// lfs f11,232(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 232);
	ctx.f11.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f13,224(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 224);
	ctx.f13.f64 = double(temp.f32);
	// fneg f8,f11
	ctx.f8.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// lfs f9,236(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 236);
	ctx.f9.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// lfs f6,216(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 216);
	ctx.f6.f64 = double(temp.f32);
	// fneg f7,f9
	ctx.f7.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// lfs f5,240(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 240);
	ctx.f5.f64 = double(temp.f32);
	// fmr f4,f6
	ctx.f4.f64 = ctx.f6.f64;
	// fneg f3,f5
	ctx.f3.u64 = ctx.f5.u64 ^ 0x8000000000000000;
	// lfs f2,228(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 228);
	ctx.f2.f64 = double(temp.f32);
	// fneg f1,f6
	ctx.f1.u64 = ctx.f6.u64 ^ 0x8000000000000000;
	// lfs f11,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f11.f64 = double(temp.f32);
	// fneg f9,f13
	ctx.f9.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// lfs f13,6380(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6380);
	ctx.f13.f64 = double(temp.f32);
	// fneg f5,f0
	ctx.f5.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lfs f6,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f6.f64 = double(temp.f32);
	// fmr f30,f2
	ctx.f30.f64 = ctx.f2.f64;
	// lfs f31,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f31.f64 = double(temp.f32);
	// fmsubs f29,f2,f2,f13
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f2.f64 - ctx.f13.f64));
	// lfs f0,7676(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f28,f12,f8
	ctx.f28.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// addi r10,r11,216
	ctx.r10.s64 = ctx.r11.s64 + 216;
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// fmuls f27,f10,f7
	ctx.f27.f64 = double(float(ctx.f10.f64 * ctx.f7.f64));
	// fmuls f26,f12,f7
	ctx.f26.f64 = double(float(ctx.f12.f64 * ctx.f7.f64));
	// fmuls f25,f3,f4
	ctx.f25.f64 = double(float(ctx.f3.f64 * ctx.f4.f64));
	// fmuls f24,f11,f1
	ctx.f24.f64 = double(float(ctx.f11.f64 * ctx.f1.f64));
	// fmuls f23,f9,f6
	ctx.f23.f64 = double(float(ctx.f9.f64 * ctx.f6.f64));
	// fmuls f22,f5,f6
	ctx.f22.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmuls f21,f5,f31
	ctx.f21.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// fmuls f19,f29,f8
	ctx.f19.f64 = double(float(ctx.f29.f64 * ctx.f8.f64));
	// fmsubs f28,f7,f4,f28
	ctx.f28.f64 = double(float(ctx.f7.f64 * ctx.f4.f64 - ctx.f28.f64));
	// fmuls f7,f7,f29
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f29.f64));
	// fmsubs f27,f12,f3,f27
	ctx.f27.f64 = double(float(ctx.f12.f64 * ctx.f3.f64 - ctx.f27.f64));
	// fmadds f26,f10,f3,f26
	ctx.f26.f64 = double(float(ctx.f10.f64 * ctx.f3.f64 + ctx.f26.f64));
	// fmsubs f25,f10,f8,f25
	ctx.f25.f64 = double(float(ctx.f10.f64 * ctx.f8.f64 - ctx.f25.f64));
	// fmuls f3,f3,f29
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f29.f64));
	// fmsubs f20,f30,f30,f13
	ctx.f20.f64 = double(float(ctx.f30.f64 * ctx.f30.f64 - ctx.f13.f64));
	// fmsubs f24,f9,f31,f24
	ctx.f24.f64 = double(float(ctx.f9.f64 * ctx.f31.f64 - ctx.f24.f64));
	// fmsubs f23,f5,f11,f23
	ctx.f23.f64 = double(float(ctx.f5.f64 * ctx.f11.f64 - ctx.f23.f64));
	// fmadds f22,f31,f1,f22
	ctx.f22.f64 = double(float(ctx.f31.f64 * ctx.f1.f64 + ctx.f22.f64));
	// fmuls f29,f28,f2
	ctx.f29.f64 = double(float(ctx.f28.f64 * ctx.f2.f64));
	// fmsubs f21,f6,f1,f21
	ctx.f21.f64 = double(float(ctx.f6.f64 * ctx.f1.f64 - ctx.f21.f64));
	// fmuls f28,f27,f2
	ctx.f28.f64 = double(float(ctx.f27.f64 * ctx.f2.f64));
	// fmadds f8,f4,f8,f26
	ctx.f8.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f26.f64));
	// fmuls f2,f25,f2
	ctx.f2.f64 = double(float(ctx.f25.f64 * ctx.f2.f64));
	// fmuls f27,f11,f20
	ctx.f27.f64 = double(float(ctx.f11.f64 * ctx.f20.f64));
	// fmuls f31,f31,f20
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f20.f64));
	// fmuls f6,f6,f20
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f20.f64));
	// fmadds f11,f9,f11,f22
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f11.f64 + ctx.f22.f64));
	// fsubs f3,f3,f29
	ctx.f3.f64 = double(float(ctx.f3.f64 - ctx.f29.f64));
	// fmuls f26,f24,f30
	ctx.f26.f64 = double(float(ctx.f24.f64 * ctx.f30.f64));
	// fmuls f25,f30,f23
	ctx.f25.f64 = double(float(ctx.f30.f64 * ctx.f23.f64));
	// fmuls f4,f8,f4
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// fsubs f29,f19,f28
	ctx.f29.f64 = double(float(ctx.f19.f64 - ctx.f28.f64));
	// fmuls f24,f21,f30
	ctx.f24.f64 = double(float(ctx.f21.f64 * ctx.f30.f64));
	// fmuls f10,f10,f8
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f8.f64));
	// fmuls f8,f12,f8
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// fsubs f7,f7,f2
	ctx.f7.f64 = double(float(ctx.f7.f64 - ctx.f2.f64));
	// fmuls f12,f11,f1
	ctx.f12.f64 = double(float(ctx.f11.f64 * ctx.f1.f64));
	// fmuls f28,f5,f11
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// fadds f6,f6,f26
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f26.f64));
	// fadds f2,f31,f25
	ctx.f2.f64 = double(float(ctx.f31.f64 + ctx.f25.f64));
	// fmuls f11,f9,f11
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// fadds f4,f29,f4
	ctx.f4.f64 = double(float(ctx.f29.f64 + ctx.f4.f64));
	// fadds f31,f27,f24
	ctx.f31.f64 = double(float(ctx.f27.f64 + ctx.f24.f64));
	// fadds f10,f3,f10
	ctx.f10.f64 = double(float(ctx.f3.f64 + ctx.f10.f64));
	// fadds f3,f7,f8
	ctx.f3.f64 = double(float(ctx.f7.f64 + ctx.f8.f64));
	// fadds f8,f6,f28
	ctx.f8.f64 = double(float(ctx.f6.f64 + ctx.f28.f64));
	// fadds f7,f2,f12
	ctx.f7.f64 = double(float(ctx.f2.f64 + ctx.f12.f64));
	// fmuls f12,f4,f0
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// lfs f4,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f4.f64 = double(temp.f32);
	// fadds f6,f31,f11
	ctx.f6.f64 = double(float(ctx.f31.f64 + ctx.f11.f64));
	// lfs f11,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f2,f10,f0
	ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f31,f30,f4
	ctx.f31.f64 = double(float(ctx.f30.f64 * ctx.f4.f64));
	// fmuls f10,f3,f0
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmuls f3,f30,f11
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f11.f64));
	// fmuls f28,f9,f4
	ctx.f28.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// lfs f27,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfs f29,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f23,176(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 176);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lfs f25,184(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 184);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f3,f27,f1,f3
	ctx.f3.f64 = double(float(ctx.f27.f64 * ctx.f1.f64 + ctx.f3.f64));
	// lfs f24,180(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f26,f4,f1
	ctx.f26.f64 = double(float(ctx.f4.f64 * ctx.f1.f64));
	// lfs f22,168(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 168);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f31,f29,f1,f31
	ctx.f31.f64 = double(float(ctx.f29.f64 * ctx.f1.f64 + ctx.f31.f64));
	// lfs f21,160(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,172(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 172);
	ctx.f20.f64 = double(temp.f32);
	// addi r10,r31,160
	ctx.r10.s64 = ctx.r31.s64 + 160;
	// lfs f19,164(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 164);
	ctx.f19.f64 = double(temp.f32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// fmadds f28,f30,f27,f28
	ctx.f28.f64 = double(float(ctx.f30.f64 * ctx.f27.f64 + ctx.f28.f64));
	// fadds f10,f8,f10
	ctx.f10.f64 = double(float(ctx.f8.f64 + ctx.f10.f64));
	// fadds f8,f7,f12
	ctx.f8.f64 = double(float(ctx.f7.f64 + ctx.f12.f64));
	// fadds f7,f6,f2
	ctx.f7.f64 = double(float(ctx.f6.f64 + ctx.f2.f64));
	// fmadds f6,f9,f29,f3
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f29.f64 + ctx.f3.f64));
	// fmsubs f30,f30,f29,f26
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f29.f64 - ctx.f26.f64));
	// fmadds f3,f5,f11,f31
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f11.f64 + ctx.f31.f64));
	// fmadds f2,f5,f29,f28
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f29.f64 + ctx.f28.f64));
	// fnmsubs f6,f5,f4,f6
	ctx.f6.f64 = double(float(-(ctx.f5.f64 * ctx.f4.f64 - ctx.f6.f64)));
	// fnmsubs f12,f5,f27,f30
	ctx.f12.f64 = double(float(-(ctx.f5.f64 * ctx.f27.f64 - ctx.f30.f64)));
	// fnmsubs f5,f9,f27,f3
	ctx.f5.f64 = double(float(-(ctx.f9.f64 * ctx.f27.f64 - ctx.f3.f64)));
	// fnmsubs f4,f11,f1,f2
	ctx.f4.f64 = double(float(-(ctx.f11.f64 * ctx.f1.f64 - ctx.f2.f64)));
	// fmuls f2,f6,f24
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f24.f64));
	// fnmsubs f3,f9,f11,f12
	ctx.f3.f64 = double(float(-(ctx.f9.f64 * ctx.f11.f64 - ctx.f12.f64)));
	// fmuls f1,f25,f5
	ctx.f1.f64 = double(float(ctx.f25.f64 * ctx.f5.f64));
	// fmuls f11,f6,f25
	ctx.f11.f64 = double(float(ctx.f6.f64 * ctx.f25.f64));
	// fmuls f31,f6,f21
	ctx.f31.f64 = double(float(ctx.f6.f64 * ctx.f21.f64));
	// fmuls f30,f6,f20
	ctx.f30.f64 = double(float(ctx.f6.f64 * ctx.f20.f64));
	// fmuls f12,f4,f23
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f23.f64));
	// fmuls f29,f21,f5
	ctx.f29.f64 = double(float(ctx.f21.f64 * ctx.f5.f64));
	// fmsubs f2,f4,f25,f2
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f25.f64 - ctx.f2.f64));
	// fmsubs f9,f3,f3,f13
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f3.f64 - ctx.f13.f64));
	// fmuls f13,f4,f22
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f22.f64));
	// fmsubs f1,f6,f23,f1
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f23.f64 - ctx.f1.f64));
	// fmadds f11,f4,f24,f11
	ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f24.f64 + ctx.f11.f64));
	// fmadds f31,f3,f19,f31
	ctx.f31.f64 = double(float(ctx.f3.f64 * ctx.f19.f64 + ctx.f31.f64));
	// fmsubs f12,f24,f5,f12
	ctx.f12.f64 = double(float(ctx.f24.f64 * ctx.f5.f64 - ctx.f12.f64));
	// fmadds f30,f3,f22,f30
	ctx.f30.f64 = double(float(ctx.f3.f64 * ctx.f22.f64 + ctx.f30.f64));
	// fmuls f2,f3,f2
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f2.f64));
	// fmuls f28,f23,f9
	ctx.f28.f64 = double(float(ctx.f23.f64 * ctx.f9.f64));
	// fmuls f27,f24,f9
	ctx.f27.f64 = double(float(ctx.f24.f64 * ctx.f9.f64));
	// fmadds f13,f3,f21,f13
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f21.f64 + ctx.f13.f64));
	// fmuls f9,f25,f9
	ctx.f9.f64 = double(float(ctx.f25.f64 * ctx.f9.f64));
	// fmuls f1,f1,f3
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f3.f64));
	// fmuls f12,f12,f3
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f3.f64));
	// fmadds f11,f23,f5,f11
	ctx.f11.f64 = double(float(ctx.f23.f64 * ctx.f5.f64 + ctx.f11.f64));
	// fmsubs f3,f3,f20,f29
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f20.f64 - ctx.f29.f64));
	// fmadds f31,f4,f20,f31
	ctx.f31.f64 = double(float(ctx.f4.f64 * ctx.f20.f64 + ctx.f31.f64));
	// fmadds f30,f19,f5,f30
	ctx.f30.f64 = double(float(ctx.f19.f64 * ctx.f5.f64 + ctx.f30.f64));
	// fadds f2,f28,f2
	ctx.f2.f64 = double(float(ctx.f28.f64 + ctx.f2.f64));
	// fmadds f13,f20,f5,f13
	ctx.f13.f64 = double(float(ctx.f20.f64 * ctx.f5.f64 + ctx.f13.f64));
	// fadds f1,f27,f1
	ctx.f1.f64 = double(float(ctx.f27.f64 + ctx.f1.f64));
	// fadds f12,f9,f12
	ctx.f12.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
	// fmuls f9,f11,f5
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f5.f64));
	// fmuls f29,f4,f11
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmuls f11,f6,f11
	ctx.f11.f64 = double(float(ctx.f6.f64 * ctx.f11.f64));
	// fnmsubs f3,f4,f19,f3
	ctx.f3.f64 = double(float(-(ctx.f4.f64 * ctx.f19.f64 - ctx.f3.f64)));
	// fnmsubs f5,f22,f5,f31
	ctx.f5.f64 = double(float(-(ctx.f22.f64 * ctx.f5.f64 - ctx.f31.f64)));
	// fnmsubs f4,f4,f21,f30
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f21.f64 - ctx.f30.f64)));
	// fnmsubs f13,f6,f19,f13
	ctx.f13.f64 = double(float(-(ctx.f6.f64 * ctx.f19.f64 - ctx.f13.f64)));
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fadds f2,f2,f9
	ctx.f2.f64 = double(float(ctx.f2.f64 + ctx.f9.f64));
	// fadds f1,f1,f29
	ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f29.f64));
	// fadds f13,f12,f11
	ctx.f13.f64 = double(float(ctx.f12.f64 + ctx.f11.f64));
	// fnmsubs f12,f6,f22,f3
	ctx.f12.f64 = double(float(-(ctx.f6.f64 * ctx.f22.f64 - ctx.f3.f64)));
	// fmuls f11,f2,f0
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmuls f9,f1,f0
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f6,f13,f0
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fadds f3,f11,f8
	ctx.f3.f64 = double(float(ctx.f11.f64 + ctx.f8.f64));
	// stfs f3,96(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f2,f9,f10
	ctx.f2.f64 = double(float(ctx.f9.f64 + ctx.f10.f64));
	// stfs f2,100(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f1,f6,f7
	ctx.f1.f64 = double(float(ctx.f6.f64 + ctx.f7.f64));
	// lwz r9,184(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 184);
	// stfs f1,104(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// stfs f5,84(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lwz r5,104(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stfs f4,88(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r10,r1,140
	ctx.r10.s64 = ctx.r1.s64 + 140;
	// stfs f12,92(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lwz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r7,96(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r9,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r9.u32);
	// lwz r9,8(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// lwz r6,100(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r4,12(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// stw r4,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r4.u32);
	// lwz r11,4(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r11.u32);
	// stw r3,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r3.u32);
	// stw r9,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r9.u32);
	// stw r7,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r7.u32);
	// stw r6,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r6.u32);
	// stw r5,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r5.u32);
	// b 0x830c1794
	goto loc_830C1794;
loc_830C1624:
	// lfs f12,120(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f12.f64 = double(temp.f32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lfs f13,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// fmr f9,f12
	ctx.f9.f64 = ctx.f12.f64;
	// lfs f10,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f10.f64 = double(temp.f32);
	// fmr f11,f13
	ctx.f11.f64 = ctx.f13.f64;
	// fmr f8,f10
	ctx.f8.f64 = ctx.f10.f64;
	// lfs f4,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f4.f64 = double(temp.f32);
	// lfs f7,184(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 184);
	ctx.f7.f64 = double(temp.f32);
	// addi r11,r31,160
	ctx.r11.s64 = ctx.r31.s64 + 160;
	// lfs f0,6380(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6380);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r31,112
	ctx.r11.s64 = ctx.r31.s64 + 112;
	// lfs f6,176(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 176);
	ctx.f6.f64 = double(temp.f32);
	// fmsubs f3,f4,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f4.f64 - ctx.f0.f64));
	// lfs f5,180(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	ctx.f5.f64 = double(temp.f32);
	// addi r7,r1,140
	ctx.r7.s64 = ctx.r1.s64 + 140;
	// lfs f2,168(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 168);
	ctx.f2.f64 = double(temp.f32);
	// stw r30,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r30.u32);
	// lfs f30,160(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f31,f2,f10
	ctx.f31.f64 = double(float(ctx.f2.f64 * ctx.f10.f64));
	// lfs f1,172(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 172);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f21,f30,f12
	ctx.f21.f64 = double(float(ctx.f30.f64 * ctx.f12.f64));
	// fmuls f26,f9,f7
	ctx.f26.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// lfs f27,164(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 164);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f28,f7,f11
	ctx.f28.f64 = double(float(ctx.f7.f64 * ctx.f11.f64));
	// lfs f0,7676(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f25,f8,f6
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f6.f64));
	// lfs f24,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f23,f9,f5
	ctx.f23.f64 = double(float(ctx.f9.f64 * ctx.f5.f64));
	// lfs f22,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f29,f1,f12
	ctx.f29.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// lfs f20,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f18,f5,f3
	ctx.f18.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fmuls f17,f7,f3
	ctx.f17.f64 = double(float(ctx.f7.f64 * ctx.f3.f64));
	// fmuls f19,f30,f13
	ctx.f19.f64 = double(float(ctx.f30.f64 * ctx.f13.f64));
	// fmadds f31,f30,f4,f31
	ctx.f31.f64 = double(float(ctx.f30.f64 * ctx.f4.f64 + ctx.f31.f64));
	// fmuls f3,f6,f3
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f3.f64));
	// fmadds f26,f8,f5,f26
	ctx.f26.f64 = double(float(ctx.f8.f64 * ctx.f5.f64 + ctx.f26.f64));
	// fmsubs f28,f9,f6,f28
	ctx.f28.f64 = double(float(ctx.f9.f64 * ctx.f6.f64 - ctx.f28.f64));
	// fmsubs f5,f5,f11,f25
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f11.f64 - ctx.f25.f64));
	// fmsubs f7,f8,f7,f23
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f7.f64 - ctx.f23.f64));
	// fmadds f25,f27,f4,f21
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f4.f64 + ctx.f21.f64));
	// fmadds f29,f2,f4,f29
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f4.f64 + ctx.f29.f64));
	// fmsubs f23,f1,f4,f19
	ctx.f23.f64 = double(float(ctx.f1.f64 * ctx.f4.f64 - ctx.f19.f64));
	// fmadds f6,f6,f11,f26
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f11.f64 + ctx.f26.f64));
	// fmuls f28,f28,f4
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f4.f64));
	// fmuls f5,f5,f4
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// fmuls f4,f7,f4
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f4.f64));
	// fmadds f7,f1,f13,f31
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f13.f64 + ctx.f31.f64));
	// fmadds f1,f1,f10,f25
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f10.f64 + ctx.f25.f64));
	// fmadds f31,f27,f13,f29
	ctx.f31.f64 = double(float(ctx.f27.f64 * ctx.f13.f64 + ctx.f29.f64));
	// fnmsubs f29,f27,f10,f23
	ctx.f29.f64 = double(float(-(ctx.f27.f64 * ctx.f10.f64 - ctx.f23.f64)));
	// fmuls f8,f8,f6
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f6.f64));
	// fmuls f9,f9,f6
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f6.f64));
	// fmuls f11,f6,f11
	ctx.f11.f64 = double(float(ctx.f6.f64 * ctx.f11.f64));
	// fadds f5,f17,f5
	ctx.f5.f64 = double(float(ctx.f17.f64 + ctx.f5.f64));
	// fadds f6,f3,f4
	ctx.f6.f64 = double(float(ctx.f3.f64 + ctx.f4.f64));
	// fadds f28,f18,f28
	ctx.f28.f64 = double(float(ctx.f18.f64 + ctx.f28.f64));
	// fnmsubs f3,f2,f13,f1
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f13.f64 - ctx.f1.f64)));
	// stfs f3,84(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f1,f30,f10,f31
	ctx.f1.f64 = double(float(-(ctx.f30.f64 * ctx.f10.f64 - ctx.f31.f64)));
	// stfs f1,88(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fnmsubs f4,f27,f12,f7
	ctx.f4.f64 = double(float(-(ctx.f27.f64 * ctx.f12.f64 - ctx.f7.f64)));
	// stfs f4,80(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fnmsubs f13,f2,f12,f29
	ctx.f13.f64 = double(float(-(ctx.f2.f64 * ctx.f12.f64 - ctx.f29.f64)));
	// stfs f13,92(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fadds f10,f5,f9
	ctx.f10.f64 = double(float(ctx.f5.f64 + ctx.f9.f64));
	// fadds f9,f6,f11
	ctx.f9.f64 = double(float(ctx.f6.f64 + ctx.f11.f64));
	// fadds f12,f28,f8
	ctx.f12.f64 = double(float(ctx.f28.f64 + ctx.f8.f64));
	// fmuls f7,f10,f0
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f6,f9,f0
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f8,f12,f0
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fadds f4,f22,f7
	ctx.f4.f64 = double(float(ctx.f22.f64 + ctx.f7.f64));
	// stfs f4,104(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fadds f3,f6,f20
	ctx.f3.f64 = double(float(ctx.f6.f64 + ctx.f20.f64));
	// stfs f3,96(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f5,f24,f8
	ctx.f5.f64 = double(float(ctx.f24.f64 + ctx.f8.f64));
	// stfs f5,100(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// lwz r6,8(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// lwz r5,12(r8)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lwz r4,0(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r10,4(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r3,100(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r11,104(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r9,96(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r4,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r4.u32);
	// stw r10,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, ctx.r10.u32);
	// stw r6,8(r7)
	PPC_STORE_U32(ctx.r7.u32 + 8, ctx.r6.u32);
	// stw r5,12(r7)
	PPC_STORE_U32(ctx.r7.u32 + 12, ctx.r5.u32);
	// stw r3,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r3.u32);
	// stw r9,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r9.u32);
	// stw r11,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r11.u32);
loc_830C1794:
	// lwz r11,152(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 152);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r3,1412(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 1412);
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r11.u32);
	// bl 0x8315fb80
	ctx.lr = 0x830C17A8;
	sub_8315FB80(ctx, base);
	// stw r3,156(r31)
	PPC_STORE_U32(ctx.r31.u32 + 156, ctx.r3.u32);
	// b 0x830c17b4
	goto loc_830C17B4;
loc_830C17B0:
	// stw r30,156(r31)
	PPC_STORE_U32(ctx.r31.u32 + 156, ctx.r30.u32);
loc_830C17B4:
	// lwz r11,96(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 96);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830c17dc
	if (ctx.cr6.eq) goto loc_830C17DC;
	// lwz r11,336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// addi r3,r11,4
	ctx.r3.s64 = ctx.r11.s64 + 4;
	// lwz r11,20(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x830c17dc
	if (!ctx.cr6.eq) goto loc_830C17DC;
	// bl 0x82d53168
	ctx.lr = 0x830C17DC;
	sub_82D53168(ctx, base);
loc_830C17DC:
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x830c1810
	if (ctx.cr6.eq) goto loc_830C1810;
	// lwz r10,288(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 288);
	// cmpwi cr6,r10,6
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 6, ctx.xer);
	// bne cr6,0x830c1810
	if (!ctx.cr6.eq) goto loc_830C1810;
	// lwz r10,276(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 276);
	// rlwinm r9,r10,0,24,24
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x830c1810
	if (ctx.cr6.eq) goto loc_830C1810;
	// lwz r10,272(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 272);
	// ori r9,r10,256
	ctx.r9.u64 = ctx.r10.u64 | 256;
	// stw r9,272(r11)
	PPC_STORE_U32(ctx.r11.u32 + 272, ctx.r9.u32);
loc_830C1810:
	// lwz r11,268(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// lwz r10,768(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 768);
	// rlwinm r9,r10,25,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x830c183c
	if (ctx.cr6.eq) goto loc_830C183C;
	// lwz r11,336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// addi r3,r11,4
	ctx.r3.s64 = ctx.r11.s64 + 4;
	// lwz r11,92(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 92);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x830c183c
	if (!ctx.cr6.eq) goto loc_830C183C;
	// bl 0x82d532d0
	ctx.lr = 0x830C183C;
	sub_82D532D0(ctx, base);
loc_830C183C:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// addi r12,r1,-40
	ctx.r12.s64 = ctx.r1.s64 + -40;
	// bl 0x82cb6b08
	ctx.lr = 0x830C184C;
	__restfpr_17(ctx, base);
	// b 0x82cb1138
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C1850"))) PPC_WEAK_FUNC(sub_830C1850);
PPC_FUNC_IMPL(__imp__sub_830C1850) {
	PPC_FUNC_PROLOGUE();
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C1858"))) PPC_WEAK_FUNC(sub_830C1858);
PPC_FUNC_IMPL(__imp__sub_830C1858) {
	PPC_FUNC_PROLOGUE();
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C1860"))) PPC_WEAK_FUNC(sub_830C1860);
PPC_FUNC_IMPL(__imp__sub_830C1860) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C1864"))) PPC_WEAK_FUNC(sub_830C1864);
PPC_FUNC_IMPL(__imp__sub_830C1864) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C1868"))) PPC_WEAK_FUNC(sub_830C1868);
PPC_FUNC_IMPL(__imp__sub_830C1868) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C186C"))) PPC_WEAK_FUNC(sub_830C186C);
PPC_FUNC_IMPL(__imp__sub_830C186C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C1870"))) PPC_WEAK_FUNC(sub_830C1870);
PPC_FUNC_IMPL(__imp__sub_830C1870) {
	PPC_FUNC_PROLOGUE();
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r11.u32);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C1884"))) PPC_WEAK_FUNC(sub_830C1884);
PPC_FUNC_IMPL(__imp__sub_830C1884) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C1888"))) PPC_WEAK_FUNC(sub_830C1888);
PPC_FUNC_IMPL(__imp__sub_830C1888) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rlwinm r10,r4,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + ctx.r11.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C1898"))) PPC_WEAK_FUNC(sub_830C1898);
PPC_FUNC_IMPL(__imp__sub_830C1898) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rlwinm r10,r4,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + ctx.r11.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C18A8"))) PPC_WEAK_FUNC(sub_830C18A8);
PPC_FUNC_IMPL(__imp__sub_830C18A8) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C18B0"))) PPC_WEAK_FUNC(sub_830C18B0);
PPC_FUNC_IMPL(__imp__sub_830C18B0) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,4(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C18B8"))) PPC_WEAK_FUNC(sub_830C18B8);
PPC_FUNC_IMPL(__imp__sub_830C18B8) {
	PPC_FUNC_PROLOGUE();
	// lis r10,-31901
	ctx.r10.s64 = -2090663936;
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
	// rlwinm r5,r4,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r4,r11
	ctx.r4.u64 = ctx.r11.u64;
	// lwz r3,-32308(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + -32308);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r8,16(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__sub_830C18DC"))) PPC_WEAK_FUNC(sub_830C18DC);
PPC_FUNC_IMPL(__imp__sub_830C18DC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C18E0"))) PPC_WEAK_FUNC(sub_830C18E0);
PPC_FUNC_IMPL(__imp__sub_830C18E0) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// lis r11,-31901
	ctx.r11.s64 = -2090663936;
	// lwz r3,-32308(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -32308);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,20(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__sub_830C1900"))) PPC_WEAK_FUNC(sub_830C1900);
PPC_FUNC_IMPL(__imp__sub_830C1900) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C1904"))) PPC_WEAK_FUNC(sub_830C1904);
PPC_FUNC_IMPL(__imp__sub_830C1904) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C1908"))) PPC_WEAK_FUNC(sub_830C1908);
PPC_FUNC_IMPL(__imp__sub_830C1908) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C190C"))) PPC_WEAK_FUNC(sub_830C190C);
PPC_FUNC_IMPL(__imp__sub_830C190C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C1910"))) PPC_WEAK_FUNC(sub_830C1910);
PPC_FUNC_IMPL(__imp__sub_830C1910) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x830c1924
	if (!ctx.cr6.eq) goto loc_830C1924;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_830C1924:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r11.s64;
	// srawi r3,r9,2
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r3.s64 = ctx.r9.s32 >> 2;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C1934"))) PPC_WEAK_FUNC(sub_830C1934);
PPC_FUNC_IMPL(__imp__sub_830C1934) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C1938"))) PPC_WEAK_FUNC(sub_830C1938);
PPC_FUNC_IMPL(__imp__sub_830C1938) {
	PPC_FUNC_PROLOGUE();
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// cmplw cr6,r4,r5
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r5.u32, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
loc_830C1944:
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// cmplw cr6,r4,r5
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r5.u32, ctx.xer);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// bne cr6,0x830c1944
	if (!ctx.cr6.eq) goto loc_830C1944;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C1960"))) PPC_WEAK_FUNC(sub_830C1960);
PPC_FUNC_IMPL(__imp__sub_830C1960) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-31901
	ctx.r11.s64 = -2090663936;
	// li r5,282
	ctx.r5.s64 = 282;
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r3,-32308(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -32308);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__sub_830C1980"))) PPC_WEAK_FUNC(sub_830C1980);
PPC_FUNC_IMPL(__imp__sub_830C1980) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x830c1994
	if (!ctx.cr6.eq) goto loc_830C1994;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_830C1994:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r11.s64;
	// srawi r3,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r9.s32 >> 4;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C19A4"))) PPC_WEAK_FUNC(sub_830C19A4);
PPC_FUNC_IMPL(__imp__sub_830C19A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C19A8"))) PPC_WEAK_FUNC(sub_830C19A8);
PPC_FUNC_IMPL(__imp__sub_830C19A8) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
loc_830C19B0:
	// lwz r11,0(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// addic. r5,r5,-1
	ctx.xer.ca = ctx.r5.u32 > 0;
	ctx.r5.s64 = ctx.r5.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r11.u32);
	// lwz r10,4(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// stw r10,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r10.u32);
	// lwz r9,8(r6)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// stw r9,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r9.u32);
	// lwz r8,12(r6)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	// stw r8,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r8.u32);
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// bne 0x830c19b0
	if (!ctx.cr0.eq) goto loc_830C19B0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C19E0"))) PPC_WEAK_FUNC(sub_830C19E0);
PPC_FUNC_IMPL(__imp__sub_830C19E0) {
	PPC_FUNC_PROLOGUE();
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// cmplw cr6,r4,r5
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r5.u32, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
loc_830C19EC:
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// lwz r9,8(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// lwz r8,12(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// addi r4,r4,16
	ctx.r4.s64 = ctx.r4.s64 + 16;
	// stw r8,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r8.u32);
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// cmplw cr6,r4,r5
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r5.u32, ctx.xer);
	// bne cr6,0x830c19ec
	if (!ctx.cr6.eq) goto loc_830C19EC;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C1A20"))) PPC_WEAK_FUNC(sub_830C1A20);
PPC_FUNC_IMPL(__imp__sub_830C1A20) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-31901
	ctx.r11.s64 = -2090663936;
	// li r5,282
	ctx.r5.s64 = 282;
	// rlwinm r4,r4,4,0,27
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r3,-32308(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -32308);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__sub_830C1A40"))) PPC_WEAK_FUNC(sub_830C1A40);
PPC_FUNC_IMPL(__imp__sub_830C1A40) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C1A44"))) PPC_WEAK_FUNC(sub_830C1A44);
PPC_FUNC_IMPL(__imp__sub_830C1A44) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C1A48"))) PPC_WEAK_FUNC(sub_830C1A48);
PPC_FUNC_IMPL(__imp__sub_830C1A48) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,40(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// rlwinm r10,r11,0,30,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFFFB;
	// stw r10,40(r3)
	PPC_STORE_U32(ctx.r3.u32 + 40, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C1A58"))) PPC_WEAK_FUNC(sub_830C1A58);
PPC_FUNC_IMPL(__imp__sub_830C1A58) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6ac0
	ctx.lr = 0x830C1A68;
	__savefpr_18(ctx, base);
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c1c78
	if (ctx.cr6.eq) goto loc_830C1C78;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c1c78
	if (ctx.cr6.eq) goto loc_830C1C78;
	// lfs f12,252(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f12.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f11,112(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 112);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f12
	ctx.f10.f64 = ctx.f12.f64;
	// fmuls f9,f11,f12
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f7,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,124(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 124);
	ctx.f6.f64 = double(temp.f32);
	// fmr f30,f7
	ctx.f30.f64 = ctx.f7.f64;
	// lfs f1,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f3,f6,f7
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// fmuls f2,f6,f12
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// lfs f8,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f8.f64 = double(temp.f32);
	// fmr f29,f1
	ctx.f29.f64 = ctx.f1.f64;
	// lfs f4,116(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 116);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f11,f7
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// lfs f31,120(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 120);
	ctx.f31.f64 = double(temp.f32);
	// lfs f28,136(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	ctx.f28.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f27,128(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 128);
	ctx.f27.f64 = double(temp.f32);
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f13,6380(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6380);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r3,112
	ctx.r10.s64 = ctx.r3.s64 + 112;
	// lfs f26,132(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 132);
	ctx.f26.f64 = double(temp.f32);
	// fmsubs f24,f8,f8,f13
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f8.f64 - ctx.f13.f64));
	// fmadds f9,f4,f8,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f9.f64));
	// lfs f23,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f21,f28,f30
	ctx.f21.f64 = double(float(ctx.f28.f64 * ctx.f30.f64));
	// lfs f0,7676(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f3,f11,f8,f3
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f8.f64 + ctx.f3.f64));
	// lfs f13,6140(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 6140);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f2,f31,f8,f2
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f8.f64 + ctx.f2.f64));
	// lfs f22,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f19,f27,f29
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// lfs f20,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fmsubs f5,f6,f8,f5
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f8.f64 - ctx.f5.f64));
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// fmuls f18,f27,f30
	ctx.f18.f64 = double(float(ctx.f27.f64 * ctx.f30.f64));
	// addi r9,r3,12
	ctx.r9.s64 = ctx.r3.s64 + 12;
	// fmuls f25,f26,f10
	ctx.f25.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// fmadds f9,f6,f1,f9
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f1.f64 + ctx.f9.f64));
	// fmsubs f21,f27,f10,f21
	ctx.f21.f64 = double(float(ctx.f27.f64 * ctx.f10.f64 - ctx.f21.f64));
	// fmadds f3,f31,f1,f3
	ctx.f3.f64 = double(float(ctx.f31.f64 * ctx.f1.f64 + ctx.f3.f64));
	// fmadds f2,f4,f7,f2
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f7.f64 + ctx.f2.f64));
	// fmsubs f19,f26,f30,f19
	ctx.f19.f64 = double(float(ctx.f26.f64 * ctx.f30.f64 - ctx.f19.f64));
	// fnmsubs f5,f4,f1,f5
	ctx.f5.f64 = double(float(-(ctx.f4.f64 * ctx.f1.f64 - ctx.f5.f64)));
	// fmuls f6,f26,f24
	ctx.f6.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// fmadds f26,f26,f29,f18
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f18.f64));
	// fmsubs f25,f28,f29,f25
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f29.f64 - ctx.f25.f64));
	// fmuls f18,f28,f24
	ctx.f18.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// fnmsubs f9,f31,f7,f9
	ctx.f9.f64 = double(float(-(ctx.f31.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// fmuls f27,f27,f24
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// fnmsubs f4,f4,f12,f3
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f12.f64 - ctx.f3.f64)));
	// fnmsubs f3,f11,f1,f2
	ctx.f3.f64 = double(float(-(ctx.f11.f64 * ctx.f1.f64 - ctx.f2.f64)));
	// fmuls f2,f21,f8
	ctx.f2.f64 = double(float(ctx.f21.f64 * ctx.f8.f64));
	// fnmsubs f5,f31,f12,f5
	ctx.f5.f64 = double(float(-(ctx.f31.f64 * ctx.f12.f64 - ctx.f5.f64)));
	// fmuls f1,f19,f8
	ctx.f1.f64 = double(float(ctx.f19.f64 * ctx.f8.f64));
	// fmadds f12,f28,f10,f26
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f10.f64 + ctx.f26.f64));
	// fmuls f7,f25,f8
	ctx.f7.f64 = double(float(ctx.f25.f64 * ctx.f8.f64));
	// fmuls f11,f9,f9
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f9.f64));
	// fmuls f31,f3,f3
	ctx.f31.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// fadds f6,f6,f2
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f2.f64));
	// fmuls f28,f5,f3
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fadds f1,f18,f1
	ctx.f1.f64 = double(float(ctx.f18.f64 + ctx.f1.f64));
	// fmuls f2,f29,f12
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f12.f64));
	// fmuls f10,f12,f10
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// fadds f8,f27,f7
	ctx.f8.f64 = double(float(ctx.f27.f64 + ctx.f7.f64));
	// fmuls f7,f9,f4
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f12,f12,f30
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f30.f64));
	// fmuls f31,f31,f0
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fmuls f30,f28,f0
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// fadds f6,f6,f2
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f2.f64));
	// fadds f2,f1,f10
	ctx.f2.f64 = double(float(ctx.f1.f64 + ctx.f10.f64));
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fsubs f1,f13,f11
	ctx.f1.f64 = double(float(ctx.f13.f64 - ctx.f11.f64));
	// fsubs f10,f7,f30
	ctx.f10.f64 = double(float(ctx.f7.f64 - ctx.f30.f64));
	// stfs f10,-156(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -156, temp.u32);
	// fsubs f1,f1,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 - ctx.f31.f64));
	// stfs f1,-160(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -160, temp.u32);
	// fmuls f10,f3,f4
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f4.f64));
	// fmuls f1,f5,f9
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// fmuls f9,f3,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f9.f64));
	// addi r11,r1,-160
	ctx.r11.s64 = ctx.r1.s64 + -160;
	// fmuls f5,f5,f4
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f29,f4,f4
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f4.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fadds f4,f8,f12
	ctx.f4.f64 = double(float(ctx.f8.f64 + ctx.f12.f64));
	// fmuls f3,f2,f0
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmuls f2,f6,f0
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fadds f12,f30,f7
	ctx.f12.f64 = double(float(ctx.f30.f64 + ctx.f7.f64));
	// stfs f12,-148(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -148, temp.u32);
	// fmuls f8,f1,f0
	ctx.f8.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f6,f9,f0
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fnmsubs f7,f29,f0,f13
	ctx.f7.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fadds f13,f20,f3
	ctx.f13.f64 = double(float(ctx.f20.f64 + ctx.f3.f64));
	// fadds f0,f22,f2
	ctx.f0.f64 = double(float(ctx.f22.f64 + ctx.f2.f64));
	// fadds f3,f8,f10
	ctx.f3.f64 = double(float(ctx.f8.f64 + ctx.f10.f64));
	// stfs f3,-152(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + -152, temp.u32);
	// fsubs f1,f10,f8
	ctx.f1.f64 = double(float(ctx.f10.f64 - ctx.f8.f64));
	// stfs f1,-136(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -136, temp.u32);
	// fsubs f12,f6,f5
	ctx.f12.f64 = double(float(ctx.f6.f64 - ctx.f5.f64));
	// stfs f12,-140(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -140, temp.u32);
	// fsubs f2,f7,f31
	ctx.f2.f64 = double(float(ctx.f7.f64 - ctx.f31.f64));
	// stfs f2,-144(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + -144, temp.u32);
	// fadds f10,f5,f6
	ctx.f10.f64 = double(float(ctx.f5.f64 + ctx.f6.f64));
	// stfs f10,-132(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -132, temp.u32);
	// fsubs f9,f7,f11
	ctx.f9.f64 = double(float(ctx.f7.f64 - ctx.f11.f64));
	// stfs f9,-128(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -128, temp.u32);
	// fadds f12,f23,f4
	ctx.f12.f64 = double(float(ctx.f23.f64 + ctx.f4.f64));
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830C1C4C:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c1c4c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C1C4C;
	// stfs f12,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f0,40(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f13,44(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
loc_830C1C78:
	// lfs f0,8(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r3,12
	ctx.r11.s64 = ctx.r3.s64 + 12;
	// lfs f12,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f10,f12,f0
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f11,28(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f11,f13
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fmuls f7,f9,f13
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// lfs f5,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f5.f64 = double(temp.f32);
	// lfs f6,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// lfs f4,24(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,36(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,12(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,32(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	ctx.f1.f64 = double(temp.f32);
	// lfs f12,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f10,f5,f13,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f13.f64 + ctx.f10.f64));
	// lfs f11,48(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f8,f4,f6,f8
	ctx.f8.f64 = double(float(ctx.f4.f64 * ctx.f6.f64 + ctx.f8.f64));
	// lfs f5,56(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f4,f3,f6,f7
	ctx.f4.f64 = double(float(ctx.f3.f64 * ctx.f6.f64 + ctx.f7.f64));
	// fmadds f3,f6,f2,f10
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f2.f64 + ctx.f10.f64));
	// fmadds f2,f1,f0,f8
	ctx.f2.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 + ctx.f8.f64));
	// fmadds f1,f12,f0,f4
	ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 + ctx.f4.f64));
	// fadds f0,f11,f3
	ctx.f0.f64 = double(float(ctx.f11.f64 + ctx.f3.f64));
	// stfs f0,0(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// fadds f13,f9,f2
	ctx.f13.f64 = double(float(ctx.f9.f64 + ctx.f2.f64));
	// stfs f13,4(r5)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// fadds f12,f5,f1
	ctx.f12.f64 = double(float(ctx.f5.f64 + ctx.f1.f64));
	// stfs f12,8(r5)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6b0c
	ctx.lr = 0x830C1CFC;
	__restfpr_18(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C1D08"))) PPC_WEAK_FUNC(sub_830C1D08);
PPC_FUNC_IMPL(__imp__sub_830C1D08) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,348(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 348);
	// rlwinm r10,r4,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// lwzx r3,r11,r10
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C1D18"))) PPC_WEAK_FUNC(sub_830C1D18);
PPC_FUNC_IMPL(__imp__sub_830C1D18) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,348(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 348);
	// rlwinm r10,r4,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// lwzx r9,r11,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	// cntlzw r8,r9
	ctx.r8.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r7,r8,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// xori r3,r7,1
	ctx.r3.u64 = ctx.r7.u64 ^ 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C1D34"))) PPC_WEAK_FUNC(sub_830C1D34);
PPC_FUNC_IMPL(__imp__sub_830C1D34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C1D38"))) PPC_WEAK_FUNC(sub_830C1D38);
PPC_FUNC_IMPL(__imp__sub_830C1D38) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10dc
	ctx.lr = 0x830C1D40;
	__savegprlr_25(ctx, base);
	// addi r12,r1,-64
	ctx.r12.s64 = ctx.r1.s64 + -64;
	// bl 0x82cb6ab8
	ctx.lr = 0x830C1D48;
	__savefpr_16(ctx, base);
	// stwu r1,-400(r1)
	ea = -400 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// mr r26,r4
	ctx.r26.u64 = ctx.r4.u64;
	// clrlwi r25,r5,24
	ctx.r25.u64 = ctx.r5.u32 & 0xFF;
	// rlwinm r28,r26,4,0,27
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 4) & 0xFFFFFFF0;
	// cmplwi cr6,r25,0
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, 0, ctx.xer);
	// lwz r11,348(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// add r11,r28,r11
	ctx.r11.u64 = ctx.r28.u64 + ctx.r11.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// bne cr6,0x830c1d7c
	if (!ctx.cr6.eq) goto loc_830C1D7C;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r10.u32);
	// b 0x830c1d84
	goto loc_830C1D84;
loc_830C1D7C:
	// oris r9,r10,16384
	ctx.r9.u64 = ctx.r10.u64 | 1073741824;
	// stw r9,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r9.u32);
loc_830C1D84:
	// lwz r11,344(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 344);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x830c244c
	if (!ctx.cr6.eq) goto loc_830C244C;
	// lwz r11,348(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// add r11,r28,r11
	ctx.r11.u64 = ctx.r28.u64 + ctx.r11.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x830c244c
	if (!ctx.cr6.gt) goto loc_830C244C;
	// lwz r11,0(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x830c244c
	if (!ctx.cr6.eq) goto loc_830C244C;
	// lwz r11,348(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// li r27,0
	ctx.r27.s64 = 0;
	// add r11,r28,r11
	ctx.r11.u64 = ctx.r28.u64 + ctx.r11.u64;
	// stb r27,12(r11)
	PPC_STORE_U8(ctx.r11.u32 + 12, ctx.r27.u8);
	// lwz r10,348(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// lwzx r9,r28,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r10.u32);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x830c244c
	if (!ctx.cr6.eq) goto loc_830C244C;
	// lfs f0,112(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 112);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f11,120(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 120);
	ctx.f11.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f13,116(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 116);
	ctx.f13.f64 = double(temp.f32);
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// lfs f5,124(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 124);
	ctx.f5.f64 = double(temp.f32);
	// lfs f8,184(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 184);
	ctx.f8.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f30,6380(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6380);
	ctx.f30.f64 = double(temp.f32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// fmsubs f4,f5,f5,f30
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f5.f64 - ctx.f30.f64));
	// lfs f6,180(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 180);
	ctx.f6.f64 = double(temp.f32);
	// lfs f7,176(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 176);
	ctx.f7.f64 = double(temp.f32);
	// addi r31,r29,112
	ctx.r31.s64 = ctx.r29.s64 + 112;
	// lfs f3,168(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 168);
	ctx.f3.f64 = double(temp.f32);
	// addi r30,r29,160
	ctx.r30.s64 = ctx.r29.s64 + 160;
	// lfs f2,172(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 172);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f3,f13
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// lfs f29,160(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 160);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f28,f2,f11
	ctx.f28.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// fmuls f27,f8,f12
	ctx.f27.f64 = double(float(ctx.f8.f64 * ctx.f12.f64));
	// lfs f26,164(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 164);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f24,f8,f9
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// lfs f31,7676(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 7676);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f25,f7,f10
	ctx.f25.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
	// lfs f23,132(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 132);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f22,f6,f9
	ctx.f22.f64 = double(float(ctx.f6.f64 * ctx.f9.f64));
	// lfs f21,136(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 136);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f20,f29,f11
	ctx.f20.f64 = double(float(ctx.f29.f64 * ctx.f11.f64));
	// lfs f19,128(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 128);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f17,f6,f4
	ctx.f17.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// addi r9,r1,124
	ctx.r9.s64 = ctx.r1.s64 + 124;
	// fmuls f16,f8,f4
	ctx.f16.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// fmuls f4,f7,f4
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f4.f64));
	// fmuls f18,f29,f0
	ctx.f18.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// fmadds f1,f29,f5,f1
	ctx.f1.f64 = double(float(ctx.f29.f64 * ctx.f5.f64 + ctx.f1.f64));
	// fmsubs f27,f7,f9,f27
	ctx.f27.f64 = double(float(ctx.f7.f64 * ctx.f9.f64 - ctx.f27.f64));
	// fmadds f7,f7,f12,f24
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f12.f64 + ctx.f24.f64));
	// fmsubs f25,f6,f12,f25
	ctx.f25.f64 = double(float(ctx.f6.f64 * ctx.f12.f64 - ctx.f25.f64));
	// fmsubs f8,f8,f10,f22
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f10.f64 - ctx.f22.f64));
	// fmadds f28,f3,f5,f28
	ctx.f28.f64 = double(float(ctx.f3.f64 * ctx.f5.f64 + ctx.f28.f64));
	// fmadds f24,f26,f5,f20
	ctx.f24.f64 = double(float(ctx.f26.f64 * ctx.f5.f64 + ctx.f20.f64));
	// fmsubs f22,f2,f5,f18
	ctx.f22.f64 = double(float(ctx.f2.f64 * ctx.f5.f64 - ctx.f18.f64));
	// fmuls f27,f27,f5
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f5.f64));
	// fmadds f7,f6,f10,f7
	ctx.f7.f64 = double(float(ctx.f6.f64 * ctx.f10.f64 + ctx.f7.f64));
	// fmuls f25,f25,f5
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f5.f64));
	// fmuls f6,f8,f5
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// fmadds f5,f2,f0,f1
	ctx.f5.f64 = double(float(ctx.f2.f64 * ctx.f0.f64 + ctx.f1.f64));
	// fmadds f1,f26,f0,f28
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f0.f64 + ctx.f28.f64));
	// fmadds f2,f2,f13,f24
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f13.f64 + ctx.f24.f64));
	// fnmsubs f8,f26,f13,f22
	ctx.f8.f64 = double(float(-(ctx.f26.f64 * ctx.f13.f64 - ctx.f22.f64)));
	// fadds f28,f17,f27
	ctx.f28.f64 = double(float(ctx.f17.f64 + ctx.f27.f64));
	// fmuls f10,f7,f10
	ctx.f10.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
	// fmuls f9,f7,f9
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f9.f64));
	// fadds f6,f4,f6
	ctx.f6.f64 = double(float(ctx.f4.f64 + ctx.f6.f64));
	// fmuls f7,f7,f12
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// fadds f27,f16,f25
	ctx.f27.f64 = double(float(ctx.f16.f64 + ctx.f25.f64));
	// fnmsubs f4,f3,f0,f2
	ctx.f4.f64 = double(float(-(ctx.f3.f64 * ctx.f0.f64 - ctx.f2.f64)));
	// stfs f4,84(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f2,f29,f13,f1
	ctx.f2.f64 = double(float(-(ctx.f29.f64 * ctx.f13.f64 - ctx.f1.f64)));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fnmsubs f1,f3,f11,f8
	ctx.f1.f64 = double(float(-(ctx.f3.f64 * ctx.f11.f64 - ctx.f8.f64)));
	// stfs f1,92(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fnmsubs f5,f26,f11,f5
	ctx.f5.f64 = double(float(-(ctx.f26.f64 * ctx.f11.f64 - ctx.f5.f64)));
	// stfs f5,80(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fadds f0,f28,f10
	ctx.f0.f64 = double(float(ctx.f28.f64 + ctx.f10.f64));
	// fadds f12,f6,f7
	ctx.f12.f64 = double(float(ctx.f6.f64 + ctx.f7.f64));
	// fadds f13,f27,f9
	ctx.f13.f64 = double(float(ctx.f27.f64 + ctx.f9.f64));
	// fmuls f11,f0,f31
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fmuls f9,f12,f31
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// fmuls f10,f13,f31
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// fadds f8,f23,f11
	ctx.f8.f64 = double(float(ctx.f23.f64 + ctx.f11.f64));
	// stfs f8,100(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f6,f19,f9
	ctx.f6.f64 = double(float(ctx.f19.f64 + ctx.f9.f64));
	// stfs f6,96(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f7,f21,f10
	ctx.f7.f64 = double(float(ctx.f21.f64 + ctx.f10.f64));
	// stfs f7,104(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// lwz r7,8(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// lwz r6,12(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lwz r11,4(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r5,100(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r4,104(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r8,96(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r11.u32);
	// stw r7,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r7.u32);
	// stw r6,12(r9)
	PPC_STORE_U32(ctx.r9.u32 + 12, ctx.r6.u32);
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// stw r5,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r5.u32);
	// stw r8,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r8.u32);
	// stw r4,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r4.u32);
	// bl 0x8315fb70
	ctx.lr = 0x830C1F4C;
	sub_8315FB70(ctx, base);
	// lwz r11,264(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 264);
	// li r7,10
	ctx.r7.s64 = 10;
	// stw r7,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r7.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c2220
	if (ctx.cr6.eq) goto loc_830C2220;
	// lfs f0,216(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 216);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r11,216
	ctx.r10.s64 = ctx.r11.s64 + 216;
	// lfs f5,240(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 240);
	ctx.f5.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f13,220(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 220);
	ctx.f13.f64 = double(temp.f32);
	// fneg f3,f5
	ctx.f3.u64 = ctx.f5.u64 ^ 0x8000000000000000;
	// lfs f11,224(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 224);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// lfs f9,232(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 232);
	ctx.f9.f64 = double(temp.f32);
	// fmr f8,f11
	ctx.f8.f64 = ctx.f11.f64;
	// lfs f7,236(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 236);
	ctx.f7.f64 = double(temp.f32);
	// fneg f6,f9
	ctx.f6.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// fneg f4,f7
	ctx.f4.u64 = ctx.f7.u64 ^ 0x8000000000000000;
	// lfs f2,228(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 228);
	ctx.f2.f64 = double(temp.f32);
	// fneg f7,f13
	ctx.f7.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// lfs f9,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f9.f64 = double(temp.f32);
	// fneg f1,f0
	ctx.f1.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lfs f0,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f0.f64 = double(temp.f32);
	// fneg f11,f11
	ctx.f11.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// lfs f5,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f5.f64 = double(temp.f32);
	// fmsubs f28,f2,f2,f30
	ctx.f28.f64 = double(float(ctx.f2.f64 * ctx.f2.f64 - ctx.f30.f64));
	// lfs f29,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f29.f64 = double(temp.f32);
	// fmr f13,f2
	ctx.f13.f64 = ctx.f2.f64;
	// lfs f27,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f24,f3,f12
	ctx.f24.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// fmuls f21,f8,f3
	ctx.f21.f64 = double(float(ctx.f8.f64 * ctx.f3.f64));
	// fmuls f26,f10,f6
	ctx.f26.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// fmuls f25,f8,f4
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// fmuls f20,f9,f7
	ctx.f20.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// fmuls f23,f0,f1
	ctx.f23.f64 = double(float(ctx.f0.f64 * ctx.f1.f64));
	// fmuls f22,f9,f11
	ctx.f22.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// fmuls f19,f5,f7
	ctx.f19.f64 = double(float(ctx.f5.f64 * ctx.f7.f64));
	// fmuls f17,f28,f6
	ctx.f17.f64 = double(float(ctx.f28.f64 * ctx.f6.f64));
	// fmsubs f24,f8,f6,f24
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f6.f64 - ctx.f24.f64));
	// fmsubs f18,f13,f13,f30
	ctx.f18.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 - ctx.f30.f64));
	// fmadds f6,f12,f6,f21
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f6.f64 + ctx.f21.f64));
	// fmsubs f26,f4,f12,f26
	ctx.f26.f64 = double(float(ctx.f4.f64 * ctx.f12.f64 - ctx.f26.f64));
	// fmsubs f25,f10,f3,f25
	ctx.f25.f64 = double(float(ctx.f10.f64 * ctx.f3.f64 - ctx.f25.f64));
	// fmadds f21,f5,f1,f20
	ctx.f21.f64 = double(float(ctx.f5.f64 * ctx.f1.f64 + ctx.f20.f64));
	// fmsubs f23,f5,f11,f23
	ctx.f23.f64 = double(float(ctx.f5.f64 * ctx.f11.f64 - ctx.f23.f64));
	// fmsubs f22,f0,f7,f22
	ctx.f22.f64 = double(float(ctx.f0.f64 * ctx.f7.f64 - ctx.f22.f64));
	// fmsubs f20,f9,f1,f19
	ctx.f20.f64 = double(float(ctx.f9.f64 * ctx.f1.f64 - ctx.f19.f64));
	// fmuls f19,f4,f28
	ctx.f19.f64 = double(float(ctx.f4.f64 * ctx.f28.f64));
	// fmuls f3,f3,f28
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f28.f64));
	// fmuls f28,f0,f18
	ctx.f28.f64 = double(float(ctx.f0.f64 * ctx.f18.f64));
	// fmadds f6,f10,f4,f6
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f4.f64 + ctx.f6.f64));
	// fmuls f26,f26,f2
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f2.f64));
	// fmuls f25,f25,f2
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f2.f64));
	// fmadds f4,f0,f11,f21
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 + ctx.f21.f64));
	// fmuls f2,f24,f2
	ctx.f2.f64 = double(float(ctx.f24.f64 * ctx.f2.f64));
	// fmuls f9,f9,f18
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f18.f64));
	// fmuls f0,f23,f13
	ctx.f0.f64 = double(float(ctx.f23.f64 * ctx.f13.f64));
	// fmuls f5,f5,f18
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f18.f64));
	// fmuls f24,f13,f22
	ctx.f24.f64 = double(float(ctx.f13.f64 * ctx.f22.f64));
	// fmuls f23,f20,f13
	ctx.f23.f64 = double(float(ctx.f20.f64 * ctx.f13.f64));
	// fmuls f8,f8,f6
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f6.f64));
	// fsubs f3,f3,f26
	ctx.f3.f64 = double(float(ctx.f3.f64 - ctx.f26.f64));
	// fmuls f10,f10,f6
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// fsubs f26,f17,f25
	ctx.f26.f64 = double(float(ctx.f17.f64 - ctx.f25.f64));
	// fsubs f2,f19,f2
	ctx.f2.f64 = double(float(ctx.f19.f64 - ctx.f2.f64));
	// fmuls f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// fadds f0,f9,f0
	ctx.f0.f64 = double(float(ctx.f9.f64 + ctx.f0.f64));
	// fmuls f25,f7,f4
	ctx.f25.f64 = double(float(ctx.f7.f64 * ctx.f4.f64));
	// fmuls f6,f4,f1
	ctx.f6.f64 = double(float(ctx.f4.f64 * ctx.f1.f64));
	// fadds f9,f5,f24
	ctx.f9.f64 = double(float(ctx.f5.f64 + ctx.f24.f64));
	// fmuls f4,f11,f4
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// fadds f5,f28,f23
	ctx.f5.f64 = double(float(ctx.f28.f64 + ctx.f23.f64));
	// fadds f8,f3,f8
	ctx.f8.f64 = double(float(ctx.f3.f64 + ctx.f8.f64));
	// fadds f3,f2,f10
	ctx.f3.f64 = double(float(ctx.f2.f64 + ctx.f10.f64));
	// fadds f12,f26,f12
	ctx.f12.f64 = double(float(ctx.f26.f64 + ctx.f12.f64));
	// fadds f2,f0,f25
	ctx.f2.f64 = double(float(ctx.f0.f64 + ctx.f25.f64));
	// fadds f0,f9,f6
	ctx.f0.f64 = double(float(ctx.f9.f64 + ctx.f6.f64));
	// fadds f10,f5,f4
	ctx.f10.f64 = double(float(ctx.f5.f64 + ctx.f4.f64));
	// fmuls f8,f8,f31
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fmuls f6,f3,f31
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// fmuls f9,f12,f31
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// fmuls f5,f2,f31
	ctx.f5.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f2,f29,f13
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f13.f64));
	// fmuls f4,f0,f31
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// lfs f0,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f3,f10,f31
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f31.f64));
	// fmuls f28,f27,f11
	ctx.f28.f64 = double(float(ctx.f27.f64 * ctx.f11.f64));
	// lfs f10,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f12,f27,f13
	ctx.f12.f64 = double(float(ctx.f27.f64 * ctx.f13.f64));
	// lfs f23,16(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f2,f10,f1,f2
	ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f1.f64 + ctx.f2.f64));
	// lfs f24,20(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f25,f27,f1
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f1.f64));
	// lfs f26,24(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 24);
	ctx.f26.f64 = double(temp.f32);
	// fadds f9,f4,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 + ctx.f9.f64));
	// lfs f4,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f4.f64 = double(temp.f32);
	// fadds f6,f5,f6
	ctx.f6.f64 = double(float(ctx.f5.f64 + ctx.f6.f64));
	// lfs f5,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// lfs f22,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f22.f64 = double(temp.f32);
	// fadds f3,f3,f8
	ctx.f3.f64 = double(float(ctx.f3.f64 + ctx.f8.f64));
	// lfs f8,12(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f8.f64 = double(temp.f32);
	// lwz r8,184(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 184);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r9,r1,172
	ctx.r9.s64 = ctx.r1.s64 + 172;
	// fmadds f28,f10,f13,f28
	ctx.f28.f64 = double(float(ctx.f10.f64 * ctx.f13.f64 + ctx.f28.f64));
	// fmadds f12,f0,f1,f12
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f1.f64 + ctx.f12.f64));
	// fmadds f2,f0,f11,f2
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 + ctx.f2.f64));
	// fmsubs f13,f0,f13,f25
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 - ctx.f25.f64));
	// fmadds f0,f0,f7,f28
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f7.f64 + ctx.f28.f64));
	// fmadds f12,f29,f7,f12
	ctx.f12.f64 = double(float(ctx.f29.f64 * ctx.f7.f64 + ctx.f12.f64));
	// fnmsubs f13,f10,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f10.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// fnmsubs f7,f27,f7,f2
	ctx.f7.f64 = double(float(-(ctx.f27.f64 * ctx.f7.f64 - ctx.f2.f64)));
	// fnmsubs f1,f29,f1,f0
	ctx.f1.f64 = double(float(-(ctx.f29.f64 * ctx.f1.f64 - ctx.f0.f64)));
	// fnmsubs f2,f10,f11,f12
	ctx.f2.f64 = double(float(-(ctx.f10.f64 * ctx.f11.f64 - ctx.f12.f64)));
	// fnmsubs f0,f29,f11,f13
	ctx.f0.f64 = double(float(-(ctx.f29.f64 * ctx.f11.f64 - ctx.f13.f64)));
	// fmuls f13,f24,f7
	ctx.f13.f64 = double(float(ctx.f24.f64 * ctx.f7.f64));
	// fmuls f28,f7,f5
	ctx.f28.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// fmuls f11,f23,f1
	ctx.f11.f64 = double(float(ctx.f23.f64 * ctx.f1.f64));
	// fmuls f10,f24,f1
	ctx.f10.f64 = double(float(ctx.f24.f64 * ctx.f1.f64));
	// fmuls f12,f26,f2
	ctx.f12.f64 = double(float(ctx.f26.f64 * ctx.f2.f64));
	// fmuls f29,f1,f4
	ctx.f29.f64 = double(float(ctx.f1.f64 * ctx.f4.f64));
	// fmsubs f30,f0,f0,f30
	ctx.f30.f64 = double(float(ctx.f0.f64 * ctx.f0.f64 - ctx.f30.f64));
	// fmuls f27,f0,f4
	ctx.f27.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// fmsubs f13,f26,f1,f13
	ctx.f13.f64 = double(float(ctx.f26.f64 * ctx.f1.f64 - ctx.f13.f64));
	// fmuls f25,f2,f5
	ctx.f25.f64 = double(float(ctx.f2.f64 * ctx.f5.f64));
	// fmadds f28,f0,f22,f28
	ctx.f28.f64 = double(float(ctx.f0.f64 * ctx.f22.f64 + ctx.f28.f64));
	// fmsubs f11,f24,f2,f11
	ctx.f11.f64 = double(float(ctx.f24.f64 * ctx.f2.f64 - ctx.f11.f64));
	// fmadds f10,f23,f2,f10
	ctx.f10.f64 = double(float(ctx.f23.f64 * ctx.f2.f64 + ctx.f10.f64));
	// fmsubs f12,f23,f7,f12
	ctx.f12.f64 = double(float(ctx.f23.f64 * ctx.f7.f64 - ctx.f12.f64));
	// fmadds f29,f0,f5,f29
	ctx.f29.f64 = double(float(ctx.f0.f64 * ctx.f5.f64 + ctx.f29.f64));
	// fmuls f23,f23,f30
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f30.f64));
	// fmuls f24,f24,f30
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f30.f64));
	// fmadds f27,f7,f8,f27
	ctx.f27.f64 = double(float(ctx.f7.f64 * ctx.f8.f64 + ctx.f27.f64));
	// fmuls f30,f26,f30
	ctx.f30.f64 = double(float(ctx.f26.f64 * ctx.f30.f64));
	// fmuls f13,f0,f13
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmadds f10,f26,f7,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f7.f64 + ctx.f10.f64));
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmadds f29,f2,f8,f29
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f8.f64 + ctx.f29.f64));
	// fmsubs f0,f0,f8,f25
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 - ctx.f25.f64));
	// fmadds f8,f1,f8,f28
	ctx.f8.f64 = double(float(ctx.f1.f64 * ctx.f8.f64 + ctx.f28.f64));
	// fmadds f28,f2,f22,f27
	ctx.f28.f64 = double(float(ctx.f2.f64 * ctx.f22.f64 + ctx.f27.f64));
	// fadds f13,f23,f13
	ctx.f13.f64 = double(float(ctx.f23.f64 + ctx.f13.f64));
	// fadds f11,f30,f11
	ctx.f11.f64 = double(float(ctx.f30.f64 + ctx.f11.f64));
	// fmuls f30,f10,f2
	ctx.f30.f64 = double(float(ctx.f10.f64 * ctx.f2.f64));
	// fmuls f27,f1,f10
	ctx.f27.f64 = double(float(ctx.f1.f64 * ctx.f10.f64));
	// fmuls f10,f7,f10
	ctx.f10.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
	// fadds f12,f24,f12
	ctx.f12.f64 = double(float(ctx.f24.f64 + ctx.f12.f64));
	// fnmsubs f8,f2,f4,f8
	ctx.f8.f64 = double(float(-(ctx.f2.f64 * ctx.f4.f64 - ctx.f8.f64)));
	// stfs f8,84(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f0,f1,f22,f0
	ctx.f0.f64 = double(float(-(ctx.f1.f64 * ctx.f22.f64 - ctx.f0.f64)));
	// fnmsubs f5,f1,f5,f28
	ctx.f5.f64 = double(float(-(ctx.f1.f64 * ctx.f5.f64 - ctx.f28.f64)));
	// fnmsubs f29,f7,f22,f29
	ctx.f29.f64 = double(float(-(ctx.f7.f64 * ctx.f22.f64 - ctx.f29.f64)));
	// stfs f29,80(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fadds f2,f13,f30
	ctx.f2.f64 = double(float(ctx.f13.f64 + ctx.f30.f64));
	// fadds f13,f11,f10
	ctx.f13.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// fadds f1,f12,f27
	ctx.f1.f64 = double(float(ctx.f12.f64 + ctx.f27.f64));
	// fnmsubs f12,f7,f4,f0
	ctx.f12.f64 = double(float(-(ctx.f7.f64 * ctx.f4.f64 - ctx.f0.f64)));
	// fmuls f11,f2,f31
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f7,f13,f31
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// fmuls f10,f1,f31
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// fadds f4,f11,f9
	ctx.f4.f64 = double(float(ctx.f11.f64 + ctx.f9.f64));
	// stfs f4,96(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f1,f7,f3
	ctx.f1.f64 = double(float(ctx.f7.f64 + ctx.f3.f64));
	// stfs f1,104(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fadds f2,f10,f6
	ctx.f2.f64 = double(float(ctx.f10.f64 + ctx.f6.f64));
	// stfs f2,100(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stfs f12,92(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lwz r7,96(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stfs f5,88(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lwz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r6,4(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r8,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r8.u32);
	// stw r6,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r6.u32);
	// lwz r11,8(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lwz r10,12(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// stw r5,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r5.u32);
	// stw r10,12(r9)
	PPC_STORE_U32(ctx.r9.u32 + 12, ctx.r10.u32);
	// stw r11,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r11.u32);
	// stw r7,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r7.u32);
	// b 0x830c2370
	goto loc_830C2370;
loc_830C2220:
	// lfs f13,4(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lfs f0,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// lfs f11,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// lfs f5,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f5.f64 = double(temp.f32);
	// lfs f2,12(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f2.f64 = double(temp.f32);
	// fmsubs f4,f5,f5,f30
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f5.f64 - ctx.f30.f64));
	// lfs f6,16(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f30,f2,f0
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// lfs f8,24(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 24);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f28,f2,f11
	ctx.f28.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// lfs f7,20(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	ctx.f7.f64 = double(temp.f32);
	// addi r10,r1,172
	ctx.r10.s64 = ctx.r1.s64 + 172;
	// lfs f3,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f3.f64 = double(temp.f32);
	// stw r27,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r27.u32);
	// fmuls f1,f3,f11
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// lfs f27,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f19,f3,f0
	ctx.f19.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// lfs f29,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f24,f10,f6
	ctx.f24.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// lfs f25,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f26,f12,f8
	ctx.f26.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// lfs f23,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f20,f9,f8
	ctx.f20.f64 = double(float(ctx.f9.f64 * ctx.f8.f64));
	// lfs f21,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f22,f9,f7
	ctx.f22.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// fmadds f30,f3,f5,f30
	ctx.f30.f64 = double(float(ctx.f3.f64 * ctx.f5.f64 + ctx.f30.f64));
	// fmuls f17,f4,f7
	ctx.f17.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// fmuls f18,f4,f6
	ctx.f18.f64 = double(float(ctx.f4.f64 * ctx.f6.f64));
	// fmuls f4,f4,f8
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f8.f64));
	// fmadds f1,f27,f5,f1
	ctx.f1.f64 = double(float(ctx.f27.f64 * ctx.f5.f64 + ctx.f1.f64));
	// fmadds f28,f29,f5,f28
	ctx.f28.f64 = double(float(ctx.f29.f64 * ctx.f5.f64 + ctx.f28.f64));
	// fmsubs f24,f12,f7,f24
	ctx.f24.f64 = double(float(ctx.f12.f64 * ctx.f7.f64 - ctx.f24.f64));
	// fmsubs f26,f9,f6,f26
	ctx.f26.f64 = double(float(ctx.f9.f64 * ctx.f6.f64 - ctx.f26.f64));
	// fmadds f7,f10,f7,f20
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f7.f64 + ctx.f20.f64));
	// fmsubs f8,f10,f8,f22
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f8.f64 - ctx.f22.f64));
	// fmadds f30,f29,f13,f30
	ctx.f30.f64 = double(float(ctx.f29.f64 * ctx.f13.f64 + ctx.f30.f64));
	// fmsubs f22,f2,f5,f19
	ctx.f22.f64 = double(float(ctx.f2.f64 * ctx.f5.f64 - ctx.f19.f64));
	// fmuls f24,f24,f5
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f5.f64));
	// fmuls f26,f26,f5
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f5.f64));
	// fmadds f7,f12,f6,f7
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f6.f64 + ctx.f7.f64));
	// fmuls f8,f8,f5
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// fmadds f6,f2,f13,f1
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f13.f64 + ctx.f1.f64));
	// fmadds f5,f27,f0,f28
	ctx.f5.f64 = double(float(ctx.f27.f64 * ctx.f0.f64 + ctx.f28.f64));
	// fnmsubs f1,f27,f11,f30
	ctx.f1.f64 = double(float(-(ctx.f27.f64 * ctx.f11.f64 - ctx.f30.f64)));
	// stfs f1,80(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fnmsubs f2,f27,f13,f22
	ctx.f2.f64 = double(float(-(ctx.f27.f64 * ctx.f13.f64 - ctx.f22.f64)));
	// fadds f4,f4,f24
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f24.f64));
	// fadds f1,f17,f26
	ctx.f1.f64 = double(float(ctx.f17.f64 + ctx.f26.f64));
	// fmuls f10,f10,f7
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f7.f64));
	// fmuls f12,f7,f12
	ctx.f12.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// fmuls f9,f9,f7
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// fadds f8,f18,f8
	ctx.f8.f64 = double(float(ctx.f18.f64 + ctx.f8.f64));
	// fnmsubs f7,f29,f0,f6
	ctx.f7.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f6.f64)));
	// stfs f7,84(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f6,f3,f13,f5
	ctx.f6.f64 = double(float(-(ctx.f3.f64 * ctx.f13.f64 - ctx.f5.f64)));
	// stfs f6,88(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fnmsubs f5,f29,f11,f2
	ctx.f5.f64 = double(float(-(ctx.f29.f64 * ctx.f11.f64 - ctx.f2.f64)));
	// stfs f5,92(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fadds f2,f1,f10
	ctx.f2.f64 = double(float(ctx.f1.f64 + ctx.f10.f64));
	// fadds f1,f4,f9
	ctx.f1.f64 = double(float(ctx.f4.f64 + ctx.f9.f64));
	// fadds f3,f8,f12
	ctx.f3.f64 = double(float(ctx.f8.f64 + ctx.f12.f64));
	// fmuls f13,f2,f31
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f12,f1,f31
	ctx.f12.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// fmuls f0,f3,f31
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// fadds f10,f23,f13
	ctx.f10.f64 = double(float(ctx.f23.f64 + ctx.f13.f64));
	// stfs f10,100(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f9,f21,f12
	ctx.f9.f64 = double(float(ctx.f21.f64 + ctx.f12.f64));
	// stfs f9,104(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fadds f11,f0,f25
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f25.f64));
	// stfs f11,96(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r7,8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// lwz r6,12(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r5,96(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// stw r7,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r7.u32);
	// stw r6,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r6.u32);
	// stw r5,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r5.u32);
loc_830C2370:
	// lwz r3,104(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// addi r5,r1,192
	ctx.r5.s64 = ctx.r1.s64 + 192;
	// lwz r4,100(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r11,152(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 152);
	// stw r3,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r3.u32);
	// lwz r3,336(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 336);
	// stw r4,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r4.u32);
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r11.u32);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,76(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 76);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830C23A4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// stw r3,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r3.u32);
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// li r9,7
	ctx.r9.s64 = 7;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_830C23B8:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c23b8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C23B8;
	// lwz r11,268(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 268);
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// lwz r31,348(r29)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// lwz r3,1412(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1412);
	// bl 0x8315fb80
	ctx.lr = 0x830C23E0;
	sub_8315FB80(ctx, base);
	// stwx r3,r28,r31
	PPC_STORE_U32(ctx.r28.u32 + ctx.r31.u32, ctx.r3.u32);
	// lwz r11,348(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// add r11,r28,r11
	ctx.r11.u64 = ctx.r28.u64 + ctx.r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830c244c
	if (ctx.cr6.eq) goto loc_830C244C;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c244c
	if (ctx.cr6.eq) goto loc_830C244C;
	// lwz r27,20(r11)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// lwz r31,16(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// cmplw cr6,r31,r27
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r27.u32, ctx.xer);
	// beq cr6,0x830c244c
	if (ctx.cr6.eq) goto loc_830C244C;
loc_830C2414:
	// lwz r30,0(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C242C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x830c2440
	if (ctx.cr6.eq) goto loc_830C2440;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x830bdd70
	ctx.lr = 0x830C2440;
	sub_830BDD70(ctx, base);
loc_830C2440:
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// cmplw cr6,r31,r27
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r27.u32, ctx.xer);
	// bne cr6,0x830c2414
	if (!ctx.cr6.eq) goto loc_830C2414;
loc_830C244C:
	// cmplwi cr6,r25,0
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, 0, ctx.xer);
	// beq cr6,0x830c2474
	if (ctx.cr6.eq) goto loc_830C2474;
	// lwz r11,348(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// add r11,r28,r11
	ctx.r11.u64 = ctx.r28.u64 + ctx.r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x830c2474
	if (!ctx.cr6.eq) goto loc_830C2474;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// rlwinm r9,r10,0,2,0
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFBFFFFFFF;
	// stw r9,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r9.u32);
loc_830C2474:
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// addi r12,r1,-64
	ctx.r12.s64 = ctx.r1.s64 + -64;
	// bl 0x82cb6b04
	ctx.lr = 0x830C2480;
	__restfpr_16(ctx, base);
	// b 0x82cb112c
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C2484"))) PPC_WEAK_FUNC(sub_830C2484);
PPC_FUNC_IMPL(__imp__sub_830C2484) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C2488"))) PPC_WEAK_FUNC(sub_830C2488);
PPC_FUNC_IMPL(__imp__sub_830C2488) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// addi r10,r11,4844
	ctx.r10.s64 = ctx.r11.s64 + 4844;
	// stw r10,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r10.u32);
	// bl 0x831f2f08
	ctx.lr = 0x830C24B4;
	sub_831F2F08(ctx, base);
	// clrlwi r9,r30,31
	ctx.r9.u64 = ctx.r30.u32 & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x830c24dc
	if (ctx.cr6.eq) goto loc_830C24DC;
	// lis r11,-31901
	ctx.r11.s64 = -2090663936;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lwz r3,-32308(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -32308);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,20(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830C24DC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830C24DC:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C24F8"))) PPC_WEAK_FUNC(sub_830C24F8);
PPC_FUNC_IMPL(__imp__sub_830C24F8) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,264(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 264);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x830c250c
	if (ctx.cr6.eq) goto loc_830C250C;
	// addi r8,r11,4
	ctx.r8.s64 = ctx.r11.s64 + 4;
	// b 0x830c2524
	goto loc_830C2524;
loc_830C250C:
	// lwz r11,4(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x830c2520
	if (ctx.cr6.eq) goto loc_830C2520;
	// lwz r8,332(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 332);
	// b 0x830c2524
	goto loc_830C2524;
loc_830C2520:
	// lwz r8,332(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 332);
loc_830C2524:
	// lwz r7,20(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 20);
	// lwz r9,16(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	// cmplw cr6,r9,r7
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, ctx.xer);
	// beq cr6,0x830c258c
	if (ctx.cr6.eq) goto loc_830C258C;
loc_830C2534:
	// lwz r11,0(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmplw cr6,r10,r8
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, ctx.xer);
	// bne cr6,0x830c2554
	if (!ctx.cr6.eq) goto loc_830C2554;
	// lwz r6,12(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r5,108(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 108);
	// cmplw cr6,r6,r5
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, ctx.r5.u32, ctx.xer);
	// beq cr6,0x830c256c
	if (ctx.cr6.eq) goto loc_830C256C;
loc_830C2554:
	// lwz r6,108(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 108);
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// bne cr6,0x830c2580
	if (!ctx.cr6.eq) goto loc_830C2580;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// cmplw cr6,r10,r8
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, ctx.xer);
	// bne cr6,0x830c2580
	if (!ctx.cr6.eq) goto loc_830C2580;
loc_830C256C:
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x830c2594
	if (ctx.cr6.eq) goto loc_830C2594;
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// beq cr6,0x830c2594
	if (ctx.cr6.eq) goto loc_830C2594;
loc_830C2580:
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmplw cr6,r9,r7
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, ctx.xer);
	// bne cr6,0x830c2534
	if (!ctx.cr6.eq) goto loc_830C2534;
loc_830C258C:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_830C2594:
	// lwz r3,0(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C259C"))) PPC_WEAK_FUNC(sub_830C259C);
PPC_FUNC_IMPL(__imp__sub_830C259C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C25A0"))) PPC_WEAK_FUNC(sub_830C25A0);
PPC_FUNC_IMPL(__imp__sub_830C25A0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x830c25d8
	if (ctx.cr6.eq) goto loc_830C25D8;
	// lis r11,-31901
	ctx.r11.s64 = -2090663936;
	// lwz r3,-32308(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -32308);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,20(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830C25D8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830C25D8:
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r11.u32);
	// stw r11,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r11.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C25FC"))) PPC_WEAK_FUNC(sub_830C25FC);
PPC_FUNC_IMPL(__imp__sub_830C25FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C2600"))) PPC_WEAK_FUNC(sub_830C2600);
PPC_FUNC_IMPL(__imp__sub_830C2600) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e8
	ctx.lr = 0x830C2608;
	__savegprlr_28(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c2628
	if (ctx.cr6.eq) goto loc_830C2628;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r11.s64;
	// srawi r11,r9,2
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r11.s64 = ctx.r9.s32 >> 2;
loc_830C2628:
	// cmplw cr6,r11,r4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r4.u32, ctx.xer);
	// bge cr6,0x830c26cc
	if (!ctx.cr6.lt) goto loc_830C26CC;
	// lis r29,-31901
	ctx.r29.s64 = -2090663936;
	// rlwinm r28,r4,2,0,29
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// li r5,282
	ctx.r5.s64 = 282;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lwz r3,-32308(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C2654;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c2684
	if (ctx.cr6.eq) goto loc_830C2684;
loc_830C266C:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x830c266c
	if (!ctx.cr6.eq) goto loc_830C266C;
loc_830C2684:
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x830c26a4
	if (ctx.cr6.eq) goto loc_830C26A4;
	// lwz r3,-32308(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C26A4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830C26A4:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// add r10,r28,r30
	ctx.r10.u64 = ctx.r28.u64 + ctx.r30.u64;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// subf r8,r9,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r9.s64;
	// stw r30,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r30.u32);
	// srawi r7,r8,2
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r11,r30
	ctx.r6.u64 = ctx.r11.u64 + ctx.r30.u64;
	// stw r6,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r6.u32);
loc_830C26CC:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82cb1138
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C26D4"))) PPC_WEAK_FUNC(sub_830C26D4);
PPC_FUNC_IMPL(__imp__sub_830C26D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C26D8"))) PPC_WEAK_FUNC(sub_830C26D8);
PPC_FUNC_IMPL(__imp__sub_830C26D8) {
	PPC_FUNC_PROLOGUE();
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmplw cr6,r5,r9
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, ctx.r9.u32, ctx.xer);
	// bge cr6,0x830c271c
	if (!ctx.cr6.lt) goto loc_830C271C;
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
	// subf r10,r5,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r5.s64;
loc_830C26EC:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// add r7,r10,r11
	ctx.r7.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stwx r8,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + ctx.r11.u32, ctx.r8.u32);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r6,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, ctx.r6.u32);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r8,8(r7)
	PPC_STORE_U32(ctx.r7.u32 + 8, ctx.r8.u32);
	// lwz r6,12(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// stw r6,12(r7)
	PPC_STORE_U32(ctx.r7.u32 + 12, ctx.r6.u32);
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// bne cr6,0x830c26ec
	if (!ctx.cr6.eq) goto loc_830C26EC;
loc_830C271C:
	// subf r11,r4,r5
	ctx.r11.s64 = ctx.r5.s64 - ctx.r4.s64;
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// srawi r9,r11,4
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 4;
	// rlwinm r8,r9,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r7,r8,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r8.s64;
	// stw r7,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r7.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C2738"))) PPC_WEAK_FUNC(sub_830C2738);
PPC_FUNC_IMPL(__imp__sub_830C2738) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e8
	ctx.lr = 0x830C2740;
	__savegprlr_28(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c2760
	if (ctx.cr6.eq) goto loc_830C2760;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r11.s64;
	// srawi r11,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r11.s64 = ctx.r9.s32 >> 4;
loc_830C2760:
	// cmplw cr6,r11,r4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r4.u32, ctx.xer);
	// bge cr6,0x830c281c
	if (!ctx.cr6.lt) goto loc_830C281C;
	// lis r29,-31901
	ctx.r29.s64 = -2090663936;
	// rlwinm r28,r4,4,0,27
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// li r5,282
	ctx.r5.s64 = 282;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lwz r3,-32308(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C278C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c27d4
	if (ctx.cr6.eq) goto loc_830C27D4;
loc_830C27A4:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r7,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r7.u32);
	// lwz r6,8(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r6,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r6.u32);
	// lwz r5,12(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// stw r5,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r5.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// bne cr6,0x830c27a4
	if (!ctx.cr6.eq) goto loc_830C27A4;
loc_830C27D4:
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x830c27f4
	if (ctx.cr6.eq) goto loc_830C27F4;
	// lwz r3,-32308(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C27F4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830C27F4:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// add r10,r28,r30
	ctx.r10.u64 = ctx.r28.u64 + ctx.r30.u64;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// subf r8,r9,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r9.s64;
	// stw r30,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r30.u32);
	// srawi r7,r8,4
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 4;
	// rlwinm r11,r7,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// add r6,r11,r30
	ctx.r6.u64 = ctx.r11.u64 + ctx.r30.u64;
	// stw r6,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r6.u32);
loc_830C281C:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82cb1138
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C2824"))) PPC_WEAK_FUNC(sub_830C2824);
PPC_FUNC_IMPL(__imp__sub_830C2824) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C2828"))) PPC_WEAK_FUNC(sub_830C2828);
PPC_FUNC_IMPL(__imp__sub_830C2828) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lwz r5,100(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 100);
	// lwz r4,104(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 104);
	// bl 0x830bd6d0
	ctx.lr = 0x830C2850;
	sub_830BD6D0(ctx, base);
	// lfs f0,96(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f13,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f13.f64 = double(temp.f32);
	// lwz r3,72(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	// lfs f12,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// lfs f10,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f10.f64 = double(temp.f32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// lfs f9,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f9.f64 = double(temp.f32);
	// fadds f8,f10,f12
	ctx.f8.f64 = double(float(ctx.f10.f64 + ctx.f12.f64));
	// lfs f7,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f7.f64 = double(temp.f32);
	// fadds f6,f7,f9
	ctx.f6.f64 = double(float(ctx.f7.f64 + ctx.f9.f64));
	// lfs f31,6380(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6380);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f5,f11,f31
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// stfs f5,80(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmuls f4,f8,f31
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// stfs f4,84(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmuls f3,f6,f31
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// stfs f3,88(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// beq cr6,0x830c28b4
	if (ctx.cr6.eq) goto loc_830C28B4;
	// lwz r11,88(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// li r4,0
	ctx.r4.s64 = 0;
	// ld r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// rldicr r6,r11,32,63
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// bl 0x8315f360
	ctx.lr = 0x830C28B4;
	sub_8315F360(ctx, base);
loc_830C28B4:
	// lfs f0,108(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f0.f64 = double(temp.f32);
	// lwz r3,72(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	// lfs f13,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f11,f0,f13
	ctx.f11.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// lfs f10,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f10.f64 = double(temp.f32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// lfs f9,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f9.f64 = double(temp.f32);
	// fsubs f8,f12,f10
	ctx.f8.f64 = double(float(ctx.f12.f64 - ctx.f10.f64));
	// lfs f7,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f7.f64 = double(temp.f32);
	// fsubs f6,f9,f7
	ctx.f6.f64 = double(float(ctx.f9.f64 - ctx.f7.f64));
	// fmuls f5,f11,f31
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// stfs f5,80(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmuls f4,f8,f31
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// stfs f4,84(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmuls f3,f6,f31
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// stfs f3,88(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// beq cr6,0x830c2910
	if (ctx.cr6.eq) goto loc_830C2910;
	// lwz r11,88(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// li r4,1
	ctx.r4.s64 = 1;
	// ld r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// rldicr r6,r11,32,63
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// bl 0x8315f360
	ctx.lr = 0x830C2910;
	sub_8315F360(ctx, base);
loc_830C2910:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C2928"))) PPC_WEAK_FUNC(sub_830C2928);
PPC_FUNC_IMPL(__imp__sub_830C2928) {
	PPC_FUNC_PROLOGUE();
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r11.u32);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C293C"))) PPC_WEAK_FUNC(sub_830C293C);
PPC_FUNC_IMPL(__imp__sub_830C293C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C2940"))) PPC_WEAK_FUNC(sub_830C2940);
PPC_FUNC_IMPL(__imp__sub_830C2940) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x830c2978
	if (ctx.cr6.eq) goto loc_830C2978;
	// lis r11,-31901
	ctx.r11.s64 = -2090663936;
	// lwz r3,-32308(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -32308);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,20(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830C2978;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830C2978:
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r11.u32);
	// stw r11,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r11.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C299C"))) PPC_WEAK_FUNC(sub_830C299C);
PPC_FUNC_IMPL(__imp__sub_830C299C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C29A0"))) PPC_WEAK_FUNC(sub_830C29A0);
PPC_FUNC_IMPL(__imp__sub_830C29A0) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C29A4"))) PPC_WEAK_FUNC(sub_830C29A4);
PPC_FUNC_IMPL(__imp__sub_830C29A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C29A8"))) PPC_WEAK_FUNC(sub_830C29A8);
PPC_FUNC_IMPL(__imp__sub_830C29A8) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C29AC"))) PPC_WEAK_FUNC(sub_830C29AC);
PPC_FUNC_IMPL(__imp__sub_830C29AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C29B0"))) PPC_WEAK_FUNC(sub_830C29B0);
PPC_FUNC_IMPL(__imp__sub_830C29B0) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C29B4"))) PPC_WEAK_FUNC(sub_830C29B4);
PPC_FUNC_IMPL(__imp__sub_830C29B4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C29B8"))) PPC_WEAK_FUNC(sub_830C29B8);
PPC_FUNC_IMPL(__imp__sub_830C29B8) {
	PPC_FUNC_PROLOGUE();
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C29C0"))) PPC_WEAK_FUNC(sub_830C29C0);
PPC_FUNC_IMPL(__imp__sub_830C29C0) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,340(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C29C8"))) PPC_WEAK_FUNC(sub_830C29C8);
PPC_FUNC_IMPL(__imp__sub_830C29C8) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,344(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 344);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C29D0"))) PPC_WEAK_FUNC(sub_830C29D0);
PPC_FUNC_IMPL(__imp__sub_830C29D0) {
	PPC_FUNC_PROLOGUE();
	// stw r4,344(r3)
	PPC_STORE_U32(ctx.r3.u32 + 344, ctx.r4.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C29D8"))) PPC_WEAK_FUNC(sub_830C29D8);
PPC_FUNC_IMPL(__imp__sub_830C29D8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e4
	ctx.lr = 0x830C29E0;
	__savegprlr_27(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmplw cr6,r9,r10
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, ctx.xer);
	// bgt cr6,0x830c2acc
	if (ctx.cr6.gt) goto loc_830C2ACC;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// srawi r10,r10,2
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
	// addi r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r8,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// bne cr6,0x830c2a20
	if (!ctx.cr6.eq) goto loc_830C2A20;
	// li r11,0
	ctx.r11.s64 = 0;
	// b 0x830c2a28
	goto loc_830C2A28;
loc_830C2A20:
	// subf r11,r11,r9
	ctx.r11.s64 = ctx.r9.s64 - ctx.r11.s64;
	// srawi r11,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 2;
loc_830C2A28:
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// bge cr6,0x830c2acc
	if (!ctx.cr6.lt) goto loc_830C2ACC;
	// lis r29,-31901
	ctx.r29.s64 = -2090663936;
	// rlwinm r28,r10,2,0,29
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// li r5,282
	ctx.r5.s64 = 282;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lwz r3,-32308(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C2A54;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c2a84
	if (ctx.cr6.eq) goto loc_830C2A84;
loc_830C2A6C:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x830c2a6c
	if (!ctx.cr6.eq) goto loc_830C2A6C;
loc_830C2A84:
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x830c2aa4
	if (ctx.cr6.eq) goto loc_830C2AA4;
	// lwz r3,-32308(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C2AA4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830C2AA4:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// add r10,r28,r30
	ctx.r10.u64 = ctx.r28.u64 + ctx.r30.u64;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// subf r8,r9,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r9.s64;
	// stw r30,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r30.u32);
	// srawi r7,r8,2
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r11,r30
	ctx.r6.u64 = ctx.r11.u64 + ctx.r30.u64;
	// stw r6,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r6.u32);
loc_830C2ACC:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// addi r9,r11,4
	ctx.r9.s64 = ctx.r11.s64 + 4;
	// stw r9,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r9.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82cb1134
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C2AEC"))) PPC_WEAK_FUNC(sub_830C2AEC);
PPC_FUNC_IMPL(__imp__sub_830C2AEC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C2AF0"))) PPC_WEAK_FUNC(sub_830C2AF0);
PPC_FUNC_IMPL(__imp__sub_830C2AF0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10dc
	ctx.lr = 0x830C2AF8;
	__savegprlr_25(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r5
	ctx.r26.u64 = ctx.r5.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r25,r6
	ctx.r25.u64 = ctx.r6.u64;
	// cmplwi cr6,r26,0
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 0, ctx.xer);
	// beq cr6,0x830c2cc0
	if (ctx.cr6.eq) goto loc_830C2CC0;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x830c2b24
	if (!ctx.cr6.eq) goto loc_830C2B24;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x830c2b30
	goto loc_830C2B30;
loc_830C2B24:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r11.s64;
	// srawi r9,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 4;
loc_830C2B30:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// subf r8,r11,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r11.s64;
	// srawi r10,r8,4
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r8.s32 >> 4;
	// add r10,r10,r26
	ctx.r10.u64 = ctx.r10.u64 + ctx.r26.u64;
	// cmplw cr6,r9,r10
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, ctx.xer);
	// bge cr6,0x830c2c30
	if (!ctx.cr6.lt) goto loc_830C2C30;
	// subf r9,r11,r4
	ctx.r9.s64 = ctx.r4.s64 - ctx.r11.s64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r27,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r27.s64 = ctx.r9.s32 >> 4;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c2b68
	if (ctx.cr6.eq) goto loc_830C2B68;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf r8,r11,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r11.s64;
	// srawi r11,r8,4
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r11.s64 = ctx.r8.s32 >> 4;
loc_830C2B68:
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// bge cr6,0x830c2c24
	if (!ctx.cr6.lt) goto loc_830C2C24;
	// lis r29,-31901
	ctx.r29.s64 = -2090663936;
	// rlwinm r28,r10,4,0,27
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// li r5,282
	ctx.r5.s64 = 282;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lwz r3,-32308(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C2B94;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c2bdc
	if (ctx.cr6.eq) goto loc_830C2BDC;
loc_830C2BAC:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r7,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r7.u32);
	// lwz r6,8(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r6,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r6.u32);
	// lwz r5,12(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// stw r5,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r5.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// bne cr6,0x830c2bac
	if (!ctx.cr6.eq) goto loc_830C2BAC;
loc_830C2BDC:
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x830c2bfc
	if (ctx.cr6.eq) goto loc_830C2BFC;
	// lwz r3,-32308(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C2BFC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830C2BFC:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// add r10,r28,r30
	ctx.r10.u64 = ctx.r28.u64 + ctx.r30.u64;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// subf r8,r9,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r9.s64;
	// stw r30,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r30.u32);
	// srawi r7,r8,4
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 4;
	// rlwinm r11,r7,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// add r6,r11,r30
	ctx.r6.u64 = ctx.r11.u64 + ctx.r30.u64;
	// stw r6,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r6.u32);
loc_830C2C24:
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// rlwinm r11,r27,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r27.u32 | (ctx.r27.u64 << 32), 4) & 0xFFFFFFF0;
	// add r4,r11,r10
	ctx.r4.u64 = ctx.r11.u64 + ctx.r10.u64;
loc_830C2C30:
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// rlwinm r8,r26,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// add r10,r8,r4
	ctx.r10.u64 = ctx.r8.u64 + ctx.r4.u64;
	// cmplw cr6,r4,r9
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c2c78
	if (ctx.cr6.eq) goto loc_830C2C78;
loc_830C2C48:
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r6,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r6.u32);
	// lwz r5,8(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r5,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r5.u32);
	// lwz r3,12(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// stw r3,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r3.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// bne cr6,0x830c2c48
	if (!ctx.cr6.eq) goto loc_830C2C48;
loc_830C2C78:
	// mr r10,r26
	ctx.r10.u64 = ctx.r26.u64;
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// cmplwi cr6,r26,0
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 0, ctx.xer);
	// beq cr6,0x830c2cb4
	if (ctx.cr6.eq) goto loc_830C2CB4;
loc_830C2C88:
	// lwz r9,0(r25)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r25.u32 + 0);
	// addic. r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// lwz r7,4(r25)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r25.u32 + 4);
	// stw r7,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r7.u32);
	// lwz r6,8(r25)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r25.u32 + 8);
	// stw r6,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r6.u32);
	// lwz r5,12(r25)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r25.u32 + 12);
	// stw r5,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r5.u32);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// bne 0x830c2c88
	if (!ctx.cr0.eq) goto loc_830C2C88;
loc_830C2CB4:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// add r11,r8,r11
	ctx.r11.u64 = ctx.r8.u64 + ctx.r11.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r11.u32);
loc_830C2CC0:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82cb112c
	__restgprlr_25(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C2CC8"))) PPC_WEAK_FUNC(sub_830C2CC8);
PPC_FUNC_IMPL(__imp__sub_830C2CC8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e4
	ctx.lr = 0x830C2CD0;
	__savegprlr_27(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r9,1540(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1540);
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// lwz r10,1536(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1536);
	// addi r31,r3,1532
	ctx.r31.s64 = ctx.r3.s64 + 1532;
	// cmplw cr6,r9,r10
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, ctx.xer);
	// bgt cr6,0x830c2dbc
	if (ctx.cr6.gt) goto loc_830C2DBC;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// srawi r10,r10,2
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
	// addi r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r8,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// bne cr6,0x830c2d10
	if (!ctx.cr6.eq) goto loc_830C2D10;
	// li r11,0
	ctx.r11.s64 = 0;
	// b 0x830c2d18
	goto loc_830C2D18;
loc_830C2D10:
	// subf r11,r11,r9
	ctx.r11.s64 = ctx.r9.s64 - ctx.r11.s64;
	// srawi r11,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 2;
loc_830C2D18:
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// bge cr6,0x830c2dbc
	if (!ctx.cr6.lt) goto loc_830C2DBC;
	// lis r29,-31901
	ctx.r29.s64 = -2090663936;
	// rlwinm r28,r10,2,0,29
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// li r5,282
	ctx.r5.s64 = 282;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lwz r3,-32308(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C2D44;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c2d74
	if (ctx.cr6.eq) goto loc_830C2D74;
loc_830C2D5C:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x830c2d5c
	if (!ctx.cr6.eq) goto loc_830C2D5C;
loc_830C2D74:
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x830c2d94
	if (ctx.cr6.eq) goto loc_830C2D94;
	// lwz r3,-32308(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C2D94;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830C2D94:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// add r10,r28,r30
	ctx.r10.u64 = ctx.r28.u64 + ctx.r30.u64;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// subf r8,r9,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r9.s64;
	// stw r30,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r30.u32);
	// srawi r7,r8,2
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r11,r30
	ctx.r6.u64 = ctx.r11.u64 + ctx.r30.u64;
	// stw r6,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r6.u32);
loc_830C2DBC:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// stw r27,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r27.u32);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// addi r10,r11,4
	ctx.r10.s64 = ctx.r11.s64 + 4;
	// stw r10,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r10.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82cb1134
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C2DD8"))) PPC_WEAK_FUNC(sub_830C2DD8);
PPC_FUNC_IMPL(__imp__sub_830C2DD8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e0
	ctx.lr = 0x830C2DE0;
	__savegprlr_26(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// clrlwi r11,r5,24
	ctx.r11.u64 = ctx.r5.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// rlwinm r11,r4,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r10,348(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 348);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// bne cr6,0x830c2e10
	if (!ctx.cr6.eq) goto loc_830C2E10;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stw r9,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r9.u32);
	// b 0x830c2e18
	goto loc_830C2E18;
loc_830C2E10:
	// rlwinm r8,r9,0,2,0
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFBFFFFFFF;
	// stw r8,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r8.u32);
loc_830C2E18:
	// lwz r10,344(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 344);
	// cmpwi cr6,r10,2
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 2, ctx.xer);
	// bne cr6,0x830c2f4c
	if (!ctx.cr6.eq) goto loc_830C2F4C;
	// lwz r10,348(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 348);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x830c2f4c
	if (!ctx.cr6.eq) goto loc_830C2F4C;
	// lbz r10,368(r27)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r27.u32 + 368);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x830c2f4c
	if (!ctx.cr6.eq) goto loc_830C2F4C;
	// li r26,1
	ctx.r26.s64 = 1;
	// stb r26,12(r11)
	PPC_STORE_U8(ctx.r11.u32 + 12, ctx.r26.u8);
	// lwz r11,268(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 268);
	// addi r31,r11,1532
	ctx.r31.s64 = ctx.r11.s64 + 1532;
	// lwz r9,1540(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1540);
	// lwz r11,1536(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1536);
	// cmplw cr6,r9,r11
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x830c2f34
	if (ctx.cr6.gt) goto loc_830C2F34;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// srawi r11,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 2;
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// rlwinm r11,r8,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// bne cr6,0x830c2e88
	if (!ctx.cr6.eq) goto loc_830C2E88;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x830c2e90
	goto loc_830C2E90;
loc_830C2E88:
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// srawi r10,r10,2
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
loc_830C2E90:
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x830c2f34
	if (!ctx.cr6.lt) goto loc_830C2F34;
	// lis r29,-31901
	ctx.r29.s64 = -2090663936;
	// rlwinm r28,r11,2,0,29
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// li r5,282
	ctx.r5.s64 = 282;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lwz r3,-32308(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C2EBC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c2eec
	if (ctx.cr6.eq) goto loc_830C2EEC;
loc_830C2ED4:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x830c2ed4
	if (!ctx.cr6.eq) goto loc_830C2ED4;
loc_830C2EEC:
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x830c2f0c
	if (ctx.cr6.eq) goto loc_830C2F0C;
	// lwz r3,-32308(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C2F0C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830C2F0C:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// add r10,r28,r30
	ctx.r10.u64 = ctx.r28.u64 + ctx.r30.u64;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// subf r8,r9,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r9.s64;
	// stw r30,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r30.u32);
	// srawi r7,r8,2
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r11,r30
	ctx.r6.u64 = ctx.r11.u64 + ctx.r30.u64;
	// stw r6,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r6.u32);
loc_830C2F34:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// stw r27,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r27.u32);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// addi r10,r11,4
	ctx.r10.s64 = ctx.r11.s64 + 4;
	// stw r10,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r10.u32);
	// stb r26,368(r27)
	PPC_STORE_U8(ctx.r27.u32 + 368, ctx.r26.u8);
loc_830C2F4C:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82cb1130
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C2F54"))) PPC_WEAK_FUNC(sub_830C2F54);
PPC_FUNC_IMPL(__imp__sub_830C2F54) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C2F58"))) PPC_WEAK_FUNC(sub_830C2F58);
PPC_FUNC_IMPL(__imp__sub_830C2F58) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10ec
	ctx.lr = 0x830C2F60;
	__savegprlr_29(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r29,20(r3)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r31,16(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// cmplw cr6,r31,r29
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r29.u32, ctx.xer);
	// beq cr6,0x830c2fac
	if (ctx.cr6.eq) goto loc_830C2FAC;
loc_830C2F74:
	// lwz r30,0(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C2F8C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x830c2fa0
	if (ctx.cr6.eq) goto loc_830C2FA0;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x830bdd70
	ctx.lr = 0x830C2FA0;
	sub_830BDD70(ctx, base);
loc_830C2FA0:
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// cmplw cr6,r31,r29
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r29.u32, ctx.xer);
	// bne cr6,0x830c2f74
	if (!ctx.cr6.eq) goto loc_830C2F74;
loc_830C2FAC:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82cb113c
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C2FB4"))) PPC_WEAK_FUNC(sub_830C2FB4);
PPC_FUNC_IMPL(__imp__sub_830C2FB4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C2FB8"))) PPC_WEAK_FUNC(sub_830C2FB8);
PPC_FUNC_IMPL(__imp__sub_830C2FB8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// bl 0x830bed00
	ctx.lr = 0x830C2FD8;
	sub_830BED00(ctx, base);
	// clrlwi r11,r30,31
	ctx.r11.u64 = ctx.r30.u32 & 0x1;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c3000
	if (ctx.cr6.eq) goto loc_830C3000;
	// lis r11,-31901
	ctx.r11.s64 = -2090663936;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lwz r3,-32308(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -32308);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,20(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830C3000;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830C3000:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C301C"))) PPC_WEAK_FUNC(sub_830C301C);
PPC_FUNC_IMPL(__imp__sub_830C301C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C3020"))) PPC_WEAK_FUNC(sub_830C3020);
PPC_FUNC_IMPL(__imp__sub_830C3020) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10d8
	ctx.lr = 0x830C3028;
	__savegprlr_24(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r26,r5
	ctx.r26.u64 = ctx.r5.u64;
	// li r24,0
	ctx.r24.s64 = 0;
	// lis r25,-31901
	ctx.r25.s64 = -2090663936;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi r8,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 4;
	// cmplw cr6,r4,r8
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r8.u32, ctx.xer);
	// ble cr6,0x830c3208
	if (!ctx.cr6.gt) goto loc_830C3208;
	// subf. r27,r8,r4
	ctx.r27.s64 = ctx.r4.s64 - ctx.r8.s64;
	ctx.cr0.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// beq 0x830c3228
	if (ctx.cr0.eq) goto loc_830C3228;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x830c3070
	if (!ctx.cr6.eq) goto loc_830C3070;
	// mr r7,r24
	ctx.r7.u64 = ctx.r24.u64;
	// b 0x830c307c
	goto loc_830C307C;
loc_830C3070:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf r7,r10,r11
	ctx.r7.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi r7,r7,4
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xF) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 4;
loc_830C307C:
	// add r11,r8,r27
	ctx.r11.u64 = ctx.r8.u64 + ctx.r27.u64;
	// cmplw cr6,r7,r11
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x830c3174
	if (!ctx.cr6.lt) goto loc_830C3174;
	// subf r9,r10,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r10.s64;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r28,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r28.s64 = ctx.r9.s32 >> 4;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x830c30a4
	if (!ctx.cr6.eq) goto loc_830C30A4;
	// mr r10,r24
	ctx.r10.u64 = ctx.r24.u64;
	// b 0x830c30b0
	goto loc_830C30B0;
loc_830C30A4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf r8,r10,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r10.s64;
	// srawi r10,r8,4
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r8.s32 >> 4;
loc_830C30B0:
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x830c3168
	if (!ctx.cr6.lt) goto loc_830C3168;
	// lwz r3,-32308(r25)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r25.u32 + -32308);
	// rlwinm r29,r11,4,0,27
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// li r5,282
	ctx.r5.s64 = 282;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C30D8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c3120
	if (ctx.cr6.eq) goto loc_830C3120;
loc_830C30F0:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r7,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r7.u32);
	// lwz r6,8(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r6,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r6.u32);
	// lwz r5,12(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// stw r5,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r5.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// bne cr6,0x830c30f0
	if (!ctx.cr6.eq) goto loc_830C30F0;
loc_830C3120:
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x830c3140
	if (ctx.cr6.eq) goto loc_830C3140;
	// lwz r3,-32308(r25)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r25.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C3140;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830C3140:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// add r10,r29,r30
	ctx.r10.u64 = ctx.r29.u64 + ctx.r30.u64;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// subf r8,r9,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r9.s64;
	// stw r30,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r30.u32);
	// srawi r7,r8,4
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 4;
	// rlwinm r11,r7,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// add r6,r11,r30
	ctx.r6.u64 = ctx.r11.u64 + ctx.r30.u64;
	// stw r6,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r6.u32);
loc_830C3168:
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// rlwinm r11,r28,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r28.u32 | (ctx.r28.u64 << 32), 4) & 0xFFFFFFF0;
	// add r9,r11,r10
	ctx.r9.u64 = ctx.r11.u64 + ctx.r10.u64;
loc_830C3174:
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// rlwinm r7,r27,4,0,27
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r27.u32 | (ctx.r27.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
	// add r10,r7,r9
	ctx.r10.u64 = ctx.r7.u64 + ctx.r9.u64;
	// cmplw cr6,r9,r8
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, ctx.xer);
	// beq cr6,0x830c31bc
	if (ctx.cr6.eq) goto loc_830C31BC;
loc_830C318C:
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r6,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r5,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r5.u32);
	// lwz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r4,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r4.u32);
	// lwz r3,12(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// stw r3,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r3.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r11,r8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r8.u32, ctx.xer);
	// bne cr6,0x830c318c
	if (!ctx.cr6.eq) goto loc_830C318C;
loc_830C31BC:
	// mr r10,r27
	ctx.r10.u64 = ctx.r27.u64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// beq cr6,0x830c31f8
	if (ctx.cr6.eq) goto loc_830C31F8;
loc_830C31CC:
	// lwz r9,0(r26)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// addic. r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// lwz r8,4(r26)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4);
	// stw r8,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r8.u32);
	// lwz r6,8(r26)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8);
	// stw r6,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r6.u32);
	// lwz r5,12(r26)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r26.u32 + 12);
	// stw r5,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r5.u32);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// bne 0x830c31cc
	if (!ctx.cr0.eq) goto loc_830C31CC;
loc_830C31F8:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// add r11,r7,r11
	ctx.r11.u64 = ctx.r7.u64 + ctx.r11.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r11.u32);
	// b 0x830c3228
	goto loc_830C3228;
loc_830C3208:
	// bge cr6,0x830c3228
	if (!ctx.cr6.lt) goto loc_830C3228;
	// rlwinm r9,r4,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// subf r10,r10,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi r9,r10,4
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 4;
	// rlwinm r8,r9,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r7,r8,r11
	ctx.r7.s64 = ctx.r11.s64 - ctx.r8.s64;
	// stw r7,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r7.u32);
loc_830C3228:
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmplw cr6,r4,r11
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x830c3260
	if (!ctx.cr6.eq) goto loc_830C3260;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x830c3254
	if (ctx.cr6.eq) goto loc_830C3254;
	// lwz r3,-32308(r25)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r25.u32 + -32308);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C3254;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830C3254:
	// stw r24,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r24.u32);
	// stw r24,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r24.u32);
	// stw r24,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r24.u32);
loc_830C3260:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x830c32a8
	if (!ctx.cr6.gt) goto loc_830C32A8;
	// lwz r3,-32308(r25)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r25.u32 + -32308);
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// subf r11,r4,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r4.s64;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// srawi r30,r11,4
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r11.s32 >> 4;
	// rlwinm r5,r30,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r9,16(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830C3294;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// rlwinm r11,r30,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r3,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r3.u32);
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r11.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
loc_830C32A8:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82cb1128
	__restgprlr_24(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C32B0"))) PPC_WEAK_FUNC(sub_830C32B0);
PPC_FUNC_IMPL(__imp__sub_830C32B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e0
	ctx.lr = 0x830C32B8;
	__savegprlr_26(ctx, base);
	// addi r12,r1,-56
	ctx.r12.s64 = ctx.r1.s64 + -56;
	// bl 0x82cb6ab8
	ctx.lr = 0x830C32C0;
	__savefpr_16(ctx, base);
	// stwu r1,-384(r1)
	ea = -384 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// li r27,0
	ctx.r27.s64 = 0;
	// rlwinm r26,r28,4,0,27
	ctx.r26.u64 = __builtin_rotateleft64(ctx.r28.u32 | (ctx.r28.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r11,348(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// add r11,r26,r11
	ctx.r11.u64 = ctx.r26.u64 + ctx.r11.u64;
	// stb r27,12(r11)
	PPC_STORE_U8(ctx.r11.u32 + 12, ctx.r27.u8);
	// lwz r10,348(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// lwzx r9,r26,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r10.u32);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x830c3304
	if (ctx.cr6.eq) goto loc_830C3304;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,384
	ctx.r1.s64 = ctx.r1.s64 + 384;
	// addi r12,r1,-56
	ctx.r12.s64 = ctx.r1.s64 + -56;
	// bl 0x82cb6b04
	ctx.lr = 0x830C3300;
	__restfpr_16(ctx, base);
	// b 0x82cb1130
	__restgprlr_26(ctx, base);
	return;
loc_830C3304:
	// lfs f0,112(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 112);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f13,116(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 116);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f11,120(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 120);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// lfs f5,124(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 124);
	ctx.f5.f64 = double(temp.f32);
	// lfs f7,184(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 184);
	ctx.f7.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f30,6380(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6380);
	ctx.f30.f64 = double(temp.f32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lfs f8,180(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 180);
	ctx.f8.f64 = double(temp.f32);
	// fmsubs f4,f5,f5,f30
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f5.f64 - ctx.f30.f64));
	// lfs f6,176(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 176);
	ctx.f6.f64 = double(temp.f32);
	// addi r31,r29,112
	ctx.r31.s64 = ctx.r29.s64 + 112;
	// lfs f3,160(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 160);
	ctx.f3.f64 = double(temp.f32);
	// addi r30,r29,160
	ctx.r30.s64 = ctx.r29.s64 + 160;
	// lfs f2,172(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 172);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f3,f11
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// fmuls f29,f2,f0
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// lfs f26,164(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 164);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f27,f7,f12
	ctx.f27.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// lfs f28,168(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 168);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f25,f10,f6
	ctx.f25.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// lfs f31,7676(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 7676);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f22,f9,f7
	ctx.f22.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// lfs f23,128(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 128);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f24,f9,f8
	ctx.f24.f64 = double(float(ctx.f9.f64 * ctx.f8.f64));
	// lfs f21,132(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 132);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f20,f2,f11
	ctx.f20.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// lfs f19,136(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 136);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f17,f6,f4
	ctx.f17.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// addi r8,r1,124
	ctx.r8.s64 = ctx.r1.s64 + 124;
	// fmuls f16,f8,f4
	ctx.f16.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// fmuls f4,f7,f4
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f4.f64));
	// fmuls f18,f3,f0
	ctx.f18.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmadds f1,f26,f5,f1
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f5.f64 + ctx.f1.f64));
	// fmsubs f27,f9,f6,f27
	ctx.f27.f64 = double(float(ctx.f9.f64 * ctx.f6.f64 - ctx.f27.f64));
	// fmsubs f25,f8,f12,f25
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f12.f64 - ctx.f25.f64));
	// fmadds f6,f6,f12,f22
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f12.f64 + ctx.f22.f64));
	// fmsubs f7,f10,f7,f24
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f7.f64 - ctx.f24.f64));
	// fmadds f29,f3,f5,f29
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f5.f64 + ctx.f29.f64));
	// fmadds f24,f28,f5,f20
	ctx.f24.f64 = double(float(ctx.f28.f64 * ctx.f5.f64 + ctx.f20.f64));
	// fmsubs f22,f2,f5,f18
	ctx.f22.f64 = double(float(ctx.f2.f64 * ctx.f5.f64 - ctx.f18.f64));
	// fmadds f2,f2,f13,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f13.f64 + ctx.f1.f64));
	// fmuls f27,f27,f5
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f5.f64));
	// fmuls f25,f25,f5
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f5.f64));
	// fmadds f6,f10,f8,f6
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f8.f64 + ctx.f6.f64));
	// fmuls f7,f7,f5
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// fmadds f5,f28,f13,f29
	ctx.f5.f64 = double(float(ctx.f28.f64 * ctx.f13.f64 + ctx.f29.f64));
	// fmadds f1,f26,f0,f24
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f0.f64 + ctx.f24.f64));
	// fnmsubs f8,f26,f13,f22
	ctx.f8.f64 = double(float(-(ctx.f26.f64 * ctx.f13.f64 - ctx.f22.f64)));
	// fadds f29,f16,f27
	ctx.f29.f64 = double(float(ctx.f16.f64 + ctx.f27.f64));
	// fadds f4,f4,f25
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f25.f64));
	// fmuls f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// fmuls f10,f10,f6
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// fmuls f9,f9,f6
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f6.f64));
	// fadds f7,f17,f7
	ctx.f7.f64 = double(float(ctx.f17.f64 + ctx.f7.f64));
	// fnmsubs f6,f26,f11,f5
	ctx.f6.f64 = double(float(-(ctx.f26.f64 * ctx.f11.f64 - ctx.f5.f64)));
	// stfs f6,80(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fnmsubs f5,f28,f0,f2
	ctx.f5.f64 = double(float(-(ctx.f28.f64 * ctx.f0.f64 - ctx.f2.f64)));
	// stfs f5,84(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f3,f3,f13,f1
	ctx.f3.f64 = double(float(-(ctx.f3.f64 * ctx.f13.f64 - ctx.f1.f64)));
	// stfs f3,88(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fnmsubs f2,f28,f11,f8
	ctx.f2.f64 = double(float(-(ctx.f28.f64 * ctx.f11.f64 - ctx.f8.f64)));
	// stfs f2,92(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fadds f0,f29,f10
	ctx.f0.f64 = double(float(ctx.f29.f64 + ctx.f10.f64));
	// fadds f13,f4,f9
	ctx.f13.f64 = double(float(ctx.f4.f64 + ctx.f9.f64));
	// fadds f1,f7,f12
	ctx.f1.f64 = double(float(ctx.f7.f64 + ctx.f12.f64));
	// fmuls f11,f0,f31
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fmuls f10,f13,f31
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// fmuls f12,f1,f31
	ctx.f12.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// fadds f8,f21,f11
	ctx.f8.f64 = double(float(ctx.f21.f64 + ctx.f11.f64));
	// stfs f8,100(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f7,f19,f10
	ctx.f7.f64 = double(float(ctx.f19.f64 + ctx.f10.f64));
	// stfs f7,104(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fadds f9,f23,f12
	ctx.f9.f64 = double(float(ctx.f23.f64 + ctx.f12.f64));
	// stfs f9,96(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lwz r6,4(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r5,8(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// lwz r4,12(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// lwz r11,96(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r10,100(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r9,104(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stw r7,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r7.u32);
	// stw r6,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r6.u32);
	// stw r5,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r5.u32);
	// stw r4,12(r8)
	PPC_STORE_U32(ctx.r8.u32 + 12, ctx.r4.u32);
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r11.u32);
	// stw r10,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r10.u32);
	// stw r9,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r9.u32);
	// bl 0x8315fb70
	ctx.lr = 0x830C3480;
	sub_8315FB70(ctx, base);
	// lwz r11,264(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 264);
	// li r8,10
	ctx.r8.s64 = 10;
	// stw r8,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r8.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c3754
	if (ctx.cr6.eq) goto loc_830C3754;
	// lfs f0,216(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 216);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r11,216
	ctx.r10.s64 = ctx.r11.s64 + 216;
	// lfs f5,240(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 240);
	ctx.f5.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f13,220(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 220);
	ctx.f13.f64 = double(temp.f32);
	// fneg f3,f5
	ctx.f3.u64 = ctx.f5.u64 ^ 0x8000000000000000;
	// lfs f11,224(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 224);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// lfs f9,232(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 232);
	ctx.f9.f64 = double(temp.f32);
	// fmr f8,f11
	ctx.f8.f64 = ctx.f11.f64;
	// lfs f7,236(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 236);
	ctx.f7.f64 = double(temp.f32);
	// fneg f6,f9
	ctx.f6.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// fneg f4,f7
	ctx.f4.u64 = ctx.f7.u64 ^ 0x8000000000000000;
	// lfs f2,228(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 228);
	ctx.f2.f64 = double(temp.f32);
	// fneg f7,f13
	ctx.f7.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// lfs f9,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f9.f64 = double(temp.f32);
	// fneg f1,f0
	ctx.f1.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lfs f0,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f0.f64 = double(temp.f32);
	// fneg f11,f11
	ctx.f11.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// lfs f5,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f5.f64 = double(temp.f32);
	// fmsubs f28,f2,f2,f30
	ctx.f28.f64 = double(float(ctx.f2.f64 * ctx.f2.f64 - ctx.f30.f64));
	// lfs f29,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f29.f64 = double(temp.f32);
	// fmr f13,f2
	ctx.f13.f64 = ctx.f2.f64;
	// lfs f27,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f24,f3,f12
	ctx.f24.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// fmuls f21,f8,f3
	ctx.f21.f64 = double(float(ctx.f8.f64 * ctx.f3.f64));
	// fmuls f26,f10,f6
	ctx.f26.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// fmuls f25,f8,f4
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// fmuls f20,f9,f7
	ctx.f20.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// fmuls f23,f0,f1
	ctx.f23.f64 = double(float(ctx.f0.f64 * ctx.f1.f64));
	// fmuls f22,f9,f11
	ctx.f22.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// fmuls f19,f5,f7
	ctx.f19.f64 = double(float(ctx.f5.f64 * ctx.f7.f64));
	// fmuls f17,f28,f6
	ctx.f17.f64 = double(float(ctx.f28.f64 * ctx.f6.f64));
	// fmsubs f24,f8,f6,f24
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f6.f64 - ctx.f24.f64));
	// fmsubs f18,f13,f13,f30
	ctx.f18.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 - ctx.f30.f64));
	// fmadds f6,f12,f6,f21
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f6.f64 + ctx.f21.f64));
	// fmsubs f26,f4,f12,f26
	ctx.f26.f64 = double(float(ctx.f4.f64 * ctx.f12.f64 - ctx.f26.f64));
	// fmsubs f25,f10,f3,f25
	ctx.f25.f64 = double(float(ctx.f10.f64 * ctx.f3.f64 - ctx.f25.f64));
	// fmadds f21,f5,f1,f20
	ctx.f21.f64 = double(float(ctx.f5.f64 * ctx.f1.f64 + ctx.f20.f64));
	// fmsubs f23,f5,f11,f23
	ctx.f23.f64 = double(float(ctx.f5.f64 * ctx.f11.f64 - ctx.f23.f64));
	// fmsubs f22,f0,f7,f22
	ctx.f22.f64 = double(float(ctx.f0.f64 * ctx.f7.f64 - ctx.f22.f64));
	// fmsubs f20,f9,f1,f19
	ctx.f20.f64 = double(float(ctx.f9.f64 * ctx.f1.f64 - ctx.f19.f64));
	// fmuls f19,f4,f28
	ctx.f19.f64 = double(float(ctx.f4.f64 * ctx.f28.f64));
	// fmuls f3,f3,f28
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f28.f64));
	// fmuls f28,f0,f18
	ctx.f28.f64 = double(float(ctx.f0.f64 * ctx.f18.f64));
	// fmadds f6,f10,f4,f6
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f4.f64 + ctx.f6.f64));
	// fmuls f26,f26,f2
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f2.f64));
	// fmuls f25,f25,f2
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f2.f64));
	// fmadds f4,f0,f11,f21
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 + ctx.f21.f64));
	// fmuls f2,f24,f2
	ctx.f2.f64 = double(float(ctx.f24.f64 * ctx.f2.f64));
	// fmuls f9,f9,f18
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f18.f64));
	// fmuls f0,f23,f13
	ctx.f0.f64 = double(float(ctx.f23.f64 * ctx.f13.f64));
	// fmuls f5,f5,f18
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f18.f64));
	// fmuls f24,f13,f22
	ctx.f24.f64 = double(float(ctx.f13.f64 * ctx.f22.f64));
	// fmuls f23,f20,f13
	ctx.f23.f64 = double(float(ctx.f20.f64 * ctx.f13.f64));
	// fmuls f8,f8,f6
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f6.f64));
	// fsubs f3,f3,f26
	ctx.f3.f64 = double(float(ctx.f3.f64 - ctx.f26.f64));
	// fmuls f10,f10,f6
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// fsubs f26,f17,f25
	ctx.f26.f64 = double(float(ctx.f17.f64 - ctx.f25.f64));
	// fsubs f2,f19,f2
	ctx.f2.f64 = double(float(ctx.f19.f64 - ctx.f2.f64));
	// fmuls f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// fadds f0,f9,f0
	ctx.f0.f64 = double(float(ctx.f9.f64 + ctx.f0.f64));
	// fmuls f25,f7,f4
	ctx.f25.f64 = double(float(ctx.f7.f64 * ctx.f4.f64));
	// fmuls f6,f4,f1
	ctx.f6.f64 = double(float(ctx.f4.f64 * ctx.f1.f64));
	// fadds f9,f5,f24
	ctx.f9.f64 = double(float(ctx.f5.f64 + ctx.f24.f64));
	// fmuls f4,f11,f4
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// fadds f5,f28,f23
	ctx.f5.f64 = double(float(ctx.f28.f64 + ctx.f23.f64));
	// fadds f8,f3,f8
	ctx.f8.f64 = double(float(ctx.f3.f64 + ctx.f8.f64));
	// fadds f3,f2,f10
	ctx.f3.f64 = double(float(ctx.f2.f64 + ctx.f10.f64));
	// fadds f12,f26,f12
	ctx.f12.f64 = double(float(ctx.f26.f64 + ctx.f12.f64));
	// fadds f2,f0,f25
	ctx.f2.f64 = double(float(ctx.f0.f64 + ctx.f25.f64));
	// fadds f0,f9,f6
	ctx.f0.f64 = double(float(ctx.f9.f64 + ctx.f6.f64));
	// fadds f10,f5,f4
	ctx.f10.f64 = double(float(ctx.f5.f64 + ctx.f4.f64));
	// fmuls f8,f8,f31
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fmuls f6,f3,f31
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// fmuls f9,f12,f31
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// fmuls f5,f2,f31
	ctx.f5.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f2,f29,f13
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f13.f64));
	// fmuls f4,f0,f31
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// lfs f0,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f3,f10,f31
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f31.f64));
	// fmuls f28,f27,f11
	ctx.f28.f64 = double(float(ctx.f27.f64 * ctx.f11.f64));
	// lfs f10,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f12,f27,f13
	ctx.f12.f64 = double(float(ctx.f27.f64 * ctx.f13.f64));
	// lfs f23,16(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f2,f10,f1,f2
	ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f1.f64 + ctx.f2.f64));
	// lfs f24,20(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f25,f27,f1
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f1.f64));
	// lfs f26,24(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 24);
	ctx.f26.f64 = double(temp.f32);
	// fadds f9,f4,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 + ctx.f9.f64));
	// lfs f4,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f4.f64 = double(temp.f32);
	// fadds f6,f5,f6
	ctx.f6.f64 = double(float(ctx.f5.f64 + ctx.f6.f64));
	// lfs f5,12(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f5.f64 = double(temp.f32);
	// fadds f3,f3,f8
	ctx.f3.f64 = double(float(ctx.f3.f64 + ctx.f8.f64));
	// lfs f8,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// lfs f22,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f22.f64 = double(temp.f32);
	// lwz r8,184(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 184);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r9,r1,172
	ctx.r9.s64 = ctx.r1.s64 + 172;
	// fmadds f28,f10,f13,f28
	ctx.f28.f64 = double(float(ctx.f10.f64 * ctx.f13.f64 + ctx.f28.f64));
	// fmadds f12,f0,f1,f12
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f1.f64 + ctx.f12.f64));
	// fmadds f2,f0,f11,f2
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 + ctx.f2.f64));
	// fmsubs f13,f0,f13,f25
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 - ctx.f25.f64));
	// fmadds f0,f0,f7,f28
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f7.f64 + ctx.f28.f64));
	// fmadds f12,f29,f7,f12
	ctx.f12.f64 = double(float(ctx.f29.f64 * ctx.f7.f64 + ctx.f12.f64));
	// fnmsubs f13,f10,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f10.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// fnmsubs f7,f27,f7,f2
	ctx.f7.f64 = double(float(-(ctx.f27.f64 * ctx.f7.f64 - ctx.f2.f64)));
	// fnmsubs f1,f29,f1,f0
	ctx.f1.f64 = double(float(-(ctx.f29.f64 * ctx.f1.f64 - ctx.f0.f64)));
	// fnmsubs f2,f10,f11,f12
	ctx.f2.f64 = double(float(-(ctx.f10.f64 * ctx.f11.f64 - ctx.f12.f64)));
	// fnmsubs f0,f29,f11,f13
	ctx.f0.f64 = double(float(-(ctx.f29.f64 * ctx.f11.f64 - ctx.f13.f64)));
	// fmuls f13,f24,f7
	ctx.f13.f64 = double(float(ctx.f24.f64 * ctx.f7.f64));
	// fmuls f28,f4,f7
	ctx.f28.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// fmuls f27,f5,f7
	ctx.f27.f64 = double(float(ctx.f5.f64 * ctx.f7.f64));
	// fmuls f11,f23,f1
	ctx.f11.f64 = double(float(ctx.f23.f64 * ctx.f1.f64));
	// fmuls f10,f24,f1
	ctx.f10.f64 = double(float(ctx.f24.f64 * ctx.f1.f64));
	// fmuls f12,f26,f2
	ctx.f12.f64 = double(float(ctx.f26.f64 * ctx.f2.f64));
	// fmsubs f30,f0,f0,f30
	ctx.f30.f64 = double(float(ctx.f0.f64 * ctx.f0.f64 - ctx.f30.f64));
	// fmsubs f13,f26,f1,f13
	ctx.f13.f64 = double(float(ctx.f26.f64 * ctx.f1.f64 - ctx.f13.f64));
	// fmuls f29,f4,f0
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fmuls f25,f4,f2
	ctx.f25.f64 = double(float(ctx.f4.f64 * ctx.f2.f64));
	// fmadds f28,f8,f0,f28
	ctx.f28.f64 = double(float(ctx.f8.f64 * ctx.f0.f64 + ctx.f28.f64));
	// fmadds f27,f8,f2,f27
	ctx.f27.f64 = double(float(ctx.f8.f64 * ctx.f2.f64 + ctx.f27.f64));
	// fmsubs f11,f24,f2,f11
	ctx.f11.f64 = double(float(ctx.f24.f64 * ctx.f2.f64 - ctx.f11.f64));
	// fmadds f10,f23,f2,f10
	ctx.f10.f64 = double(float(ctx.f23.f64 * ctx.f2.f64 + ctx.f10.f64));
	// fmsubs f12,f23,f7,f12
	ctx.f12.f64 = double(float(ctx.f23.f64 * ctx.f7.f64 - ctx.f12.f64));
	// fmuls f23,f23,f30
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f30.f64));
	// fmuls f24,f24,f30
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f30.f64));
	// fmuls f30,f26,f30
	ctx.f30.f64 = double(float(ctx.f26.f64 * ctx.f30.f64));
	// fmuls f13,f0,f13
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// fmadds f29,f5,f2,f29
	ctx.f29.f64 = double(float(ctx.f5.f64 * ctx.f2.f64 + ctx.f29.f64));
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmadds f10,f26,f7,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f7.f64 + ctx.f10.f64));
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmsubs f26,f5,f0,f25
	ctx.f26.f64 = double(float(ctx.f5.f64 * ctx.f0.f64 - ctx.f25.f64));
	// fmadds f5,f5,f1,f28
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f1.f64 + ctx.f28.f64));
	// fmadds f0,f22,f0,f27
	ctx.f0.f64 = double(float(ctx.f22.f64 * ctx.f0.f64 + ctx.f27.f64));
	// fadds f13,f23,f13
	ctx.f13.f64 = double(float(ctx.f23.f64 + ctx.f13.f64));
	// fmadds f29,f22,f1,f29
	ctx.f29.f64 = double(float(ctx.f22.f64 * ctx.f1.f64 + ctx.f29.f64));
	// fadds f11,f30,f11
	ctx.f11.f64 = double(float(ctx.f30.f64 + ctx.f11.f64));
	// fmuls f30,f10,f2
	ctx.f30.f64 = double(float(ctx.f10.f64 * ctx.f2.f64));
	// fmuls f28,f1,f10
	ctx.f28.f64 = double(float(ctx.f1.f64 * ctx.f10.f64));
	// fmuls f10,f7,f10
	ctx.f10.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
	// fadds f12,f24,f12
	ctx.f12.f64 = double(float(ctx.f24.f64 + ctx.f12.f64));
	// fnmsubs f27,f8,f1,f26
	ctx.f27.f64 = double(float(-(ctx.f8.f64 * ctx.f1.f64 - ctx.f26.f64)));
	// fnmsubs f5,f22,f2,f5
	ctx.f5.f64 = double(float(-(ctx.f22.f64 * ctx.f2.f64 - ctx.f5.f64)));
	// stfs f5,84(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f4,f4,f1,f0
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f1.f64 - ctx.f0.f64)));
	// fnmsubs f8,f8,f7,f29
	ctx.f8.f64 = double(float(-(ctx.f8.f64 * ctx.f7.f64 - ctx.f29.f64)));
	// stfs f8,80(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fadds f2,f13,f30
	ctx.f2.f64 = double(float(ctx.f13.f64 + ctx.f30.f64));
	// fadds f0,f11,f10
	ctx.f0.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// fadds f1,f12,f28
	ctx.f1.f64 = double(float(ctx.f12.f64 + ctx.f28.f64));
	// fnmsubs f13,f22,f7,f27
	ctx.f13.f64 = double(float(-(ctx.f22.f64 * ctx.f7.f64 - ctx.f27.f64)));
	// fmuls f12,f2,f31
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f10,f0,f31
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fmuls f11,f1,f31
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// fadds f9,f12,f9
	ctx.f9.f64 = double(float(ctx.f12.f64 + ctx.f9.f64));
	// stfs f9,96(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f7,f10,f3
	ctx.f7.f64 = double(float(ctx.f10.f64 + ctx.f3.f64));
	// stfs f7,104(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fadds f8,f11,f6
	ctx.f8.f64 = double(float(ctx.f11.f64 + ctx.f6.f64));
	// stfs f8,100(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stfs f4,88(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lwz r7,96(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stfs f13,92(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lwz r6,8(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lwz r5,4(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r11,12(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r8,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r8.u32);
	// stw r6,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r6.u32);
	// stw r5,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r5.u32);
	// stw r11,12(r9)
	PPC_STORE_U32(ctx.r9.u32 + 12, ctx.r11.u32);
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// stw r7,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r7.u32);
	// b 0x830c38a4
	goto loc_830C38A4;
loc_830C3754:
	// lfs f13,4(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lfs f0,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// lfs f11,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// lfs f3,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// lfs f5,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f1,f13,f3
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f3.f64));
	// lfs f6,16(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// fmsubs f4,f5,f5,f30
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f5.f64 - ctx.f30.f64));
	// lfs f8,24(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 24);
	ctx.f8.f64 = double(temp.f32);
	// addi r10,r1,172
	ctx.r10.s64 = ctx.r1.s64 + 172;
	// lfs f7,20(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	ctx.f7.f64 = double(temp.f32);
	// stw r27,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r27.u32);
	// lfs f2,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// lfs f30,12(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f29,f11,f2
	ctx.f29.f64 = double(float(ctx.f11.f64 * ctx.f2.f64));
	// fmuls f28,f11,f30
	ctx.f28.f64 = double(float(ctx.f11.f64 * ctx.f30.f64));
	// lfs f27,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f24,f6,f10
	ctx.f24.f64 = double(float(ctx.f6.f64 * ctx.f10.f64));
	// lfs f25,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f26,f8,f12
	ctx.f26.f64 = double(float(ctx.f8.f64 * ctx.f12.f64));
	// lfs f23,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f20,f8,f9
	ctx.f20.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// lfs f21,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f22,f7,f9
	ctx.f22.f64 = double(float(ctx.f7.f64 * ctx.f9.f64));
	// fmadds f1,f0,f30,f1
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f30.f64 + ctx.f1.f64));
	// fmuls f17,f7,f4
	ctx.f17.f64 = double(float(ctx.f7.f64 * ctx.f4.f64));
	// fmuls f18,f6,f4
	ctx.f18.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// fmuls f4,f8,f4
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// fmuls f19,f0,f2
	ctx.f19.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// fmadds f29,f13,f30,f29
	ctx.f29.f64 = double(float(ctx.f13.f64 * ctx.f30.f64 + ctx.f29.f64));
	// fmsubs f24,f7,f12,f24
	ctx.f24.f64 = double(float(ctx.f7.f64 * ctx.f12.f64 - ctx.f24.f64));
	// fmsubs f26,f6,f9,f26
	ctx.f26.f64 = double(float(ctx.f6.f64 * ctx.f9.f64 - ctx.f26.f64));
	// fmadds f7,f7,f10,f20
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f10.f64 + ctx.f20.f64));
	// fmsubs f8,f8,f10,f22
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f10.f64 - ctx.f22.f64));
	// fmadds f1,f5,f2,f1
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f2.f64 + ctx.f1.f64));
	// fmadds f28,f0,f27,f28
	ctx.f28.f64 = double(float(ctx.f0.f64 * ctx.f27.f64 + ctx.f28.f64));
	// fmsubs f30,f5,f30,f19
	ctx.f30.f64 = double(float(ctx.f5.f64 * ctx.f30.f64 - ctx.f19.f64));
	// fmuls f24,f24,f5
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f5.f64));
	// fmuls f26,f26,f5
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f5.f64));
	// fmadds f7,f6,f12,f7
	ctx.f7.f64 = double(float(ctx.f6.f64 * ctx.f12.f64 + ctx.f7.f64));
	// fmuls f8,f8,f5
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// fmadds f6,f5,f27,f29
	ctx.f6.f64 = double(float(ctx.f5.f64 * ctx.f27.f64 + ctx.f29.f64));
	// fnmsubs f1,f11,f27,f1
	ctx.f1.f64 = double(float(-(ctx.f11.f64 * ctx.f27.f64 - ctx.f1.f64)));
	// stfs f1,80(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmadds f5,f5,f3,f28
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f3.f64 + ctx.f28.f64));
	// fnmsubs f30,f13,f27,f30
	ctx.f30.f64 = double(float(-(ctx.f13.f64 * ctx.f27.f64 - ctx.f30.f64)));
	// fadds f4,f4,f24
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f24.f64));
	// fadds f1,f17,f26
	ctx.f1.f64 = double(float(ctx.f17.f64 + ctx.f26.f64));
	// fmuls f10,f10,f7
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f7.f64));
	// fmuls f12,f7,f12
	ctx.f12.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// fmuls f9,f9,f7
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// fadds f8,f18,f8
	ctx.f8.f64 = double(float(ctx.f18.f64 + ctx.f8.f64));
	// fnmsubs f7,f0,f3,f6
	ctx.f7.f64 = double(float(-(ctx.f0.f64 * ctx.f3.f64 - ctx.f6.f64)));
	// stfs f7,84(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f6,f13,f2,f5
	ctx.f6.f64 = double(float(-(ctx.f13.f64 * ctx.f2.f64 - ctx.f5.f64)));
	// stfs f6,88(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fnmsubs f5,f11,f3,f30
	ctx.f5.f64 = double(float(-(ctx.f11.f64 * ctx.f3.f64 - ctx.f30.f64)));
	// stfs f5,92(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fadds f2,f1,f10
	ctx.f2.f64 = double(float(ctx.f1.f64 + ctx.f10.f64));
	// fadds f1,f4,f9
	ctx.f1.f64 = double(float(ctx.f4.f64 + ctx.f9.f64));
	// fadds f3,f8,f12
	ctx.f3.f64 = double(float(ctx.f8.f64 + ctx.f12.f64));
	// fmuls f13,f2,f31
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f12,f1,f31
	ctx.f12.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// fmuls f0,f3,f31
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// fadds f10,f23,f13
	ctx.f10.f64 = double(float(ctx.f23.f64 + ctx.f13.f64));
	// stfs f10,100(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f9,f21,f12
	ctx.f9.f64 = double(float(ctx.f21.f64 + ctx.f12.f64));
	// stfs f9,104(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fadds f11,f0,f25
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f25.f64));
	// stfs f11,96(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r7,8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// lwz r5,96(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r6,12(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// stw r7,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r7.u32);
	// stw r6,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r6.u32);
	// stw r5,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r5.u32);
loc_830C38A4:
	// lwz r3,104(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// addi r5,r1,192
	ctx.r5.s64 = ctx.r1.s64 + 192;
	// lwz r4,100(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r11,152(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 152);
	// stw r3,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r3.u32);
	// lwz r3,336(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 336);
	// stw r4,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r4.u32);
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r11.u32);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,76(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 76);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830C38D8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// stw r3,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r3.u32);
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// li r9,7
	ctx.r9.s64 = 7;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_830C38EC:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c38ec
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C38EC;
	// lwz r11,268(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 268);
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// lwz r31,348(r29)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// lwz r3,1412(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1412);
	// bl 0x8315fb80
	ctx.lr = 0x830C3914;
	sub_8315FB80(ctx, base);
	// stwx r3,r26,r31
	PPC_STORE_U32(ctx.r26.u32 + ctx.r31.u32, ctx.r3.u32);
	// lwz r11,348(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// add r11,r26,r11
	ctx.r11.u64 = ctx.r26.u64 + ctx.r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830c3980
	if (ctx.cr6.eq) goto loc_830C3980;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c3980
	if (ctx.cr6.eq) goto loc_830C3980;
	// lwz r28,20(r11)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// lwz r31,16(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// cmplw cr6,r31,r28
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r28.u32, ctx.xer);
	// beq cr6,0x830c3980
	if (ctx.cr6.eq) goto loc_830C3980;
loc_830C3948:
	// lwz r30,0(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C3960;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x830c3974
	if (ctx.cr6.eq) goto loc_830C3974;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x830bdd70
	ctx.lr = 0x830C3974;
	sub_830BDD70(ctx, base);
loc_830C3974:
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// cmplw cr6,r31,r28
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r28.u32, ctx.xer);
	// bne cr6,0x830c3948
	if (!ctx.cr6.eq) goto loc_830C3948;
loc_830C3980:
	// lwz r11,348(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 348);
	// lwzx r10,r26,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r11.u32);
	// cntlzw r9,r10
	ctx.r9.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r8,r9,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// xori r3,r8,1
	ctx.r3.u64 = ctx.r8.u64 ^ 1;
	// addi r1,r1,384
	ctx.r1.s64 = ctx.r1.s64 + 384;
	// addi r12,r1,-56
	ctx.r12.s64 = ctx.r1.s64 + -56;
	// bl 0x82cb6b04
	ctx.lr = 0x830C39A0;
	__restfpr_16(ctx, base);
	// b 0x82cb1130
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C39A4"))) PPC_WEAK_FUNC(sub_830C39A4);
PPC_FUNC_IMPL(__imp__sub_830C39A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C39A8"))) PPC_WEAK_FUNC(sub_830C39A8);
PPC_FUNC_IMPL(__imp__sub_830C39A8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r30,20(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lwz r31,16(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// cmplw cr6,r31,r30
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r30.u32, ctx.xer);
	// beq cr6,0x830c39e4
	if (ctx.cr6.eq) goto loc_830C39E4;
loc_830C39CC:
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// bl 0x830bf750
	ctx.lr = 0x830C39D8;
	sub_830BF750(ctx, base);
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// cmplw cr6,r31,r30
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r30.u32, ctx.xer);
	// bne cr6,0x830c39cc
	if (!ctx.cr6.eq) goto loc_830C39CC;
loc_830C39E4:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C39FC"))) PPC_WEAK_FUNC(sub_830C39FC);
PPC_FUNC_IMPL(__imp__sub_830C39FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C3A00"))) PPC_WEAK_FUNC(sub_830C3A00);
PPC_FUNC_IMPL(__imp__sub_830C3A00) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e8
	ctx.lr = 0x830C3A08;
	__savegprlr_28(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// rlwinm r29,r4,4,0,27
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r11,348(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 348);
	// add r11,r29,r11
	ctx.r11.u64 = ctx.r29.u64 + ctx.r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830c3a74
	if (ctx.cr6.eq) goto loc_830C3A74;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c3a5c
	if (ctx.cr6.eq) goto loc_830C3A5C;
	// lwz r30,20(r11)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// lwz r31,16(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// cmplw cr6,r31,r30
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r30.u32, ctx.xer);
	// beq cr6,0x830c3a5c
	if (ctx.cr6.eq) goto loc_830C3A5C;
loc_830C3A44:
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// bl 0x830bf750
	ctx.lr = 0x830C3A50;
	sub_830BF750(ctx, base);
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// cmplw cr6,r31,r30
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r30.u32, ctx.xer);
	// bne cr6,0x830c3a44
	if (!ctx.cr6.eq) goto loc_830C3A44;
loc_830C3A5C:
	// lwz r11,348(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 348);
	// lwzx r3,r29,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + ctx.r11.u32);
	// bl 0x8315fca0
	ctx.lr = 0x830C3A68;
	sub_8315FCA0(ctx, base);
	// lwz r9,348(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 348);
	// li r10,0
	ctx.r10.s64 = 0;
	// stwx r10,r29,r9
	PPC_STORE_U32(ctx.r29.u32 + ctx.r9.u32, ctx.r10.u32);
loc_830C3A74:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82cb1138
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C3A7C"))) PPC_WEAK_FUNC(sub_830C3A7C);
PPC_FUNC_IMPL(__imp__sub_830C3A7C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C3A80"))) PPC_WEAK_FUNC(sub_830C3A80);
PPC_FUNC_IMPL(__imp__sub_830C3A80) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// bl 0x830bf988
	ctx.lr = 0x830C3AA0;
	sub_830BF988(ctx, base);
	// clrlwi r11,r30,31
	ctx.r11.u64 = ctx.r30.u32 & 0x1;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c3ac8
	if (ctx.cr6.eq) goto loc_830C3AC8;
	// lis r11,-31901
	ctx.r11.s64 = -2090663936;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lwz r3,-32308(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -32308);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,20(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830C3AC8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830C3AC8:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C3AE4"))) PPC_WEAK_FUNC(sub_830C3AE4);
PPC_FUNC_IMPL(__imp__sub_830C3AE4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C3AE8"))) PPC_WEAK_FUNC(sub_830C3AE8);
PPC_FUNC_IMPL(__imp__sub_830C3AE8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f0,0(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fneg f12,f0
	ctx.f12.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lfs f11,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// fneg f10,f13
	ctx.f10.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// fneg f9,f11
	ctx.f9.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// stfs f12,0(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// stfs f10,4(r3)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// stfs f9,8(r3)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C3B10"))) PPC_WEAK_FUNC(sub_830C3B10);
PPC_FUNC_IMPL(__imp__sub_830C3B10) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,0(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// lfs f13,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r5)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// lfs f12,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,8(r5)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C3B3C"))) PPC_WEAK_FUNC(sub_830C3B3C);
PPC_FUNC_IMPL(__imp__sub_830C3B3C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C3B40"))) PPC_WEAK_FUNC(sub_830C3B40);
PPC_FUNC_IMPL(__imp__sub_830C3B40) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,336(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C3B48"))) PPC_WEAK_FUNC(sub_830C3B48);
PPC_FUNC_IMPL(__imp__sub_830C3B48) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,336(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C3B50"))) PPC_WEAK_FUNC(sub_830C3B50);
PPC_FUNC_IMPL(__imp__sub_830C3B50) {
	PPC_FUNC_PROLOGUE();
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r11,164(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 164);
	// add r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 + ctx.r10.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r11,r10
	ctx.r3.u64 = ctx.r11.u64 + ctx.r10.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C3B68"))) PPC_WEAK_FUNC(sub_830C3B68);
PPC_FUNC_IMPL(__imp__sub_830C3B68) {
	PPC_FUNC_PROLOGUE();
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r11,172(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 172);
	// add r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 + ctx.r10.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r11,r10
	ctx.r3.u64 = ctx.r11.u64 + ctx.r10.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C3B80"))) PPC_WEAK_FUNC(sub_830C3B80);
PPC_FUNC_IMPL(__imp__sub_830C3B80) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,156
	ctx.r3.s64 = ctx.r3.s64 + 156;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C3B88"))) PPC_WEAK_FUNC(sub_830C3B88);
PPC_FUNC_IMPL(__imp__sub_830C3B88) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f1,152(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 152);
	ctx.f1.f64 = double(temp.f32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C3B90"))) PPC_WEAK_FUNC(sub_830C3B90);
PPC_FUNC_IMPL(__imp__sub_830C3B90) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,136
	ctx.r3.s64 = ctx.r3.s64 + 136;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C3B98"))) PPC_WEAK_FUNC(sub_830C3B98);
PPC_FUNC_IMPL(__imp__sub_830C3B98) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C3BA0"))) PPC_WEAK_FUNC(sub_830C3BA0);
PPC_FUNC_IMPL(__imp__sub_830C3BA0) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// addi r10,r11,5504
	ctx.r10.s64 = ctx.r11.s64 + 5504;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// b 0x83088de0
	sub_83088DE0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C3BB0"))) PPC_WEAK_FUNC(sub_830C3BB0);
PPC_FUNC_IMPL(__imp__sub_830C3BB0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,336(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	// lfs f0,136(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 136);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,0(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// lfs f13,140(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// lfs f12,144(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 144);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,8(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// lfs f11,148(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 148);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,12(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C3BD8"))) PPC_WEAK_FUNC(sub_830C3BD8);
PPC_FUNC_IMPL(__imp__sub_830C3BD8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,336(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	// addi r11,r11,112
	ctx.r11.s64 = ctx.r11.s64 + 112;
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,0(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// lfs f13,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// lfs f12,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,8(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// lfs f11,12(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,12(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// lfs f10,16(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,16(r4)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// lfs f9,20(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,20(r4)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C3C14"))) PPC_WEAK_FUNC(sub_830C3C14);
PPC_FUNC_IMPL(__imp__sub_830C3C14) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C3C18"))) PPC_WEAK_FUNC(sub_830C3C18);
PPC_FUNC_IMPL(__imp__sub_830C3C18) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,336(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	// lwz r3,344(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 344);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C3C24"))) PPC_WEAK_FUNC(sub_830C3C24);
PPC_FUNC_IMPL(__imp__sub_830C3C24) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C3C28"))) PPC_WEAK_FUNC(sub_830C3C28);
PPC_FUNC_IMPL(__imp__sub_830C3C28) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,336(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C3C30"))) PPC_WEAK_FUNC(sub_830C3C30);
PPC_FUNC_IMPL(__imp__sub_830C3C30) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,336(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C3C38"))) PPC_WEAK_FUNC(sub_830C3C38);
PPC_FUNC_IMPL(__imp__sub_830C3C38) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// stw r4,336(r30)
	PPC_STORE_U32(ctx.r30.u32 + 336, ctx.r4.u32);
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r10,52(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 52);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830C3C68;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,336(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 336);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lwz r7,48(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 48);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x830C3C84;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmplw cr6,r3,r31
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r31.u32, ctx.xer);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// blt cr6,0x830c3c94
	if (ctx.cr6.lt) goto loc_830C3C94;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
loc_830C3C94:
	// lwz r3,268(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 268);
	// bl 0x83042848
	ctx.lr = 0x830C3C9C;
	sub_83042848(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C3CB4"))) PPC_WEAK_FUNC(sub_830C3CB4);
PPC_FUNC_IMPL(__imp__sub_830C3CB4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C3CB8"))) PPC_WEAK_FUNC(sub_830C3CB8);
PPC_FUNC_IMPL(__imp__sub_830C3CB8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10ec
	ctx.lr = 0x830C3CC0;
	__savegprlr_29(ctx, base);
	// addi r12,r1,-32
	ctx.r12.s64 = ctx.r1.s64 + -32;
	// bl 0x82cb6abc
	ctx.lr = 0x830C3CC8;
	__savefpr_17(ctx, base);
	// stwu r1,-384(r1)
	ea = -384 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// mr r30,r5
	ctx.r30.u64 = ctx.r5.u64;
	// bl 0x8308a278
	ctx.lr = 0x830C3CDC;
	sub_8308A278(ctx, base);
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// li r10,5
	ctx.r10.s64 = 5;
	// addi r9,r11,5504
	ctx.r9.s64 = ctx.r11.s64 + 5504;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r9,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r9.u32);
	// lwz r8,100(r29)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r29.u32 + 100);
	// stw r8,340(r31)
	PPC_STORE_U32(ctx.r31.u32 + 340, ctx.r8.u32);
	// stw r10,288(r31)
	PPC_STORE_U32(ctx.r31.u32 + 288, ctx.r10.u32);
	// lwz r4,96(r29)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r29.u32 + 96);
	// bl 0x830c3c38
	ctx.lr = 0x830C3D04;
	sub_830C3C38(ctx, base);
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// bl 0x8315fb70
	ctx.lr = 0x830C3D0C;
	sub_8315FB70(ctx, base);
	// lwz r11,336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// lwz r5,112(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 112);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,4
	ctx.r6.s64 = ctx.r11.s64 + 4;
	// lwz r3,1412(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 1412);
	// clrlwi r11,r5,31
	ctx.r11.u64 = ctx.r5.u32 & 0x1;
	// stw r7,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r7.u32);
	// stw r6,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, ctx.r6.u32);
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// stb r11,188(r1)
	PPC_STORE_U8(ctx.r1.u32 + 188, ctx.r11.u8);
	// bl 0x8315fb80
	ctx.lr = 0x830C3D38;
	sub_8315FB80(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,152(r31)
	PPC_STORE_U32(ctx.r31.u32 + 152, ctx.r3.u32);
	// beq cr6,0x830c41d8
	if (ctx.cr6.eq) goto loc_830C41D8;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x8315fb70
	ctx.lr = 0x830C3D4C;
	sub_8315FB70(ctx, base);
	// li r10,10
	ctx.r10.s64 = 10;
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// stw r10,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r10.u32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c4048
	if (ctx.cr6.eq) goto loc_830C4048;
	// lfs f0,220(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 220);
	ctx.f0.f64 = double(temp.f32);
	// lfs f11,232(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 232);
	ctx.f11.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f13,224(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 224);
	ctx.f13.f64 = double(temp.f32);
	// fneg f8,f11
	ctx.f8.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// lfs f9,236(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 236);
	ctx.f9.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// lfs f7,240(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 240);
	ctx.f7.f64 = double(temp.f32);
	// fneg f6,f9
	ctx.f6.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// lfs f4,216(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 216);
	ctx.f4.f64 = double(temp.f32);
	// fneg f5,f7
	ctx.f5.u64 = ctx.f7.u64 ^ 0x8000000000000000;
	// fmr f3,f4
	ctx.f3.f64 = ctx.f4.f64;
	// lfs f2,228(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 228);
	ctx.f2.f64 = double(temp.f32);
	// fneg f1,f4
	ctx.f1.u64 = ctx.f4.u64 ^ 0x8000000000000000;
	// lfs f11,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f11.f64 = double(temp.f32);
	// fneg f9,f13
	ctx.f9.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// lfs f7,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f7.f64 = double(temp.f32);
	// fneg f4,f0
	ctx.f4.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lfs f13,6380(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6380);
	ctx.f13.f64 = double(temp.f32);
	// fmr f30,f2
	ctx.f30.f64 = ctx.f2.f64;
	// lfs f31,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f31.f64 = double(temp.f32);
	// fmsubs f29,f2,f2,f13
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f2.f64 - ctx.f13.f64));
	// lfs f0,7676(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f28,f12,f8
	ctx.f28.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// addi r10,r11,216
	ctx.r10.s64 = ctx.r11.s64 + 216;
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// fmuls f27,f10,f6
	ctx.f27.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// fmuls f26,f10,f5
	ctx.f26.f64 = double(float(ctx.f10.f64 * ctx.f5.f64));
	// fmuls f25,f5,f3
	ctx.f25.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fmuls f24,f11,f1
	ctx.f24.f64 = double(float(ctx.f11.f64 * ctx.f1.f64));
	// fmuls f23,f9,f7
	ctx.f23.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// fmuls f22,f4,f7
	ctx.f22.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// fmuls f21,f4,f31
	ctx.f21.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// fmsubs f20,f30,f30,f13
	ctx.f20.f64 = double(float(ctx.f30.f64 * ctx.f30.f64 - ctx.f13.f64));
	// fmsubs f28,f6,f3,f28
	ctx.f28.f64 = double(float(ctx.f6.f64 * ctx.f3.f64 - ctx.f28.f64));
	// fmuls f19,f29,f8
	ctx.f19.f64 = double(float(ctx.f29.f64 * ctx.f8.f64));
	// fmsubs f27,f12,f5,f27
	ctx.f27.f64 = double(float(ctx.f12.f64 * ctx.f5.f64 - ctx.f27.f64));
	// fmadds f26,f12,f6,f26
	ctx.f26.f64 = double(float(ctx.f12.f64 * ctx.f6.f64 + ctx.f26.f64));
	// fmsubs f25,f10,f8,f25
	ctx.f25.f64 = double(float(ctx.f10.f64 * ctx.f8.f64 - ctx.f25.f64));
	// fmsubs f24,f9,f31,f24
	ctx.f24.f64 = double(float(ctx.f9.f64 * ctx.f31.f64 - ctx.f24.f64));
	// fmuls f5,f5,f29
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f29.f64));
	// fmsubs f23,f4,f11,f23
	ctx.f23.f64 = double(float(ctx.f4.f64 * ctx.f11.f64 - ctx.f23.f64));
	// fmadds f22,f31,f1,f22
	ctx.f22.f64 = double(float(ctx.f31.f64 * ctx.f1.f64 + ctx.f22.f64));
	// fmuls f6,f6,f29
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f29.f64));
	// fmuls f29,f28,f2
	ctx.f29.f64 = double(float(ctx.f28.f64 * ctx.f2.f64));
	// fmsubs f21,f7,f1,f21
	ctx.f21.f64 = double(float(ctx.f7.f64 * ctx.f1.f64 - ctx.f21.f64));
	// fmuls f28,f27,f2
	ctx.f28.f64 = double(float(ctx.f27.f64 * ctx.f2.f64));
	// fmadds f8,f3,f8,f26
	ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f8.f64 + ctx.f26.f64));
	// fmuls f2,f25,f2
	ctx.f2.f64 = double(float(ctx.f25.f64 * ctx.f2.f64));
	// fmuls f27,f11,f20
	ctx.f27.f64 = double(float(ctx.f11.f64 * ctx.f20.f64));
	// fmuls f7,f7,f20
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f20.f64));
	// fmuls f26,f24,f30
	ctx.f26.f64 = double(float(ctx.f24.f64 * ctx.f30.f64));
	// fmadds f11,f9,f11,f22
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f11.f64 + ctx.f22.f64));
	// fmuls f31,f31,f20
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f20.f64));
	// fmuls f25,f30,f23
	ctx.f25.f64 = double(float(ctx.f30.f64 * ctx.f23.f64));
	// fmuls f24,f21,f30
	ctx.f24.f64 = double(float(ctx.f21.f64 * ctx.f30.f64));
	// fsubs f5,f5,f29
	ctx.f5.f64 = double(float(ctx.f5.f64 - ctx.f29.f64));
	// fmuls f10,f10,f8
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f8.f64));
	// fmuls f3,f8,f3
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f3.f64));
	// fmuls f8,f12,f8
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// fsubs f6,f6,f2
	ctx.f6.f64 = double(float(ctx.f6.f64 - ctx.f2.f64));
	// fsubs f29,f19,f28
	ctx.f29.f64 = double(float(ctx.f19.f64 - ctx.f28.f64));
	// fadds f2,f7,f26
	ctx.f2.f64 = double(float(ctx.f7.f64 + ctx.f26.f64));
	// fmuls f12,f11,f1
	ctx.f12.f64 = double(float(ctx.f11.f64 * ctx.f1.f64));
	// fmuls f28,f4,f11
	ctx.f28.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fadds f7,f31,f25
	ctx.f7.f64 = double(float(ctx.f31.f64 + ctx.f25.f64));
	// fmuls f11,f9,f11
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// fadds f31,f27,f24
	ctx.f31.f64 = double(float(ctx.f27.f64 + ctx.f24.f64));
	// fadds f5,f5,f10
	ctx.f5.f64 = double(float(ctx.f5.f64 + ctx.f10.f64));
	// fadds f10,f6,f8
	ctx.f10.f64 = double(float(ctx.f6.f64 + ctx.f8.f64));
	// fadds f3,f29,f3
	ctx.f3.f64 = double(float(ctx.f29.f64 + ctx.f3.f64));
	// fadds f8,f2,f28
	ctx.f8.f64 = double(float(ctx.f2.f64 + ctx.f28.f64));
	// lfs f2,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f2.f64 = double(temp.f32);
	// fadds f7,f7,f12
	ctx.f7.f64 = double(float(ctx.f7.f64 + ctx.f12.f64));
	// fadds f6,f31,f11
	ctx.f6.f64 = double(float(ctx.f31.f64 + ctx.f11.f64));
	// lfs f11,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fmuls f31,f30,f11
	ctx.f31.f64 = double(float(ctx.f30.f64 * ctx.f11.f64));
	// fmuls f12,f10,f0
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f3,f3,f0
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmuls f10,f30,f2
	ctx.f10.f64 = double(float(ctx.f30.f64 * ctx.f2.f64));
	// fmuls f28,f9,f11
	ctx.f28.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// lfs f27,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f27.f64 = double(temp.f32);
	// lfs f29,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmadds f10,f27,f1,f10
	ctx.f10.f64 = double(float(ctx.f27.f64 * ctx.f1.f64 + ctx.f10.f64));
	// lfs f23,176(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 176);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f26,f11,f1
	ctx.f26.f64 = double(float(ctx.f11.f64 * ctx.f1.f64));
	// lfs f25,184(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 184);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f24,180(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f31,f29,f1,f31
	ctx.f31.f64 = double(float(ctx.f29.f64 * ctx.f1.f64 + ctx.f31.f64));
	// lfs f22,168(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 168);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfs f21,160(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,172(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 172);
	ctx.f20.f64 = double(temp.f32);
	// addi r10,r31,160
	ctx.r10.s64 = ctx.r31.s64 + 160;
	// lfs f19,164(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 164);
	ctx.f19.f64 = double(temp.f32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// fmadds f28,f30,f27,f28
	ctx.f28.f64 = double(float(ctx.f30.f64 * ctx.f27.f64 + ctx.f28.f64));
	// fadds f6,f6,f5
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f5.f64));
	// fmadds f5,f9,f29,f10
	ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f29.f64 + ctx.f10.f64));
	// fmsubs f30,f30,f29,f26
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f29.f64 - ctx.f26.f64));
	// fadds f7,f7,f3
	ctx.f7.f64 = double(float(ctx.f7.f64 + ctx.f3.f64));
	// fmadds f3,f4,f2,f31
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f2.f64 + ctx.f31.f64));
	// fadds f8,f8,f12
	ctx.f8.f64 = double(float(ctx.f8.f64 + ctx.f12.f64));
	// fmadds f12,f4,f29,f28
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f29.f64 + ctx.f28.f64));
	// fnmsubs f5,f4,f11,f5
	ctx.f5.f64 = double(float(-(ctx.f4.f64 * ctx.f11.f64 - ctx.f5.f64)));
	// fnmsubs f10,f4,f27,f30
	ctx.f10.f64 = double(float(-(ctx.f4.f64 * ctx.f27.f64 - ctx.f30.f64)));
	// fnmsubs f4,f9,f27,f3
	ctx.f4.f64 = double(float(-(ctx.f9.f64 * ctx.f27.f64 - ctx.f3.f64)));
	// fnmsubs f3,f2,f1,f12
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f1.f64 - ctx.f12.f64)));
	// fmuls f1,f5,f24
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f24.f64));
	// fnmsubs f2,f9,f2,f10
	ctx.f2.f64 = double(float(-(ctx.f9.f64 * ctx.f2.f64 - ctx.f10.f64)));
	// fmuls f31,f5,f21
	ctx.f31.f64 = double(float(ctx.f5.f64 * ctx.f21.f64));
	// fmuls f12,f25,f4
	ctx.f12.f64 = double(float(ctx.f25.f64 * ctx.f4.f64));
	// fmuls f30,f5,f20
	ctx.f30.f64 = double(float(ctx.f5.f64 * ctx.f20.f64));
	// fmuls f29,f21,f4
	ctx.f29.f64 = double(float(ctx.f21.f64 * ctx.f4.f64));
	// fmuls f11,f3,f23
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f23.f64));
	// fmuls f10,f3,f24
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f24.f64));
	// fmsubs f1,f3,f25,f1
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f25.f64 - ctx.f1.f64));
	// fmsubs f9,f2,f2,f13
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f2.f64 - ctx.f13.f64));
	// fmuls f13,f3,f22
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f22.f64));
	// fmsubs f12,f5,f23,f12
	ctx.f12.f64 = double(float(ctx.f5.f64 * ctx.f23.f64 - ctx.f12.f64));
	// fmadds f31,f2,f19,f31
	ctx.f31.f64 = double(float(ctx.f2.f64 * ctx.f19.f64 + ctx.f31.f64));
	// fmadds f30,f2,f22,f30
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f22.f64 + ctx.f30.f64));
	// fmsubs f11,f24,f4,f11
	ctx.f11.f64 = double(float(ctx.f24.f64 * ctx.f4.f64 - ctx.f11.f64));
	// fmadds f10,f23,f4,f10
	ctx.f10.f64 = double(float(ctx.f23.f64 * ctx.f4.f64 + ctx.f10.f64));
	// fmuls f1,f2,f1
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// fmuls f28,f23,f9
	ctx.f28.f64 = double(float(ctx.f23.f64 * ctx.f9.f64));
	// fmadds f13,f2,f21,f13
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f21.f64 + ctx.f13.f64));
	// fmuls f27,f24,f9
	ctx.f27.f64 = double(float(ctx.f24.f64 * ctx.f9.f64));
	// fmuls f9,f25,f9
	ctx.f9.f64 = double(float(ctx.f25.f64 * ctx.f9.f64));
	// fmuls f12,f12,f2
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f2.f64));
	// fmadds f31,f3,f20,f31
	ctx.f31.f64 = double(float(ctx.f3.f64 * ctx.f20.f64 + ctx.f31.f64));
	// fmadds f30,f19,f4,f30
	ctx.f30.f64 = double(float(ctx.f19.f64 * ctx.f4.f64 + ctx.f30.f64));
	// fmuls f11,f11,f2
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f2.f64));
	// fmadds f10,f5,f25,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f25.f64 + ctx.f10.f64));
	// fmsubs f2,f2,f20,f29
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f20.f64 - ctx.f29.f64));
	// fadds f1,f28,f1
	ctx.f1.f64 = double(float(ctx.f28.f64 + ctx.f1.f64));
	// fmadds f13,f20,f4,f13
	ctx.f13.f64 = double(float(ctx.f20.f64 * ctx.f4.f64 + ctx.f13.f64));
	// fadds f12,f27,f12
	ctx.f12.f64 = double(float(ctx.f27.f64 + ctx.f12.f64));
	// fadds f11,f9,f11
	ctx.f11.f64 = double(float(ctx.f9.f64 + ctx.f11.f64));
	// fmuls f9,f10,f4
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f4.f64));
	// fmuls f29,f3,f10
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f10.f64));
	// fmuls f10,f5,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f10.f64));
	// fnmsubs f13,f5,f19,f13
	ctx.f13.f64 = double(float(-(ctx.f5.f64 * ctx.f19.f64 - ctx.f13.f64)));
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fnmsubs f2,f3,f19,f2
	ctx.f2.f64 = double(float(-(ctx.f3.f64 * ctx.f19.f64 - ctx.f2.f64)));
	// fnmsubs f4,f22,f4,f31
	ctx.f4.f64 = double(float(-(ctx.f22.f64 * ctx.f4.f64 - ctx.f31.f64)));
	// fnmsubs f3,f3,f21,f30
	ctx.f3.f64 = double(float(-(ctx.f3.f64 * ctx.f21.f64 - ctx.f30.f64)));
	// fadds f1,f1,f9
	ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f9.f64));
	// fadds f13,f12,f29
	ctx.f13.f64 = double(float(ctx.f12.f64 + ctx.f29.f64));
	// fadds f12,f11,f10
	ctx.f12.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// fnmsubs f11,f5,f22,f2
	ctx.f11.f64 = double(float(-(ctx.f5.f64 * ctx.f22.f64 - ctx.f2.f64)));
	// fmuls f10,f1,f0
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f9,f13,f0
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fmuls f5,f12,f0
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fadds f0,f10,f7
	ctx.f0.f64 = double(float(ctx.f10.f64 + ctx.f7.f64));
	// fadds f2,f9,f8
	ctx.f2.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
	// stfs f2,100(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f1,f5,f6
	ctx.f1.f64 = double(float(ctx.f5.f64 + ctx.f6.f64));
	// stfs f1,104(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lwz r7,96(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stfs f4,84(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lwz r5,184(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 184);
	// stfs f3,88(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r11,r1,140
	ctx.r11.s64 = ctx.r1.s64 + 140;
	// stfs f11,92(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lwz r4,4(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r10,8(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// lwz r8,12(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lwz r6,104(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// stw r5,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r5.u32);
	// stw r4,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r4.u32);
	// stw r3,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r3.u32);
	// stw r10,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r10.u32);
	// stw r8,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r8.u32);
	// stw r7,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r7.u32);
	// stw r9,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r9.u32);
	// stw r6,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r6.u32);
	// b 0x830c41bc
	goto loc_830C41BC;
loc_830C4048:
	// lfs f12,120(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f12.f64 = double(temp.f32);
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// lfs f13,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// fmr f9,f12
	ctx.f9.f64 = ctx.f12.f64;
	// lfs f10,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f10.f64 = double(temp.f32);
	// fmr f11,f13
	ctx.f11.f64 = ctx.f13.f64;
	// fmr f8,f10
	ctx.f8.f64 = ctx.f10.f64;
	// lfs f4,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f4.f64 = double(temp.f32);
	// lfs f7,184(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 184);
	ctx.f7.f64 = double(temp.f32);
	// li r8,0
	ctx.r8.s64 = 0;
	// lfs f0,6380(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6380);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r31,160
	ctx.r11.s64 = ctx.r31.s64 + 160;
	// lfs f5,176(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 176);
	ctx.f5.f64 = double(temp.f32);
	// fmsubs f3,f4,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f4.f64 - ctx.f0.f64));
	// lfs f6,180(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	ctx.f6.f64 = double(temp.f32);
	// addi r11,r31,112
	ctx.r11.s64 = ctx.r31.s64 + 112;
	// lfs f2,160(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	ctx.f2.f64 = double(temp.f32);
	// addi r6,r1,140
	ctx.r6.s64 = ctx.r1.s64 + 140;
	// lfs f1,172(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 172);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f31,f2,f12
	ctx.f31.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// fmuls f30,f1,f13
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
	// lfs f27,164(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 164);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f26,f9,f7
	ctx.f26.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// lfs f29,168(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 168);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f28,f7,f11
	ctx.f28.f64 = double(float(ctx.f7.f64 * ctx.f11.f64));
	// lfs f0,7676(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f25,f8,f5
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// lfs f24,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f23,f9,f6
	ctx.f23.f64 = double(float(ctx.f9.f64 * ctx.f6.f64));
	// lfs f22,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f21,f1,f12
	ctx.f21.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// lfs f20,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f18,f6,f3
	ctx.f18.f64 = double(float(ctx.f6.f64 * ctx.f3.f64));
	// stw r8,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r8.u32);
	// fmuls f17,f5,f3
	ctx.f17.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fmuls f3,f7,f3
	ctx.f3.f64 = double(float(ctx.f7.f64 * ctx.f3.f64));
	// fmuls f19,f2,f13
	ctx.f19.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// fmadds f30,f2,f4,f30
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f4.f64 + ctx.f30.f64));
	// fmadds f26,f8,f6,f26
	ctx.f26.f64 = double(float(ctx.f8.f64 * ctx.f6.f64 + ctx.f26.f64));
	// fmsubs f28,f9,f5,f28
	ctx.f28.f64 = double(float(ctx.f9.f64 * ctx.f5.f64 - ctx.f28.f64));
	// fmsubs f6,f6,f11,f25
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f11.f64 - ctx.f25.f64));
	// fmsubs f7,f8,f7,f23
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f7.f64 - ctx.f23.f64));
	// fmadds f31,f27,f4,f31
	ctx.f31.f64 = double(float(ctx.f27.f64 * ctx.f4.f64 + ctx.f31.f64));
	// fmadds f25,f29,f4,f21
	ctx.f25.f64 = double(float(ctx.f29.f64 * ctx.f4.f64 + ctx.f21.f64));
	// fmsubs f23,f1,f4,f19
	ctx.f23.f64 = double(float(ctx.f1.f64 * ctx.f4.f64 - ctx.f19.f64));
	// fmadds f5,f5,f11,f26
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f11.f64 + ctx.f26.f64));
	// fmuls f28,f28,f4
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f4.f64));
	// fmuls f6,f6,f4
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// fmuls f4,f7,f4
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f4.f64));
	// fmadds f7,f29,f10,f30
	ctx.f7.f64 = double(float(ctx.f29.f64 * ctx.f10.f64 + ctx.f30.f64));
	// fmadds f1,f1,f10,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f10.f64 + ctx.f31.f64));
	// fmadds f31,f27,f13,f25
	ctx.f31.f64 = double(float(ctx.f27.f64 * ctx.f13.f64 + ctx.f25.f64));
	// fnmsubs f30,f27,f10,f23
	ctx.f30.f64 = double(float(-(ctx.f27.f64 * ctx.f10.f64 - ctx.f23.f64)));
	// fmuls f8,f8,f5
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// fmuls f9,f9,f5
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f5.f64));
	// fmuls f11,f5,f11
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// fadds f6,f3,f6
	ctx.f6.f64 = double(float(ctx.f3.f64 + ctx.f6.f64));
	// fadds f5,f17,f4
	ctx.f5.f64 = double(float(ctx.f17.f64 + ctx.f4.f64));
	// fadds f28,f18,f28
	ctx.f28.f64 = double(float(ctx.f18.f64 + ctx.f28.f64));
	// fnmsubs f3,f29,f13,f1
	ctx.f3.f64 = double(float(-(ctx.f29.f64 * ctx.f13.f64 - ctx.f1.f64)));
	// stfs f3,84(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f4,f27,f12,f7
	ctx.f4.f64 = double(float(-(ctx.f27.f64 * ctx.f12.f64 - ctx.f7.f64)));
	// stfs f4,80(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fnmsubs f1,f29,f12,f30
	ctx.f1.f64 = double(float(-(ctx.f29.f64 * ctx.f12.f64 - ctx.f30.f64)));
	// stfs f1,92(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fnmsubs f2,f2,f10,f31
	ctx.f2.f64 = double(float(-(ctx.f2.f64 * ctx.f10.f64 - ctx.f31.f64)));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fadds f12,f6,f9
	ctx.f12.f64 = double(float(ctx.f6.f64 + ctx.f9.f64));
	// fadds f11,f5,f11
	ctx.f11.f64 = double(float(ctx.f5.f64 + ctx.f11.f64));
	// fadds f13,f28,f8
	ctx.f13.f64 = double(float(ctx.f28.f64 + ctx.f8.f64));
	// fmuls f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f8,f11,f0
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f10,f13,f0
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fadds f6,f20,f9
	ctx.f6.f64 = double(float(ctx.f20.f64 + ctx.f9.f64));
	// stfs f6,104(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fadds f5,f8,f22
	ctx.f5.f64 = double(float(ctx.f8.f64 + ctx.f22.f64));
	// stfs f5,96(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f7,f24,f10
	ctx.f7.f64 = double(float(ctx.f24.f64 + ctx.f10.f64));
	// stfs f7,100(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// lwz r5,104(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r4,8(r7)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// lwz r3,12(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// lwz r10,4(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r9,96(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r4,8(r6)
	PPC_STORE_U32(ctx.r6.u32 + 8, ctx.r4.u32);
	// stw r10,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r10.u32);
	// stw r8,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r8.u32);
	// stw r3,12(r6)
	PPC_STORE_U32(ctx.r6.u32 + 12, ctx.r3.u32);
	// stw r9,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r9.u32);
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r11.u32);
	// stw r5,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r5.u32);
loc_830C41BC:
	// lwz r11,152(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 152);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r3,1412(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 1412);
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r11.u32);
	// bl 0x8315fb80
	ctx.lr = 0x830C41D0;
	sub_8315FB80(ctx, base);
	// stw r3,156(r31)
	PPC_STORE_U32(ctx.r31.u32 + 156, ctx.r3.u32);
	// b 0x830c41e0
	goto loc_830C41E0;
loc_830C41D8:
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,156(r31)
	PPC_STORE_U32(ctx.r31.u32 + 156, ctx.r11.u32);
loc_830C41E0:
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x830c4228
	if (ctx.cr6.eq) goto loc_830C4228;
	// lwz r10,288(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 288);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// cmpwi cr6,r10,5
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 5, ctx.xer);
	// bne cr6,0x830c422c
	if (!ctx.cr6.eq) goto loc_830C422C;
	// lwz r10,276(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 276);
	// rlwinm r9,r10,0,24,24
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x830c422c
	if (ctx.cr6.eq) goto loc_830C422C;
	// lwz r10,272(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 272);
	// ori r9,r10,256
	ctx.r9.u64 = ctx.r10.u64 | 256;
	// stw r9,272(r11)
	PPC_STORE_U32(ctx.r11.u32 + 272, ctx.r9.u32);
	// addi r1,r1,384
	ctx.r1.s64 = ctx.r1.s64 + 384;
	// addi r12,r1,-32
	ctx.r12.s64 = ctx.r1.s64 + -32;
	// bl 0x82cb6b08
	ctx.lr = 0x830C4224;
	__restfpr_17(ctx, base);
	// b 0x82cb113c
	__restgprlr_29(ctx, base);
	return;
loc_830C4228:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
loc_830C422C:
	// addi r1,r1,384
	ctx.r1.s64 = ctx.r1.s64 + 384;
	// addi r12,r1,-32
	ctx.r12.s64 = ctx.r1.s64 + -32;
	// bl 0x82cb6b08
	ctx.lr = 0x830C4238;
	__restfpr_17(ctx, base);
	// b 0x82cb113c
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C423C"))) PPC_WEAK_FUNC(sub_830C423C);
PPC_FUNC_IMPL(__imp__sub_830C423C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C4240"))) PPC_WEAK_FUNC(sub_830C4240);
PPC_FUNC_IMPL(__imp__sub_830C4240) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6ab4
	ctx.lr = 0x830C4250;
	__savefpr_15(ctx, base);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f12,6380(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6380);
	ctx.f12.f64 = double(temp.f32);
	// beq cr6,0x830c4460
	if (ctx.cr6.eq) goto loc_830C4460;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c4460
	if (ctx.cr6.eq) goto loc_830C4460;
	// lfs f11,252(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f11.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f10,112(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 112);
	ctx.f10.f64 = double(temp.f32);
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// fmuls f8,f10,f11
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
	// lfs f6,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f6.f64 = double(temp.f32);
	// lfs f4,124(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 124);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f3,f10,f6
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// lfs f30,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f1,f4,f6
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f6.f64));
	// lfs f7,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f7.f64 = double(temp.f32);
	// fmr f29,f6
	ctx.f29.f64 = ctx.f6.f64;
	// lfs f2,116(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 116);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f31,f4,f11
	ctx.f31.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmr f26,f30
	ctx.f26.f64 = ctx.f30.f64;
	// lfs f25,132(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 132);
	ctx.f25.f64 = double(temp.f32);
	// lfs f27,136(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	ctx.f27.f64 = double(temp.f32);
	// fmsubs f5,f7,f7,f12
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f7.f64 - ctx.f12.f64));
	// lfs f28,120(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 120);
	ctx.f28.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f24,128(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 128);
	ctx.f24.f64 = double(temp.f32);
	// addi r10,r3,112
	ctx.r10.s64 = ctx.r3.s64 + 112;
	// fmuls f22,f9,f25
	ctx.f22.f64 = double(float(ctx.f9.f64 * ctx.f25.f64));
	// lfs f0,7676(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f8,f2,f7,f8
	ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f7.f64 + ctx.f8.f64));
	// lfs f21,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f21.f64 = double(temp.f32);
	// fmsubs f3,f4,f7,f3
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f7.f64 - ctx.f3.f64));
	// lfs f20,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f1,f10,f7,f1
	ctx.f1.f64 = double(float(ctx.f10.f64 * ctx.f7.f64 + ctx.f1.f64));
	// lfs f13,6140(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 6140);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f23,f9,f27
	ctx.f23.f64 = double(float(ctx.f9.f64 * ctx.f27.f64));
	// lfs f19,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f31,f28,f7,f31
	ctx.f31.f64 = double(float(ctx.f28.f64 * ctx.f7.f64 + ctx.f31.f64));
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// fmuls f18,f27,f29
	ctx.f18.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// addi r9,r3,12
	ctx.r9.s64 = ctx.r3.s64 + 12;
	// fmuls f17,f26,f24
	ctx.f17.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// fmuls f15,f24,f5
	ctx.f15.f64 = double(float(ctx.f24.f64 * ctx.f5.f64));
	// fmuls f16,f25,f5
	ctx.f16.f64 = double(float(ctx.f25.f64 * ctx.f5.f64));
	// fmuls f5,f27,f5
	ctx.f5.f64 = double(float(ctx.f27.f64 * ctx.f5.f64));
	// fmadds f8,f4,f30,f8
	ctx.f8.f64 = double(float(ctx.f4.f64 * ctx.f30.f64 + ctx.f8.f64));
	// fmsubs f4,f26,f27,f22
	ctx.f4.f64 = double(float(ctx.f26.f64 * ctx.f27.f64 - ctx.f22.f64));
	// fnmsubs f3,f2,f30,f3
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f30.f64 - ctx.f3.f64)));
	// fmadds f1,f28,f30,f1
	ctx.f1.f64 = double(float(ctx.f28.f64 * ctx.f30.f64 + ctx.f1.f64));
	// fmadds f23,f24,f29,f23
	ctx.f23.f64 = double(float(ctx.f24.f64 * ctx.f29.f64 + ctx.f23.f64));
	// fmadds f31,f2,f6,f31
	ctx.f31.f64 = double(float(ctx.f2.f64 * ctx.f6.f64 + ctx.f31.f64));
	// fmsubs f24,f9,f24,f18
	ctx.f24.f64 = double(float(ctx.f9.f64 * ctx.f24.f64 - ctx.f18.f64));
	// fmsubs f22,f25,f29,f17
	ctx.f22.f64 = double(float(ctx.f25.f64 * ctx.f29.f64 - ctx.f17.f64));
	// fnmsubs f8,f28,f6,f8
	ctx.f8.f64 = double(float(-(ctx.f28.f64 * ctx.f6.f64 - ctx.f8.f64)));
	// fmuls f6,f4,f7
	ctx.f6.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// fnmsubs f4,f28,f11,f3
	ctx.f4.f64 = double(float(-(ctx.f28.f64 * ctx.f11.f64 - ctx.f3.f64)));
	// fnmsubs f3,f2,f11,f1
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f11.f64 - ctx.f1.f64)));
	// fmadds f27,f26,f25,f23
	ctx.f27.f64 = double(float(ctx.f26.f64 * ctx.f25.f64 + ctx.f23.f64));
	// fnmsubs f2,f10,f30,f31
	ctx.f2.f64 = double(float(-(ctx.f10.f64 * ctx.f30.f64 - ctx.f31.f64)));
	// fmuls f1,f24,f7
	ctx.f1.f64 = double(float(ctx.f24.f64 * ctx.f7.f64));
	// fmuls f11,f22,f7
	ctx.f11.f64 = double(float(ctx.f22.f64 * ctx.f7.f64));
	// fmuls f7,f8,f8
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f8.f64));
	// fadds f6,f15,f6
	ctx.f6.f64 = double(float(ctx.f15.f64 + ctx.f6.f64));
	// fmuls f30,f8,f3
	ctx.f30.f64 = double(float(ctx.f8.f64 * ctx.f3.f64));
	// fmuls f31,f27,f29
	ctx.f31.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// fmuls f9,f9,f27
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f27.f64));
	// fmuls f29,f2,f2
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f2.f64));
	// fadds f11,f5,f11
	ctx.f11.f64 = double(float(ctx.f5.f64 + ctx.f11.f64));
	// fmuls f28,f4,f2
	ctx.f28.f64 = double(float(ctx.f4.f64 * ctx.f2.f64));
	// fmuls f10,f26,f27
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f27.f64));
	// fmuls f5,f7,f0
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fadds f1,f16,f1
	ctx.f1.f64 = double(float(ctx.f16.f64 + ctx.f1.f64));
	// fmuls f7,f30,f0
	ctx.f7.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmuls f30,f29,f0
	ctx.f30.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// fadds f11,f11,f9
	ctx.f11.f64 = double(float(ctx.f11.f64 + ctx.f9.f64));
	// fmuls f29,f28,f0
	ctx.f29.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// fsubs f9,f13,f5
	ctx.f9.f64 = double(float(ctx.f13.f64 - ctx.f5.f64));
	// fadds f1,f1,f10
	ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f10.f64));
	// fadds f10,f6,f31
	ctx.f10.f64 = double(float(ctx.f6.f64 + ctx.f31.f64));
	// fsubs f6,f7,f29
	ctx.f6.f64 = double(float(ctx.f7.f64 - ctx.f29.f64));
	// stfs f6,-188(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + -188, temp.u32);
	// fmuls f6,f2,f3
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f3.f64));
	// fsubs f9,f9,f30
	ctx.f9.f64 = double(float(ctx.f9.f64 - ctx.f30.f64));
	// stfs f9,-192(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -192, temp.u32);
	// fmuls f1,f1,f0
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f9,f4,f8
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f8.f64));
	// fmuls f8,f2,f8
	ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f8.f64));
	// addi r11,r1,-192
	ctx.r11.s64 = ctx.r1.s64 + -192;
	// fmuls f31,f3,f3
	ctx.f31.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f4,f4,f3
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fmuls f3,f10,f0
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f2,f11,f0
	ctx.f2.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f10,f6,f0
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fadds f11,f29,f7
	ctx.f11.f64 = double(float(ctx.f29.f64 + ctx.f7.f64));
	// stfs f11,-180(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -180, temp.u32);
	// fadds f11,f20,f1
	ctx.f11.f64 = double(float(ctx.f20.f64 + ctx.f1.f64));
	// fmuls f6,f8,f0
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fnmsubs f7,f31,f0,f13
	ctx.f7.f64 = double(float(-(ctx.f31.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fadds f13,f3,f21
	ctx.f13.f64 = double(float(ctx.f3.f64 + ctx.f21.f64));
	// fadds f0,f19,f2
	ctx.f0.f64 = double(float(ctx.f19.f64 + ctx.f2.f64));
	// fadds f3,f9,f10
	ctx.f3.f64 = double(float(ctx.f9.f64 + ctx.f10.f64));
	// stfs f3,-184(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + -184, temp.u32);
	// fsubs f1,f10,f9
	ctx.f1.f64 = double(float(ctx.f10.f64 - ctx.f9.f64));
	// stfs f1,-168(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -168, temp.u32);
	// fsubs f2,f7,f30
	ctx.f2.f64 = double(float(ctx.f7.f64 - ctx.f30.f64));
	// stfs f2,-176(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + -176, temp.u32);
	// fsubs f10,f6,f4
	ctx.f10.f64 = double(float(ctx.f6.f64 - ctx.f4.f64));
	// stfs f10,-172(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -172, temp.u32);
	// fadds f9,f4,f6
	ctx.f9.f64 = double(float(ctx.f4.f64 + ctx.f6.f64));
	// stfs f9,-164(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -164, temp.u32);
	// fsubs f8,f7,f5
	ctx.f8.f64 = double(float(ctx.f7.f64 - ctx.f5.f64));
	// stfs f8,-160(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + -160, temp.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830C4434:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c4434
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C4434;
	// stfs f13,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f11,40(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f0,44(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
loc_830C4460:
	// lwz r10,336(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	// addi r11,r4,12
	ctx.r11.s64 = ctx.r4.s64 + 12;
	// addi r11,r3,12
	ctx.r11.s64 = ctx.r3.s64 + 12;
	// addi r11,r10,112
	ctx.r11.s64 = ctx.r10.s64 + 112;
	// lfs f0,112(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 112);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,0(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// lfs f13,116(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 116);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// lfs f11,120(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 120);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,8(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// lfs f10,124(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 124);
	ctx.f10.f64 = double(temp.f32);
	// fmr f5,f10
	ctx.f5.f64 = ctx.f10.f64;
	// stfs f10,12(r4)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// fmr f4,f10
	ctx.f4.f64 = ctx.f10.f64;
	// lfs f9,128(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 128);
	ctx.f9.f64 = double(temp.f32);
	// fmr f6,f9
	ctx.f6.f64 = ctx.f9.f64;
	// stfs f9,16(r4)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// fmr f30,f6
	ctx.f30.f64 = ctx.f6.f64;
	// lfs f8,132(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 132);
	ctx.f8.f64 = double(temp.f32);
	// fmr f3,f8
	ctx.f3.f64 = ctx.f8.f64;
	// stfs f8,20(r4)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// fmr f11,f8
	ctx.f11.f64 = ctx.f8.f64;
	// lfs f7,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// lfs f2,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,28(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,12(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,24(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,36(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f8.f64 = double(temp.f32);
	// lfs f31,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f31.f64 = double(temp.f32);
	// fsubs f30,f30,f31
	ctx.f30.f64 = double(float(ctx.f30.f64 - ctx.f31.f64));
	// fadds f6,f6,f31
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f31.f64));
	// lfs f29,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f29.f64 = double(temp.f32);
	// fsubs f5,f5,f7
	ctx.f5.f64 = double(float(ctx.f5.f64 - ctx.f7.f64));
	// lfs f31,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f31.f64 = double(temp.f32);
	// fadds f4,f4,f7
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f7.f64));
	// lfs f7,56(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	ctx.f7.f64 = double(temp.f32);
	// fsubs f3,f3,f31
	ctx.f3.f64 = double(float(ctx.f3.f64 - ctx.f31.f64));
	// lfs f28,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f28.f64 = double(temp.f32);
	// fadds f11,f11,f31
	ctx.f11.f64 = double(float(ctx.f11.f64 + ctx.f31.f64));
	// lfs f27,48(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f30,f30,f12
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f12.f64));
	// lfs f31,32(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f6,f6,f12
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// fmuls f5,f5,f12
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// fmuls f4,f4,f12
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// fmuls f3,f3,f12
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// fmuls f12,f11,f12
	ctx.f12.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// fmuls f26,f2,f30
	ctx.f26.f64 = double(float(ctx.f2.f64 * ctx.f30.f64));
	// fmuls f11,f1,f30
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f30.f64));
	// fmuls f30,f0,f30
	ctx.f30.f64 = double(float(ctx.f0.f64 * ctx.f30.f64));
	// fmuls f2,f2,f6
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f6.f64));
	// fmuls f25,f13,f5
	ctx.f25.f64 = double(float(ctx.f13.f64 * ctx.f5.f64));
	// fmuls f0,f0,f6
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f6.f64));
	// fmuls f1,f1,f6
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f6.f64));
	// fmuls f6,f10,f5
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f5.f64));
	// fmuls f24,f8,f3
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f3.f64));
	// fmuls f5,f9,f5
	ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f5.f64));
	// fabs f26,f26
	ctx.f26.u64 = ctx.f26.u64 & ~0x8000000000000000;
	// fabs f11,f11
	ctx.f11.u64 = ctx.f11.u64 & ~0x8000000000000000;
	// fmuls f23,f31,f3
	ctx.f23.f64 = double(float(ctx.f31.f64 * ctx.f3.f64));
	// fmadds f2,f13,f4,f2
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f4.f64 + ctx.f2.f64));
	// fmadds f0,f9,f4,f0
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f4.f64 + ctx.f0.f64));
	// fabs f25,f25
	ctx.f25.u64 = ctx.f25.u64 & ~0x8000000000000000;
	// fmadds f1,f10,f4,f1
	ctx.f1.f64 = double(float(ctx.f10.f64 * ctx.f4.f64 + ctx.f1.f64));
	// fabs f13,f6
	ctx.f13.u64 = ctx.f6.u64 & ~0x8000000000000000;
	// fabs f10,f24
	ctx.f10.u64 = ctx.f24.u64 & ~0x8000000000000000;
	// fmuls f9,f29,f3
	ctx.f9.f64 = double(float(ctx.f29.f64 * ctx.f3.f64));
	// fmadds f4,f8,f12,f2
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f12.f64 + ctx.f2.f64));
	// fmadds f2,f29,f12,f0
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f12.f64 + ctx.f0.f64));
	// fadds f6,f25,f26
	ctx.f6.f64 = double(float(ctx.f25.f64 + ctx.f26.f64));
	// fmadds f3,f31,f12,f1
	ctx.f3.f64 = double(float(ctx.f31.f64 * ctx.f12.f64 + ctx.f1.f64));
	// fadds f1,f13,f11
	ctx.f1.f64 = double(float(ctx.f13.f64 + ctx.f11.f64));
	// fabs f8,f30
	ctx.f8.u64 = ctx.f30.u64 & ~0x8000000000000000;
	// fadds f13,f27,f4
	ctx.f13.f64 = double(float(ctx.f27.f64 + ctx.f4.f64));
	// fadds f11,f7,f2
	ctx.f11.f64 = double(float(ctx.f7.f64 + ctx.f2.f64));
	// fadds f0,f6,f10
	ctx.f0.f64 = double(float(ctx.f6.f64 + ctx.f10.f64));
	// fadds f12,f28,f3
	ctx.f12.f64 = double(float(ctx.f28.f64 + ctx.f3.f64));
	// fabs f10,f5
	ctx.f10.u64 = ctx.f5.u64 & ~0x8000000000000000;
	// fabs f7,f23
	ctx.f7.u64 = ctx.f23.u64 & ~0x8000000000000000;
	// fabs f6,f9
	ctx.f6.u64 = ctx.f9.u64 & ~0x8000000000000000;
	// fadds f3,f10,f8
	ctx.f3.f64 = double(float(ctx.f10.f64 + ctx.f8.f64));
	// fadds f2,f1,f7
	ctx.f2.f64 = double(float(ctx.f1.f64 + ctx.f7.f64));
	// fsubs f5,f13,f0
	ctx.f5.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// stfs f5,0(r4)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fadds f4,f0,f13
	ctx.f4.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// fadds f1,f3,f6
	ctx.f1.f64 = double(float(ctx.f3.f64 + ctx.f6.f64));
	// fsubs f0,f12,f2
	ctx.f0.f64 = double(float(ctx.f12.f64 - ctx.f2.f64));
	// stfs f0,4(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// fadds f13,f12,f2
	ctx.f13.f64 = double(float(ctx.f12.f64 + ctx.f2.f64));
	// fsubs f12,f11,f1
	ctx.f12.f64 = double(float(ctx.f11.f64 - ctx.f1.f64));
	// stfs f12,8(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// fadds f11,f11,f1
	ctx.f11.f64 = double(float(ctx.f11.f64 + ctx.f1.f64));
	// stfs f4,12(r4)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// stfs f13,16(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// stfs f11,20(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6b00
	ctx.lr = 0x830C45EC;
	__restfpr_15(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C45F8"))) PPC_WEAK_FUNC(sub_830C45F8);
PPC_FUNC_IMPL(__imp__sub_830C45F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6ac0
	ctx.lr = 0x830C4608;
	__savefpr_18(ctx, base);
	// lwz r11,336(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	// lfs f0,136(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 136);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,0(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// lfs f13,140(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// lfs f12,144(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 144);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,8(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// lfs f11,148(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 148);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,12(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c483c
	if (ctx.cr6.eq) goto loc_830C483C;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c483c
	if (ctx.cr6.eq) goto loc_830C483C;
	// lfs f11,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f11.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f10,124(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 124);
	ctx.f10.f64 = double(temp.f32);
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// fmuls f8,f10,f11
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
	// lfs f4,120(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 120);
	ctx.f4.f64 = double(temp.f32);
	// lfs f7,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f1,f4,f11
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// lfs f2,112(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 112);
	ctx.f2.f64 = double(temp.f32);
	// fmr f5,f7
	ctx.f5.f64 = ctx.f7.f64;
	// lfs f31,116(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 116);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f30,f2,f7
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f7.f64));
	// lfs f6,252(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f29,f31,f7
	ctx.f29.f64 = double(float(ctx.f31.f64 * ctx.f7.f64));
	// fmr f3,f6
	ctx.f3.f64 = ctx.f6.f64;
	// lfs f28,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,136(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	ctx.f27.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f12,6380(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6380);
	ctx.f12.f64 = double(temp.f32);
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f26,128(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 128);
	ctx.f26.f64 = double(temp.f32);
	// fmsubs f12,f28,f28,f12
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f28.f64 - ctx.f12.f64));
	// lfs f25,132(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 132);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f24,f26,f9
	ctx.f24.f64 = double(float(ctx.f26.f64 * ctx.f9.f64));
	// fmadds f8,f2,f6,f8
	ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f6.f64 + ctx.f8.f64));
	// addi r10,r3,112
	ctx.r10.s64 = ctx.r3.s64 + 112;
	// fmadds f1,f10,f7,f1
	ctx.f1.f64 = double(float(ctx.f10.f64 * ctx.f7.f64 + ctx.f1.f64));
	// lfs f0,7676(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f23,f27,f5
	ctx.f23.f64 = double(float(ctx.f27.f64 * ctx.f5.f64));
	// lfs f13,6140(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 6140);
	ctx.f13.f64 = double(temp.f32);
	// fmsubs f30,f10,f28,f30
	ctx.f30.f64 = double(float(ctx.f10.f64 * ctx.f28.f64 - ctx.f30.f64));
	// lfs f21,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f10,f10,f6,f29
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f6.f64 + ctx.f29.f64));
	// lfs f20,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f22,f27,f3
	ctx.f22.f64 = double(float(ctx.f27.f64 * ctx.f3.f64));
	// lfs f29,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f19,f25,f3
	ctx.f19.f64 = double(float(ctx.f25.f64 * ctx.f3.f64));
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// addi r9,r3,12
	ctx.r9.s64 = ctx.r3.s64 + 12;
	// fmuls f18,f27,f12
	ctx.f18.f64 = double(float(ctx.f27.f64 * ctx.f12.f64));
	// fmsubs f24,f25,f5,f24
	ctx.f24.f64 = double(float(ctx.f25.f64 * ctx.f5.f64 - ctx.f24.f64));
	// fmadds f8,f31,f28,f8
	ctx.f8.f64 = double(float(ctx.f31.f64 * ctx.f28.f64 + ctx.f8.f64));
	// fmadds f1,f2,f28,f1
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f28.f64 + ctx.f1.f64));
	// fmsubs f23,f26,f3,f23
	ctx.f23.f64 = double(float(ctx.f26.f64 * ctx.f3.f64 - ctx.f23.f64));
	// fnmsubs f30,f31,f11,f30
	ctx.f30.f64 = double(float(-(ctx.f31.f64 * ctx.f11.f64 - ctx.f30.f64)));
	// fmadds f10,f4,f28,f10
	ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f28.f64 + ctx.f10.f64));
	// fmadds f22,f25,f9,f22
	ctx.f22.f64 = double(float(ctx.f25.f64 * ctx.f9.f64 + ctx.f22.f64));
	// fmsubs f27,f27,f9,f19
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f9.f64 - ctx.f19.f64));
	// fmuls f25,f25,f12
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f12.f64));
	// fmuls f24,f24,f28
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f28.f64));
	// fnmsubs f8,f4,f7,f8
	ctx.f8.f64 = double(float(-(ctx.f4.f64 * ctx.f7.f64 - ctx.f8.f64)));
	// fnmsubs f1,f31,f6,f1
	ctx.f1.f64 = double(float(-(ctx.f31.f64 * ctx.f6.f64 - ctx.f1.f64)));
	// fmuls f7,f23,f28
	ctx.f7.f64 = double(float(ctx.f23.f64 * ctx.f28.f64));
	// fnmsubs f6,f4,f6,f30
	ctx.f6.f64 = double(float(-(ctx.f4.f64 * ctx.f6.f64 - ctx.f30.f64)));
	// fnmsubs f4,f2,f11,f10
	ctx.f4.f64 = double(float(-(ctx.f2.f64 * ctx.f11.f64 - ctx.f10.f64)));
	// fmadds f23,f26,f5,f22
	ctx.f23.f64 = double(float(ctx.f26.f64 * ctx.f5.f64 + ctx.f22.f64));
	// fmuls f2,f26,f12
	ctx.f2.f64 = double(float(ctx.f26.f64 * ctx.f12.f64));
	// fmuls f12,f27,f28
	ctx.f12.f64 = double(float(ctx.f27.f64 * ctx.f28.f64));
	// fadds f11,f18,f24
	ctx.f11.f64 = double(float(ctx.f18.f64 + ctx.f24.f64));
	// fmuls f10,f8,f8
	ctx.f10.f64 = double(float(ctx.f8.f64 * ctx.f8.f64));
	// fmuls f31,f8,f1
	ctx.f31.f64 = double(float(ctx.f8.f64 * ctx.f1.f64));
	// fadds f7,f25,f7
	ctx.f7.f64 = double(float(ctx.f25.f64 + ctx.f7.f64));
	// fmuls f30,f4,f4
	ctx.f30.f64 = double(float(ctx.f4.f64 * ctx.f4.f64));
	// fmuls f9,f9,f23
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f23.f64));
	// fmuls f3,f3,f23
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f23.f64));
	// fadds f2,f2,f12
	ctx.f2.f64 = double(float(ctx.f2.f64 + ctx.f12.f64));
	// fmuls f28,f6,f4
	ctx.f28.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// fmuls f5,f23,f5
	ctx.f5.f64 = double(float(ctx.f23.f64 * ctx.f5.f64));
	// fmuls f12,f10,f0
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fadds f10,f7,f9
	ctx.f10.f64 = double(float(ctx.f7.f64 + ctx.f9.f64));
	// fmuls f9,f31,f0
	ctx.f9.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fadds f7,f11,f3
	ctx.f7.f64 = double(float(ctx.f11.f64 + ctx.f3.f64));
	// fmuls f3,f30,f0
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmuls f11,f28,f0
	ctx.f11.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// fsubs f31,f13,f12
	ctx.f31.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// fsubs f30,f9,f11
	ctx.f30.f64 = double(float(ctx.f9.f64 - ctx.f11.f64));
	// stfs f30,-156(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + -156, temp.u32);
	// fsubs f31,f31,f3
	ctx.f31.f64 = double(float(ctx.f31.f64 - ctx.f3.f64));
	// stfs f31,-160(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + -160, temp.u32);
	// fmuls f31,f4,f1
	ctx.f31.f64 = double(float(ctx.f4.f64 * ctx.f1.f64));
	// fmuls f30,f6,f8
	ctx.f30.f64 = double(float(ctx.f6.f64 * ctx.f8.f64));
	// fmuls f28,f1,f1
	ctx.f28.f64 = double(float(ctx.f1.f64 * ctx.f1.f64));
	// addi r11,r1,-160
	ctx.r11.s64 = ctx.r1.s64 + -160;
	// fmuls f6,f6,f1
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f1.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f8,f4,f8
	ctx.f8.f64 = double(float(ctx.f4.f64 * ctx.f8.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fadds f5,f2,f5
	ctx.f5.f64 = double(float(ctx.f2.f64 + ctx.f5.f64));
	// fmuls f4,f7,f0
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fadds f1,f11,f9
	ctx.f1.f64 = double(float(ctx.f11.f64 + ctx.f9.f64));
	// stfs f1,-148(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -148, temp.u32);
	// fmuls f2,f10,f0
	ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f11,f31,f0
	ctx.f11.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fmuls f10,f30,f0
	ctx.f10.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fnmsubs f9,f28,f0,f13
	ctx.f9.f64 = double(float(-(ctx.f28.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f7,f6,f0
	ctx.f7.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmuls f6,f5,f0
	ctx.f6.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fadds f13,f29,f4
	ctx.f13.f64 = double(float(ctx.f29.f64 + ctx.f4.f64));
	// fadds f0,f20,f2
	ctx.f0.f64 = double(float(ctx.f20.f64 + ctx.f2.f64));
	// fadds f5,f10,f11
	ctx.f5.f64 = double(float(ctx.f10.f64 + ctx.f11.f64));
	// stfs f5,-152(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + -152, temp.u32);
	// fsubs f4,f9,f3
	ctx.f4.f64 = double(float(ctx.f9.f64 - ctx.f3.f64));
	// stfs f4,-144(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + -144, temp.u32);
	// fsubs f12,f9,f12
	ctx.f12.f64 = double(float(ctx.f9.f64 - ctx.f12.f64));
	// stfs f12,-128(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -128, temp.u32);
	// fsubs f3,f11,f10
	ctx.f3.f64 = double(float(ctx.f11.f64 - ctx.f10.f64));
	// stfs f3,-136(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + -136, temp.u32);
	// fsubs f2,f8,f7
	ctx.f2.f64 = double(float(ctx.f8.f64 - ctx.f7.f64));
	// stfs f2,-140(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + -140, temp.u32);
	// fadds f1,f7,f8
	ctx.f1.f64 = double(float(ctx.f7.f64 + ctx.f8.f64));
	// stfs f1,-132(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -132, temp.u32);
	// fadds f12,f21,f6
	ctx.f12.f64 = double(float(ctx.f21.f64 + ctx.f6.f64));
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830C4810:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c4810
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C4810;
	// stfs f12,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f0,40(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f13,44(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
loc_830C483C:
	// lfs f0,8(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r3,12
	ctx.r11.s64 = ctx.r3.s64 + 12;
	// lfs f12,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f10,f12,f0
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f11,28(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f11,f13
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fmuls f7,f9,f13
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// lfs f5,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,24(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	ctx.f4.f64 = double(temp.f32);
	// lfs f6,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// lfs f3,36(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,12(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,32(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	ctx.f1.f64 = double(temp.f32);
	// lfs f12,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f10,f5,f13,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f13.f64 + ctx.f10.f64));
	// lfs f11,48(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f8,f4,f6,f8
	ctx.f8.f64 = double(float(ctx.f4.f64 * ctx.f6.f64 + ctx.f8.f64));
	// lfs f5,56(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f4,f3,f6,f7
	ctx.f4.f64 = double(float(ctx.f3.f64 * ctx.f6.f64 + ctx.f7.f64));
	// fmadds f3,f6,f2,f10
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f2.f64 + ctx.f10.f64));
	// fmadds f2,f1,f0,f8
	ctx.f2.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 + ctx.f8.f64));
	// fmadds f1,f12,f0,f4
	ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 + ctx.f4.f64));
	// fadds f0,f3,f11
	ctx.f0.f64 = double(float(ctx.f3.f64 + ctx.f11.f64));
	// stfs f0,0(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fadds f13,f9,f2
	ctx.f13.f64 = double(float(ctx.f9.f64 + ctx.f2.f64));
	// stfs f13,4(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// fadds f12,f5,f1
	ctx.f12.f64 = double(float(ctx.f5.f64 + ctx.f1.f64));
	// stfs f12,8(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6b0c
	ctx.lr = 0x830C48C0;
	__restfpr_18(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C48CC"))) PPC_WEAK_FUNC(sub_830C48CC);
PPC_FUNC_IMPL(__imp__sub_830C48CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C48D0"))) PPC_WEAK_FUNC(sub_830C48D0);
PPC_FUNC_IMPL(__imp__sub_830C48D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10bc
	ctx.lr = 0x830C48D8;
	__savegprlr_17(ctx, base);
	// addi r12,r1,-128
	ctx.r12.s64 = ctx.r1.s64 + -128;
	// bl 0x82cb6ac0
	ctx.lr = 0x830C48E0;
	__savefpr_18(ctx, base);
	// stwu r1,-496(r1)
	ea = -496 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r21,r3
	ctx.r21.u64 = ctx.r3.u64;
	// mr r19,r4
	ctx.r19.u64 = ctx.r4.u64;
	// lwz r11,312(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 312);
	// rlwinm r10,r11,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x8;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x830c506c
	if (ctx.cr6.eq) goto loc_830C506C;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lwz r11,264(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 264);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f26,6140(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6140);
	ctx.f26.f64 = double(temp.f32);
	// beq cr6,0x830c4b0c
	if (ctx.cr6.eq) goto loc_830C4B0C;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r21)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r21.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c4b0c
	if (ctx.cr6.eq) goto loc_830C4B0C;
	// lfs f12,252(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f12.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f11,112(r21)
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + 112);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f12
	ctx.f10.f64 = ctx.f12.f64;
	// fmuls f9,f11,f12
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f7,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,124(r21)
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + 124);
	ctx.f6.f64 = double(temp.f32);
	// fmr f31,f7
	ctx.f31.f64 = ctx.f7.f64;
	// lfs f2,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f3,f6,f7
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// lfs f8,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f1,f6,f12
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// lfs f4,116(r21)
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + 116);
	ctx.f4.f64 = double(temp.f32);
	// fmr f29,f2
	ctx.f29.f64 = ctx.f2.f64;
	// fmuls f5,f11,f7
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// lfs f30,120(r21)
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + 120);
	ctx.f30.f64 = double(temp.f32);
	// lfs f28,132(r21)
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + 132);
	ctx.f28.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f27,136(r21)
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + 136);
	ctx.f27.f64 = double(temp.f32);
	// addi r10,r21,112
	ctx.r10.s64 = ctx.r21.s64 + 112;
	// lfs f13,6380(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6380);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// fmuls f24,f28,f10
	ctx.f24.f64 = double(float(ctx.f28.f64 * ctx.f10.f64));
	// lfs f25,128(r21)
	temp.u32 = PPC_LOAD_U32(ctx.r21.u32 + 128);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f9,f4,f8,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f9.f64));
	// lfs f23,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f20,f27,f31
	ctx.f20.f64 = double(float(ctx.f27.f64 * ctx.f31.f64));
	// lfs f0,7676(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f3,f11,f8,f3
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f8.f64 + ctx.f3.f64));
	// lfs f22,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f1,f30,f8,f1
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f8.f64 + ctx.f1.f64));
	// lfs f21,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f19,f28,f29
	ctx.f19.f64 = double(float(ctx.f28.f64 * ctx.f29.f64));
	// addi r9,r21,12
	ctx.r9.s64 = ctx.r21.s64 + 12;
	// fmsubs f5,f6,f8,f5
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f8.f64 - ctx.f5.f64));
	// fmsubs f13,f8,f8,f13
	ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f8.f64 - ctx.f13.f64));
	// fmuls f18,f25,f29
	ctx.f18.f64 = double(float(ctx.f25.f64 * ctx.f29.f64));
	// fmsubs f24,f27,f29,f24
	ctx.f24.f64 = double(float(ctx.f27.f64 * ctx.f29.f64 - ctx.f24.f64));
	// fmadds f9,f6,f2,f9
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f2.f64 + ctx.f9.f64));
	// fmsubs f20,f25,f10,f20
	ctx.f20.f64 = double(float(ctx.f25.f64 * ctx.f10.f64 - ctx.f20.f64));
	// fmadds f3,f30,f2,f3
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f2.f64 + ctx.f3.f64));
	// fmadds f1,f4,f7,f1
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f7.f64 + ctx.f1.f64));
	// fmadds f19,f27,f10,f19
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f10.f64 + ctx.f19.f64));
	// fnmsubs f5,f4,f2,f5
	ctx.f5.f64 = double(float(-(ctx.f4.f64 * ctx.f2.f64 - ctx.f5.f64)));
	// fmuls f6,f28,f13
	ctx.f6.f64 = double(float(ctx.f28.f64 * ctx.f13.f64));
	// fmsubs f28,f28,f31,f18
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f31.f64 - ctx.f18.f64));
	// fmuls f18,f25,f13
	ctx.f18.f64 = double(float(ctx.f25.f64 * ctx.f13.f64));
	// fmuls f13,f27,f13
	ctx.f13.f64 = double(float(ctx.f27.f64 * ctx.f13.f64));
	// fnmsubs f9,f30,f7,f9
	ctx.f9.f64 = double(float(-(ctx.f30.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// fmuls f7,f24,f8
	ctx.f7.f64 = double(float(ctx.f24.f64 * ctx.f8.f64));
	// fnmsubs f4,f4,f12,f3
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f12.f64 - ctx.f3.f64)));
	// fnmsubs f3,f11,f2,f1
	ctx.f3.f64 = double(float(-(ctx.f11.f64 * ctx.f2.f64 - ctx.f1.f64)));
	// fmuls f2,f20,f8
	ctx.f2.f64 = double(float(ctx.f20.f64 * ctx.f8.f64));
	// fnmsubs f5,f30,f12,f5
	ctx.f5.f64 = double(float(-(ctx.f30.f64 * ctx.f12.f64 - ctx.f5.f64)));
	// fmadds f1,f25,f31,f19
	ctx.f1.f64 = double(float(ctx.f25.f64 * ctx.f31.f64 + ctx.f19.f64));
	// fmuls f12,f28,f8
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f8.f64));
	// fmuls f11,f9,f9
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f9.f64));
	// fadds f8,f18,f7
	ctx.f8.f64 = double(float(ctx.f18.f64 + ctx.f7.f64));
	// fmuls f7,f9,f4
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// fmuls f30,f3,f3
	ctx.f30.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// fadds f6,f6,f2
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f2.f64));
	// fmuls f28,f5,f3
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fmuls f29,f29,f1
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f1.f64));
	// fmuls f2,f1,f31
	ctx.f2.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// fmuls f1,f10,f1
	ctx.f1.f64 = double(float(ctx.f10.f64 * ctx.f1.f64));
	// fadds f10,f13,f12
	ctx.f10.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fmuls f31,f30,f0
	ctx.f31.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmuls f13,f28,f0
	ctx.f13.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// fadds f12,f6,f29
	ctx.f12.f64 = double(float(ctx.f6.f64 + ctx.f29.f64));
	// fadds f8,f8,f2
	ctx.f8.f64 = double(float(ctx.f8.f64 + ctx.f2.f64));
	// fadds f2,f10,f1
	ctx.f2.f64 = double(float(ctx.f10.f64 + ctx.f1.f64));
	// fsubs f6,f26,f11
	ctx.f6.f64 = double(float(ctx.f26.f64 - ctx.f11.f64));
	// fsubs f1,f7,f13
	ctx.f1.f64 = double(float(ctx.f7.f64 - ctx.f13.f64));
	// stfs f1,212(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 212, temp.u32);
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f1,f3,f4
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f4.f64));
	// fsubs f10,f6,f31
	ctx.f10.f64 = double(float(ctx.f6.f64 - ctx.f31.f64));
	// stfs f10,208(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 208, temp.u32);
	// fmuls f6,f5,f9
	ctx.f6.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// fmuls f10,f4,f4
	ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f4.f64));
	// addi r11,r1,208
	ctx.r11.s64 = ctx.r1.s64 + 208;
	// fmuls f5,f5,f4
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f9,f3,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f9.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fmuls f3,f2,f0
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmuls f4,f8,f0
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fadds f2,f13,f7
	ctx.f2.f64 = double(float(ctx.f13.f64 + ctx.f7.f64));
	// stfs f2,220(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fmuls f7,f1,f0
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f8,f6,f0
	ctx.f8.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fadds f13,f22,f12
	ctx.f13.f64 = double(float(ctx.f22.f64 + ctx.f12.f64));
	// fnmsubs f6,f10,f0,f26
	ctx.f6.f64 = double(float(-(ctx.f10.f64 * ctx.f0.f64 - ctx.f26.f64)));
	// fmuls f1,f5,f0
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fmuls f2,f9,f0
	ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fadds f0,f21,f3
	ctx.f0.f64 = double(float(ctx.f21.f64 + ctx.f3.f64));
	// fadds f12,f23,f4
	ctx.f12.f64 = double(float(ctx.f23.f64 + ctx.f4.f64));
	// fadds f10,f7,f8
	ctx.f10.f64 = double(float(ctx.f7.f64 + ctx.f8.f64));
	// stfs f10,216(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 216, temp.u32);
	// fsubs f8,f7,f8
	ctx.f8.f64 = double(float(ctx.f7.f64 - ctx.f8.f64));
	// stfs f8,232(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 232, temp.u32);
	// fsubs f9,f6,f31
	ctx.f9.f64 = double(float(ctx.f6.f64 - ctx.f31.f64));
	// stfs f9,224(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 224, temp.u32);
	// fsubs f4,f6,f11
	ctx.f4.f64 = double(float(ctx.f6.f64 - ctx.f11.f64));
	// stfs f4,240(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 240, temp.u32);
	// fsubs f7,f2,f1
	ctx.f7.f64 = double(float(ctx.f2.f64 - ctx.f1.f64));
	// stfs f7,228(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 228, temp.u32);
	// fadds f5,f1,f2
	ctx.f5.f64 = double(float(ctx.f1.f64 + ctx.f2.f64));
	// stfs f5,236(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 236, temp.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830C4AE0:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c4ae0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C4AE0;
	// stfs f12,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f13,40(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f0,44(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r21)
	PPC_STORE_U32(ctx.r21.u32 + 8, ctx.r10.u32);
loc_830C4B0C:
	// mr r4,r19
	ctx.r4.u64 = ctx.r19.u64;
	// mr r3,r21
	ctx.r3.u64 = ctx.r21.u64;
	// addi r31,r21,12
	ctx.r31.s64 = ctx.r21.s64 + 12;
	// bl 0x83089320
	ctx.lr = 0x830C4B1C;
	sub_83089320(ctx, base);
	// lis r11,-31890
	ctx.r11.s64 = -2089943040;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// addi r23,r11,22552
	ctx.r23.s64 = ctx.r11.s64 + 22552;
	// lfs f0,36(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 36);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,168(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 168);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,172(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 172);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f25,f13,f0
	ctx.f25.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fmuls f28,f12,f0
	ctx.f28.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lwz r11,336(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 336);
	// addic. r28,r11,156
	ctx.xer.ca = ctx.r11.u32 > 4294967139;
	ctx.r28.s64 = ctx.r11.s64 + 156;
	ctx.cr0.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// beq 0x830c4f44
	if (ctx.cr0.eq) goto loc_830C4F44;
	// lwz r11,36(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 36);
	// lfs f27,6048(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6048);
	ctx.f27.f64 = double(temp.f32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c4e0c
	if (ctx.cr6.eq) goto loc_830C4E0C;
	// mr r18,r11
	ctx.r18.u64 = ctx.r11.u64;
	// lis r11,255
	ctx.r11.s64 = 16711680;
	// li r20,0
	ctx.r20.s64 = 0;
	// ori r17,r11,32832
	ctx.r17.u64 = ctx.r11.u64 | 32832;
loc_830C4B68:
	// lwz r11,40(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 40);
	// fmr f31,f27
	ctx.fpscr.disableFlushMode();
	ctx.f31.f64 = ctx.f27.f64;
	// fmr f30,f27
	ctx.f30.f64 = ctx.f27.f64;
	// add r11,r20,r11
	ctx.r11.u64 = ctx.r20.u64 + ctx.r11.u64;
	// fmr f29,f27
	ctx.f29.f64 = ctx.f27.f64;
	// lhz r27,0(r11)
	ctx.r27.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// lwz r25,4(r11)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// beq cr6,0x830c4cf0
	if (ctx.cr6.eq) goto loc_830C4CF0;
	// addi r30,r31,36
	ctx.r30.s64 = ctx.r31.s64 + 36;
	// li r29,1
	ctx.r29.s64 = 1;
	// addi r24,r25,-1
	ctx.r24.s64 = ctx.r25.s64 + -1;
	// mr r22,r27
	ctx.r22.u64 = ctx.r27.u64;
loc_830C4B9C:
	// lbzx r11,r24,r29
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r24.u32 + ctx.r29.u32);
	// lfs f0,4(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// lwz r9,16(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 16);
	// lfs f13,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// rotlwi r10,r11,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r11.u32, 1);
	// lfs f12,28(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	ctx.f12.f64 = double(temp.f32);
	// lfs f23,156(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 156);
	ctx.f23.f64 = double(temp.f32);
	// divwu r8,r29,r27
	ctx.r8.u32 = ctx.r29.u32 / ctx.r27.u32;
	// add r7,r11,r10
	ctx.r7.u64 = ctx.r11.u64 + ctx.r10.u64;
	// fcmpu cr6,f23,f27
	ctx.cr6.compare(ctx.f23.f64, ctx.f27.f64);
	// lfs f11,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// mullw r6,r8,r27
	ctx.r6.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r27.s32);
	// lfs f10,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,32(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f4.f64 = double(temp.f32);
	// fmr f2,f5
	ctx.f2.f64 = ctx.f5.f64;
	// lfs f3,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// fmr f1,f4
	ctx.f1.f64 = ctx.f4.f64;
	// fmr f24,f3
	ctx.f24.f64 = ctx.f3.f64;
	// rlwinm r26,r7,2,0,29
	ctx.r26.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r5,r6,r29
	ctx.r5.s64 = ctx.r29.s64 - ctx.r6.s64;
	// add r11,r26,r9
	ctx.r11.u64 = ctx.r26.u64 + ctx.r9.u64;
	// lfsx f23,r26,r9
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + ctx.r9.u32);
	ctx.f23.f64 = double(temp.f32);
	// lbzx r10,r5,r25
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r5.u32 + ctx.r25.u32);
	// lfs f22,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f21,f0,f22
	ctx.f21.f64 = double(float(ctx.f0.f64 * ctx.f22.f64));
	// lfs f20,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f19,f13,f22
	ctx.f19.f64 = double(float(ctx.f13.f64 * ctx.f22.f64));
	// rotlwi r11,r10,1
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// fmuls f22,f12,f22
	ctx.f22.f64 = double(float(ctx.f12.f64 * ctx.f22.f64));
	// add r4,r10,r11
	ctx.r4.u64 = ctx.r10.u64 + ctx.r11.u64;
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// fmadds f21,f11,f23,f21
	ctx.f21.f64 = double(float(ctx.f11.f64 * ctx.f23.f64 + ctx.f21.f64));
	// fmadds f19,f10,f23,f19
	ctx.f19.f64 = double(float(ctx.f10.f64 * ctx.f23.f64 + ctx.f19.f64));
	// fmadds f23,f9,f23,f22
	ctx.f23.f64 = double(float(ctx.f9.f64 * ctx.f23.f64 + ctx.f22.f64));
	// fmadds f22,f8,f20,f21
	ctx.f22.f64 = double(float(ctx.f8.f64 * ctx.f20.f64 + ctx.f21.f64));
	// fmadds f21,f7,f20,f19
	ctx.f21.f64 = double(float(ctx.f7.f64 * ctx.f20.f64 + ctx.f19.f64));
	// fmadds f23,f6,f20,f23
	ctx.f23.f64 = double(float(ctx.f6.f64 * ctx.f20.f64 + ctx.f23.f64));
	// fadds f5,f5,f22
	ctx.f5.f64 = double(float(ctx.f5.f64 + ctx.f22.f64));
	// stfs f5,176(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// fadds f4,f4,f21
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f21.f64));
	// stfs f4,180(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fadds f3,f3,f23
	ctx.f3.f64 = double(float(ctx.f3.f64 + ctx.f23.f64));
	// stfs f3,184(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// lwz r11,16(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 16);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lfs f5,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f0,f0,f3
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f3.f64));
	// fmuls f13,f13,f3
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f3.f64));
	// fmuls f12,f12,f3
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f3.f64));
	// fmadds f11,f11,f5,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f5.f64 + ctx.f0.f64));
	// fmadds f10,f10,f5,f13
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f5.f64 + ctx.f13.f64));
	// fmadds f9,f9,f5,f12
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f5.f64 + ctx.f12.f64));
	// fmadds f8,f8,f4,f11
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f4.f64 + ctx.f11.f64));
	// fmadds f7,f7,f4,f10
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f4.f64 + ctx.f10.f64));
	// fmadds f6,f6,f4,f9
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f4.f64 + ctx.f9.f64));
	// fadds f5,f2,f8
	ctx.f5.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f5,112(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fadds f4,f7,f1
	ctx.f4.f64 = double(float(ctx.f7.f64 + ctx.f1.f64));
	// stfs f4,116(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fadds f3,f6,f24
	ctx.f3.f64 = double(float(ctx.f6.f64 + ctx.f24.f64));
	// stfs f3,120(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// beq cr6,0x830c4cc4
	if (ctx.cr6.eq) goto loc_830C4CC4;
	// mr r6,r17
	ctx.r6.u64 = ctx.r17.u64;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// mr r3,r19
	ctx.r3.u64 = ctx.r19.u64;
	// bl 0x831bfb50
	ctx.lr = 0x830C4CC4;
	sub_831BFB50(ctx, base);
loc_830C4CC4:
	// lwz r11,16(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 16);
	// addic. r22,r22,-1
	ctx.xer.ca = ctx.r22.u32 > 0;
	ctx.r22.s64 = ctx.r22.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r22.s32, 0, ctx.xer);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// add r11,r26,r11
	ctx.r11.u64 = ctx.r26.u64 + ctx.r11.u64;
	// lfs f0,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fadds f31,f0,f31
	ctx.f31.f64 = double(float(ctx.f0.f64 + ctx.f31.f64));
	// lfs f12,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fadds f30,f13,f30
	ctx.f30.f64 = double(float(ctx.f13.f64 + ctx.f30.f64));
	// fadds f29,f12,f29
	ctx.f29.f64 = double(float(ctx.f12.f64 + ctx.f29.f64));
	// bne 0x830c4b9c
	if (!ctx.cr0.eq) goto loc_830C4B9C;
loc_830C4CF0:
	// fcmpu cr6,f28,f27
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f28.f64, ctx.f27.f64);
	// beq cr6,0x830c4e00
	if (ctx.cr6.eq) goto loc_830C4E00;
	// clrldi r11,r27,32
	ctx.r11.u64 = ctx.r27.u64 & 0xFFFFFFFF;
	// lfs f13,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// lis r6,64
	ctx.r6.s64 = 4194304;
	// std r11,248(r1)
	PPC_STORE_U64(ctx.r1.u32 + 248, ctx.r11.u64);
	// lfd f2,248(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + 248);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// lfs f12,32(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	ctx.f12.f64 = double(temp.f32);
	// frsp f2,f1
	ctx.f2.f64 = double(float(ctx.f1.f64));
	// lfs f11,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// ori r6,r6,33023
	ctx.r6.u64 = ctx.r6.u64 | 33023;
	// lfs f9,28(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	ctx.f9.f64 = double(temp.f32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lfs f8,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// lfs f7,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// mr r3,r19
	ctx.r3.u64 = ctx.r19.u64;
	// lfs f6,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,36(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,40(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,44(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	ctx.f3.f64 = double(temp.f32);
	// fdivs f1,f26,f2
	ctx.f1.f64 = double(float(ctx.f26.f64 / ctx.f2.f64));
	// fmuls f2,f29,f1
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f1.f64));
	// fmuls f30,f30,f1
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f1.f64));
	// fmuls f1,f31,f1
	ctx.f1.f64 = double(float(ctx.f31.f64 * ctx.f1.f64));
	// fmuls f31,f2,f0
	ctx.f31.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmuls f29,f2,f13
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// fmuls f2,f2,f12
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// fmadds f31,f30,f11,f31
	ctx.f31.f64 = double(float(ctx.f30.f64 * ctx.f11.f64 + ctx.f31.f64));
	// fmadds f29,f30,f10,f29
	ctx.f29.f64 = double(float(ctx.f30.f64 * ctx.f10.f64 + ctx.f29.f64));
	// fmadds f2,f30,f9,f2
	ctx.f2.f64 = double(float(ctx.f30.f64 * ctx.f9.f64 + ctx.f2.f64));
	// fmadds f31,f8,f1,f31
	ctx.f31.f64 = double(float(ctx.f8.f64 * ctx.f1.f64 + ctx.f31.f64));
	// fmadds f30,f1,f7,f29
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f7.f64 + ctx.f29.f64));
	// fmadds f1,f1,f6,f2
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f6.f64 + ctx.f2.f64));
	// fadds f5,f31,f5
	ctx.f5.f64 = double(float(ctx.f31.f64 + ctx.f5.f64));
	// stfs f5,144(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// fadds f4,f4,f30
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f30.f64));
	// stfs f4,148(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fadds f3,f3,f1
	ctx.f3.f64 = double(float(ctx.f3.f64 + ctx.f1.f64));
	// stfs f3,152(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// lwz r11,40(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 40);
	// add r11,r20,r11
	ctx.r11.u64 = ctx.r20.u64 + ctx.r11.u64;
	// lfs f2,20(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	ctx.f2.f64 = double(temp.f32);
	// addi r10,r11,12
	ctx.r10.s64 = ctx.r11.s64 + 12;
	// lfs f1,16(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,12(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f7,f31,f7
	ctx.f7.f64 = double(float(ctx.f31.f64 * ctx.f7.f64));
	// fmuls f0,f2,f0
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmuls f6,f31,f6
	ctx.f6.f64 = double(float(ctx.f31.f64 * ctx.f6.f64));
	// fmadds f10,f1,f10,f7
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f10.f64 + ctx.f7.f64));
	// fmadds f7,f1,f11,f0
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f11.f64 + ctx.f0.f64));
	// fmadds f6,f1,f9,f6
	ctx.f6.f64 = double(float(ctx.f1.f64 * ctx.f9.f64 + ctx.f6.f64));
	// fmadds f1,f2,f13,f10
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f13.f64 + ctx.f10.f64));
	// fmadds f0,f8,f31,f7
	ctx.f0.f64 = double(float(ctx.f8.f64 * ctx.f31.f64 + ctx.f7.f64));
	// fmadds f13,f2,f12,f6
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f12.f64 + ctx.f6.f64));
	// fmuls f12,f1,f28
	ctx.f12.f64 = double(float(ctx.f1.f64 * ctx.f28.f64));
	// fmuls f11,f0,f28
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f28.f64));
	// fmuls f10,f13,f28
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f28.f64));
	// fadds f9,f12,f4
	ctx.f9.f64 = double(float(ctx.f12.f64 + ctx.f4.f64));
	// stfs f9,84(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fadds f8,f11,f5
	ctx.f8.f64 = double(float(ctx.f11.f64 + ctx.f5.f64));
	// stfs f8,80(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fadds f7,f10,f3
	ctx.f7.f64 = double(float(ctx.f10.f64 + ctx.f3.f64));
	// stfs f7,88(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// bl 0x831bfb50
	ctx.lr = 0x830C4E00;
	sub_831BFB50(ctx, base);
loc_830C4E00:
	// addic. r18,r18,-1
	ctx.xer.ca = ctx.r18.u32 > 0;
	ctx.r18.s64 = ctx.r18.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r18.s32, 0, ctx.xer);
	// addi r20,r20,36
	ctx.r20.s64 = ctx.r20.s64 + 36;
	// bne 0x830c4b68
	if (!ctx.cr0.eq) goto loc_830C4B68;
loc_830C4E0C:
	// fcmpu cr6,f25,f27
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f25.f64, ctx.f27.f64);
	// beq cr6,0x830c506c
	if (ctx.cr6.eq) goto loc_830C506C;
	// lwz r11,336(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 336);
	// lwz r29,168(r11)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r11.u32 + 168);
	// lwz r11,176(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 176);
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x830c506c
	if (ctx.cr6.eq) goto loc_830C506C;
	// addi r28,r31,36
	ctx.r28.s64 = ctx.r31.s64 + 36;
	// addi r30,r11,4
	ctx.r30.s64 = ctx.r11.s64 + 4;
	// subfic r27,r11,-4
	ctx.xer.ca = ctx.r11.u32 <= 4294967292;
	ctx.r27.s64 = -4 - ctx.r11.s64;
	// li r26,-1
	ctx.r26.s64 = -1;
loc_830C4E38:
	// lwz r11,336(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 336);
	// lfs f0,28(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// mr r6,r26
	ctx.r6.u64 = ctx.r26.u64;
	// lfs f10,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// lfs f12,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f12.f64 = double(temp.f32);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// lfs f11,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// mr r3,r19
	ctx.r3.u64 = ctx.r19.u64;
	// lwz r11,172(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 172);
	// lfs f7,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// lfs f9,32(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	ctx.f9.f64 = double(temp.f32);
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
	// lfs f8,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f4,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f4.f64 = double(temp.f32);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// lfs f6,8(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,0(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// lfs f3,4(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 4);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// lfs f31,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f30,f2,f13
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// lfs f29,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f2,f2,f10
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f10.f64));
	// fmadds f1,f31,f12,f1
	ctx.f1.f64 = double(float(ctx.f31.f64 * ctx.f12.f64 + ctx.f1.f64));
	// fmadds f30,f11,f31,f30
	ctx.f30.f64 = double(float(ctx.f11.f64 * ctx.f31.f64 + ctx.f30.f64));
	// fmadds f2,f31,f7,f2
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f7.f64 + ctx.f2.f64));
	// fmadds f1,f29,f9,f1
	ctx.f1.f64 = double(float(ctx.f29.f64 * ctx.f9.f64 + ctx.f1.f64));
	// fmadds f31,f29,f8,f30
	ctx.f31.f64 = double(float(ctx.f29.f64 * ctx.f8.f64 + ctx.f30.f64));
	// fmadds f2,f29,f4,f2
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f4.f64 + ctx.f2.f64));
	// fadds f1,f1,f6
	ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f6.f64));
	// stfs f1,104(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fadds f6,f31,f5
	ctx.f6.f64 = double(float(ctx.f31.f64 + ctx.f5.f64));
	// stfs f6,96(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f5,f2,f3
	ctx.f5.f64 = double(float(ctx.f2.f64 + ctx.f3.f64));
	// stfs f5,100(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// lfs f3,-4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + -4);
	ctx.f3.f64 = double(temp.f32);
	// lfs f31,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f31.f64 = double(temp.f32);
	// lfs f2,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f8,f2,f8
	ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f8.f64));
	// fmuls f4,f2,f4
	ctx.f4.f64 = double(float(ctx.f2.f64 * ctx.f4.f64));
	// fmuls f2,f2,f9
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f9.f64));
	// fmadds f11,f11,f3,f8
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f3.f64 + ctx.f8.f64));
	// fmadds f9,f3,f7,f4
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f7.f64 + ctx.f4.f64));
	// fmadds f8,f3,f12,f2
	ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f12.f64 + ctx.f2.f64));
	// fmadds f7,f31,f13,f11
	ctx.f7.f64 = double(float(ctx.f31.f64 * ctx.f13.f64 + ctx.f11.f64));
	// fmadds f4,f31,f10,f9
	ctx.f4.f64 = double(float(ctx.f31.f64 * ctx.f10.f64 + ctx.f9.f64));
	// fmadds f3,f31,f0,f8
	ctx.f3.f64 = double(float(ctx.f31.f64 * ctx.f0.f64 + ctx.f8.f64));
	// fmuls f2,f7,f25
	ctx.f2.f64 = double(float(ctx.f7.f64 * ctx.f25.f64));
	// fmuls f0,f4,f25
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f25.f64));
	// fmuls f13,f3,f25
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f25.f64));
	// fadds f12,f2,f6
	ctx.f12.f64 = double(float(ctx.f2.f64 + ctx.f6.f64));
	// stfs f12,128(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// fadds f11,f0,f5
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f5.f64));
	// stfs f11,132(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 132, temp.u32);
	// fadds f10,f13,f1
	ctx.f10.f64 = double(float(ctx.f13.f64 + ctx.f1.f64));
	// stfs f10,136(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// bl 0x831bfb50
	ctx.lr = 0x830C4F28;
	sub_831BFB50(ctx, base);
	// addic. r29,r29,-1
	ctx.xer.ca = ctx.r29.u32 > 0;
	ctx.r29.s64 = ctx.r29.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// addi r30,r30,12
	ctx.r30.s64 = ctx.r30.s64 + 12;
	// bne 0x830c4e38
	if (!ctx.cr0.eq) goto loc_830C4E38;
	// addi r1,r1,496
	ctx.r1.s64 = ctx.r1.s64 + 496;
	// addi r12,r1,-128
	ctx.r12.s64 = ctx.r1.s64 + -128;
	// bl 0x82cb6b0c
	ctx.lr = 0x830C4F40;
	__restfpr_18(ctx, base);
	// b 0x82cb110c
	__restgprlr_17(ctx, base);
	return;
loc_830C4F44:
	// lfs f0,6048(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6048);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f25,f0
	ctx.cr6.compare(ctx.f25.f64, ctx.f0.f64);
	// beq cr6,0x830c506c
	if (ctx.cr6.eq) goto loc_830C506C;
	// lwz r29,168(r11)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r11.u32 + 168);
	// lwz r11,176(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 176);
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x830c506c
	if (ctx.cr6.eq) goto loc_830C506C;
	// addi r28,r31,36
	ctx.r28.s64 = ctx.r31.s64 + 36;
	// addi r30,r11,4
	ctx.r30.s64 = ctx.r11.s64 + 4;
	// subfic r27,r11,-4
	ctx.xer.ca = ctx.r11.u32 <= 4294967292;
	ctx.r27.s64 = -4 - ctx.r11.s64;
loc_830C4F6C:
	// lwz r11,336(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 336);
	// lfs f0,8(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f13.f64 = double(temp.f32);
	// lis r6,34
	ctx.r6.s64 = 2228224;
	// lfs f12,32(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	ctx.f12.f64 = double(temp.f32);
	// addi r5,r1,192
	ctx.r5.s64 = ctx.r1.s64 + 192;
	// lfs f11,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// ori r6,r6,26282
	ctx.r6.u64 = ctx.r6.u64 | 26282;
	// lfs f10,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// lwz r11,172(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 172);
	// lfs f9,28(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// mr r3,r19
	ctx.r3.u64 = ctx.r19.u64;
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// lfs f7,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f6.f64 = double(temp.f32);
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
	// lfs f5,0(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,4(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 4);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,8(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// lfs f31,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f30,f2,f13
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// lfs f29,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f2,f2,f12
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// fmadds f1,f31,f11,f1
	ctx.f1.f64 = double(float(ctx.f31.f64 * ctx.f11.f64 + ctx.f1.f64));
	// fmadds f30,f31,f10,f30
	ctx.f30.f64 = double(float(ctx.f31.f64 * ctx.f10.f64 + ctx.f30.f64));
	// fmadds f2,f31,f9,f2
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f9.f64 + ctx.f2.f64));
	// fmadds f1,f8,f29,f1
	ctx.f1.f64 = double(float(ctx.f8.f64 * ctx.f29.f64 + ctx.f1.f64));
	// fmadds f31,f29,f7,f30
	ctx.f31.f64 = double(float(ctx.f29.f64 * ctx.f7.f64 + ctx.f30.f64));
	// fmadds f2,f29,f6,f2
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f6.f64 + ctx.f2.f64));
	// fadds f1,f1,f5
	ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f5.f64));
	// stfs f1,160(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// fadds f5,f31,f4
	ctx.f5.f64 = double(float(ctx.f31.f64 + ctx.f4.f64));
	// stfs f5,164(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// fadds f4,f2,f3
	ctx.f4.f64 = double(float(ctx.f2.f64 + ctx.f3.f64));
	// stfs f4,168(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 168, temp.u32);
	// lfs f2,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f0,f2,f0
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmuls f13,f2,f13
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// lfs f31,-4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + -4);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f12,f2,f12
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// lfs f3,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f8,f31,f8,f0
	ctx.f8.f64 = double(float(ctx.f31.f64 * ctx.f8.f64 + ctx.f0.f64));
	// fmadds f7,f31,f7,f13
	ctx.f7.f64 = double(float(ctx.f31.f64 * ctx.f7.f64 + ctx.f13.f64));
	// fmadds f6,f31,f6,f12
	ctx.f6.f64 = double(float(ctx.f31.f64 * ctx.f6.f64 + ctx.f12.f64));
	// fmadds f2,f3,f11,f8
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f11.f64 + ctx.f8.f64));
	// fmadds f0,f3,f10,f7
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f10.f64 + ctx.f7.f64));
	// fmadds f13,f3,f9,f6
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f9.f64 + ctx.f6.f64));
	// fmuls f12,f2,f25
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f25.f64));
	// fmuls f11,f0,f25
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f25.f64));
	// fmuls f10,f13,f25
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f25.f64));
	// fadds f9,f12,f1
	ctx.f9.f64 = double(float(ctx.f12.f64 + ctx.f1.f64));
	// stfs f9,192(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 192, temp.u32);
	// fadds f8,f11,f5
	ctx.f8.f64 = double(float(ctx.f11.f64 + ctx.f5.f64));
	// stfs f8,196(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 196, temp.u32);
	// fadds f7,f10,f4
	ctx.f7.f64 = double(float(ctx.f10.f64 + ctx.f4.f64));
	// stfs f7,200(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 200, temp.u32);
	// bl 0x831bfb50
	ctx.lr = 0x830C5060;
	sub_831BFB50(ctx, base);
	// addic. r29,r29,-1
	ctx.xer.ca = ctx.r29.u32 > 0;
	ctx.r29.s64 = ctx.r29.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// addi r30,r30,12
	ctx.r30.s64 = ctx.r30.s64 + 12;
	// bne 0x830c4f6c
	if (!ctx.cr0.eq) goto loc_830C4F6C;
loc_830C506C:
	// addi r1,r1,496
	ctx.r1.s64 = ctx.r1.s64 + 496;
	// addi r12,r1,-128
	ctx.r12.s64 = ctx.r1.s64 + -128;
	// bl 0x82cb6b0c
	ctx.lr = 0x830C5078;
	__restfpr_18(ctx, base);
	// b 0x82cb110c
	__restgprlr_17(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C507C"))) PPC_WEAK_FUNC(sub_830C507C);
PPC_FUNC_IMPL(__imp__sub_830C507C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C5080"))) PPC_WEAK_FUNC(sub_830C5080);
PPC_FUNC_IMPL(__imp__sub_830C5080) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e0
	ctx.lr = 0x830C5088;
	__savegprlr_26(ctx, base);
	// addi r12,r1,-56
	ctx.r12.s64 = ctx.r1.s64 + -56;
	// bl 0x82cb6ab0
	ctx.lr = 0x830C5090;
	__savefpr_14(ctx, base);
	// stwu r1,-608(r1)
	ea = -608 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// fmr f27,f1
	ctx.fpscr.disableFlushMode();
	ctx.f27.f64 = ctx.f1.f64;
	// mr r29,r8
	ctx.r29.u64 = ctx.r8.u64;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lwz r11,264(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 264);
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// mr r26,r6
	ctx.r26.u64 = ctx.r6.u64;
	// lfs f30,6140(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6140);
	ctx.f30.f64 = double(temp.f32);
	// mr r27,r7
	ctx.r27.u64 = ctx.r7.u64;
	// lfs f31,7676(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 7676);
	ctx.f31.f64 = double(temp.f32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f28,6380(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 6380);
	ctx.f28.f64 = double(temp.f32);
	// stfs f30,136(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// beq cr6,0x830c52c0
	if (ctx.cr6.eq) goto loc_830C52C0;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c52c0
	if (ctx.cr6.eq) goto loc_830C52C0;
	// lfs f0,252(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r30,112
	ctx.r10.s64 = ctx.r30.s64 + 112;
	// lfs f13,112(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f11,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f10,f13,f0
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f9,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f9.f64 = double(temp.f32);
	// fmr f8,f11
	ctx.f8.f64 = ctx.f11.f64;
	// fmr f7,f9
	ctx.f7.f64 = ctx.f9.f64;
	// lfs f5,124(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 124);
	ctx.f5.f64 = double(temp.f32);
	// lfs f6,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f29,f5,f0
	ctx.f29.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f3,116(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 116);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f2,f5,f11
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// lfs f1,136(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 136);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f4,f13,f11
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// lfs f26,128(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 128);
	ctx.f26.f64 = double(temp.f32);
	// fmsubs f25,f6,f6,f28
	ctx.f25.f64 = double(float(ctx.f6.f64 * ctx.f6.f64 - ctx.f28.f64));
	// lfs f24,120(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 120);
	ctx.f24.f64 = double(temp.f32);
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// lfs f23,132(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 132);
	ctx.f23.f64 = double(temp.f32);
	// addi r9,r30,12
	ctx.r9.s64 = ctx.r30.s64 + 12;
	// fmuls f22,f12,f1
	ctx.f22.f64 = double(float(ctx.f12.f64 * ctx.f1.f64));
	// lfs f21,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f10,f3,f6,f10
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f6.f64 + ctx.f10.f64));
	// lfs f20,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f19,f1,f8
	ctx.f19.f64 = double(float(ctx.f1.f64 * ctx.f8.f64));
	// lfs f18,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f17,f26,f7
	ctx.f17.f64 = double(float(ctx.f26.f64 * ctx.f7.f64));
	// fmadds f29,f24,f6,f29
	ctx.f29.f64 = double(float(ctx.f24.f64 * ctx.f6.f64 + ctx.f29.f64));
	// fmadds f2,f13,f6,f2
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f6.f64 + ctx.f2.f64));
	// fmsubs f4,f5,f6,f4
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f6.f64 - ctx.f4.f64));
	// fmuls f16,f12,f23
	ctx.f16.f64 = double(float(ctx.f12.f64 * ctx.f23.f64));
	// fmuls f14,f1,f25
	ctx.f14.f64 = double(float(ctx.f1.f64 * ctx.f25.f64));
	// fmuls f15,f23,f25
	ctx.f15.f64 = double(float(ctx.f23.f64 * ctx.f25.f64));
	// fmadds f22,f26,f8,f22
	ctx.f22.f64 = double(float(ctx.f26.f64 * ctx.f8.f64 + ctx.f22.f64));
	// fmadds f10,f5,f9,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f9.f64 + ctx.f10.f64));
	// fmsubs f5,f12,f26,f19
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f26.f64 - ctx.f19.f64));
	// fmsubs f19,f23,f8,f17
	ctx.f19.f64 = double(float(ctx.f23.f64 * ctx.f8.f64 - ctx.f17.f64));
	// fmadds f29,f3,f11,f29
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f11.f64 + ctx.f29.f64));
	// fmadds f2,f24,f9,f2
	ctx.f2.f64 = double(float(ctx.f24.f64 * ctx.f9.f64 + ctx.f2.f64));
	// fnmsubs f4,f3,f9,f4
	ctx.f4.f64 = double(float(-(ctx.f3.f64 * ctx.f9.f64 - ctx.f4.f64)));
	// fmuls f26,f26,f25
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f25.f64));
	// fmsubs f1,f7,f1,f16
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f1.f64 - ctx.f16.f64));
	// fmadds f25,f7,f23,f22
	ctx.f25.f64 = double(float(ctx.f7.f64 * ctx.f23.f64 + ctx.f22.f64));
	// fnmsubs f11,f24,f11,f10
	ctx.f11.f64 = double(float(-(ctx.f24.f64 * ctx.f11.f64 - ctx.f10.f64)));
	// fmuls f10,f5,f6
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmuls f5,f19,f6
	ctx.f5.f64 = double(float(ctx.f19.f64 * ctx.f6.f64));
	// fnmsubs f3,f3,f0,f2
	ctx.f3.f64 = double(float(-(ctx.f3.f64 * ctx.f0.f64 - ctx.f2.f64)));
	// fnmsubs f2,f13,f9,f29
	ctx.f2.f64 = double(float(-(ctx.f13.f64 * ctx.f9.f64 - ctx.f29.f64)));
	// fnmsubs f4,f24,f0,f4
	ctx.f4.f64 = double(float(-(ctx.f24.f64 * ctx.f0.f64 - ctx.f4.f64)));
	// fmuls f1,f1,f6
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f6.f64));
	// fmuls f0,f7,f25
	ctx.f0.f64 = double(float(ctx.f7.f64 * ctx.f25.f64));
	// fmuls f13,f11,f11
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f11.f64));
	// fmuls f12,f12,f25
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f25.f64));
	// fadds f9,f14,f5
	ctx.f9.f64 = double(float(ctx.f14.f64 + ctx.f5.f64));
	// fadds f10,f15,f10
	ctx.f10.f64 = double(float(ctx.f15.f64 + ctx.f10.f64));
	// fmuls f7,f11,f3
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f3.f64));
	// fmuls f6,f2,f2
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f2.f64));
	// fmuls f5,f4,f2
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f2.f64));
	// fmuls f8,f25,f8
	ctx.f8.f64 = double(float(ctx.f25.f64 * ctx.f8.f64));
	// fadds f1,f26,f1
	ctx.f1.f64 = double(float(ctx.f26.f64 + ctx.f1.f64));
	// fmuls f29,f13,f31
	ctx.f29.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// fadds f13,f9,f12
	ctx.f13.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
	// fadds f0,f10,f0
	ctx.f0.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// fmuls f12,f7,f31
	ctx.f12.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// fmuls f10,f6,f31
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// fmuls f9,f5,f31
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// fadds f8,f1,f8
	ctx.f8.f64 = double(float(ctx.f1.f64 + ctx.f8.f64));
	// fsubs f7,f30,f29
	ctx.f7.f64 = double(float(ctx.f30.f64 - ctx.f29.f64));
	// fmuls f5,f13,f31
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// fmuls f6,f0,f31
	ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fsubs f1,f12,f9
	ctx.f1.f64 = double(float(ctx.f12.f64 - ctx.f9.f64));
	// stfs f1,100(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fmuls f8,f8,f31
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fsubs f7,f7,f10
	ctx.f7.f64 = double(float(ctx.f7.f64 - ctx.f10.f64));
	// stfs f7,96(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f13,f20,f5
	ctx.f13.f64 = double(float(ctx.f20.f64 + ctx.f5.f64));
	// fadds f0,f21,f6
	ctx.f0.f64 = double(float(ctx.f21.f64 + ctx.f6.f64));
	// fmuls f6,f2,f3
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f3.f64));
	// fmuls f5,f4,f11
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmuls f7,f4,f3
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// fmuls f1,f3,f3
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f11,f2,f11
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fadds f4,f9,f12
	ctx.f4.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
	// stfs f4,108(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fmuls f3,f6,f31
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// fmuls f2,f5,f31
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// fadds f12,f8,f18
	ctx.f12.f64 = double(float(ctx.f8.f64 + ctx.f18.f64));
	// fmuls f9,f7,f31
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// fnmsubs f1,f1,f31,f30
	ctx.f1.f64 = double(float(-(ctx.f1.f64 * ctx.f31.f64 - ctx.f30.f64)));
	// fmuls f11,f11,f31
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// fadds f8,f2,f3
	ctx.f8.f64 = double(float(ctx.f2.f64 + ctx.f3.f64));
	// stfs f8,104(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fsubs f6,f3,f2
	ctx.f6.f64 = double(float(ctx.f3.f64 - ctx.f2.f64));
	// stfs f6,120(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// fsubs f7,f1,f10
	ctx.f7.f64 = double(float(ctx.f1.f64 - ctx.f10.f64));
	// stfs f7,112(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fsubs f5,f11,f9
	ctx.f5.f64 = double(float(ctx.f11.f64 - ctx.f9.f64));
	// stfs f5,116(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fadds f4,f9,f11
	ctx.f4.f64 = double(float(ctx.f9.f64 + ctx.f11.f64));
	// stfs f4,124(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// fsubs f3,f1,f29
	ctx.f3.f64 = double(float(ctx.f1.f64 - ctx.f29.f64));
	// stfs f3,128(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830C5294:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c5294
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C5294;
	// stfs f12,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f0,40(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f13,44(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r10.u32);
loc_830C52C0:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r31,r30,12
	ctx.r31.s64 = ctx.r30.s64 + 12;
	// bl 0x82d62918
	ctx.lr = 0x830C52CC;
	sub_82D62918(ctx, base);
	// addi r5,r1,384
	ctx.r5.s64 = ctx.r1.s64 + 384;
	// li r4,4
	ctx.r4.s64 = 4;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82d632a8
	ctx.lr = 0x830C52DC;
	sub_82D632A8(ctx, base);
	// addi r3,r1,240
	ctx.r3.s64 = ctx.r1.s64 + 240;
	// bl 0x82d68d88
	ctx.lr = 0x830C52E4;
	sub_82D68D88(ctx, base);
	// clrlwi r11,r29,24
	ctx.r11.u64 = ctx.r29.u32 & 0xFF;
	// lwz r10,244(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c52fc
	if (ctx.cr6.eq) goto loc_830C52FC;
	// ori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 | 1;
	// b 0x830c5300
	goto loc_830C5300;
loc_830C52FC:
	// rlwinm r10,r10,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
loc_830C5300:
	// cntlzw r9,r11
	ctx.r9.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// lwz r11,336(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 336);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// stfs f27,372(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 372, temp.u32);
	// rlwinm r7,r10,0,31,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFFFFFD;
	// lfs f0,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r6,r9,27,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// lfs f13,36(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	ctx.f13.f64 = double(temp.f32);
	// li r5,1
	ctx.r5.s64 = 1;
	// stw r7,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, ctx.r7.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stb r6,380(r1)
	PPC_STORE_U8(ctx.r1.u32 + 380, ctx.r6.u8);
	// stb r5,381(r1)
	PPC_STORE_U8(ctx.r1.u32 + 381, ctx.r5.u8);
	// lfs f29,6048(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 6048);
	ctx.f29.f64 = double(temp.f32);
	// stw r4,332(r1)
	PPC_STORE_U32(ctx.r1.u32 + 332, ctx.r4.u32);
	// lfs f12,152(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 152);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,40(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	ctx.f11.f64 = double(temp.f32);
	// li r7,0
	ctx.r7.s64 = 0;
	// lfs f10,44(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	ctx.f10.f64 = double(temp.f32);
	// addi r6,r1,176
	ctx.r6.s64 = ctx.r1.s64 + 176;
	// lfs f9,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// addi r5,r11,8
	ctx.r5.s64 = ctx.r11.s64 + 8;
	// lfs f8,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lfs f7,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// addi r3,r1,240
	ctx.r3.s64 = ctx.r1.s64 + 240;
	// lfs f6,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,28(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,32(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	ctx.f2.f64 = double(temp.f32);
	// stfs f12,376(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 376, temp.u32);
	// stfs f29,220(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// stfs f0,176(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// stfs f29,204(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// stfs f9,192(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 192, temp.u32);
	// stfs f29,188(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 188, temp.u32);
	// stfs f8,208(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 208, temp.u32);
	// stfs f30,236(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 236, temp.u32);
	// stfs f7,180(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// stfs f6,196(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 196, temp.u32);
	// stfs f5,212(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 212, temp.u32);
	// stfs f4,184(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// stfs f3,200(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 200, temp.u32);
	// stfs f2,216(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 216, temp.u32);
	// stfs f13,224(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 224, temp.u32);
	// stfs f11,228(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 228, temp.u32);
	// stfs f10,232(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 232, temp.u32);
	// stfs f29,144(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// bl 0x82d6dbe8
	ctx.lr = 0x830C53C8;
	sub_82D6DBE8(ctx, base);
	// clrlwi r3,r3,24
	ctx.r3.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x830c53f8
	if (!ctx.cr6.eq) goto loc_830C53F8;
loc_830C53D4:
	// addi r3,r1,240
	ctx.r3.s64 = ctx.r1.s64 + 240;
	// bl 0x82d68dc8
	ctx.lr = 0x830C53DC;
	sub_82D68DC8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82d632a0
	ctx.lr = 0x830C53E4;
	sub_82D632A0(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,608
	ctx.r1.s64 = ctx.r1.s64 + 608;
	// addi r12,r1,-56
	ctx.r12.s64 = ctx.r1.s64 + -56;
	// bl 0x82cb6afc
	ctx.lr = 0x830C53F4;
	__restfpr_14(ctx, base);
	// b 0x82cb1130
	__restgprlr_26(ctx, base);
	return;
loc_830C53F8:
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// rlwinm r10,r11,0,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFC;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830c53d4
	if (ctx.cr6.eq) goto loc_830C53D4;
	// lwz r4,88(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lfs f0,4(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,36(r27)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r27.u32 + 36, temp.u32);
	// fcmpu cr6,f0,f27
	ctx.cr6.compare(ctx.f0.f64, ctx.f27.f64);
	// bgt cr6,0x830c53d4
	if (ctx.cr6.gt) goto loc_830C53D4;
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lfs f0,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// lwz r5,336(r30)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r30.u32 + 336);
	// fsubs f12,f30,f0
	ctx.f12.f64 = double(float(ctx.f30.f64 - ctx.f0.f64));
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lfs f13,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// lwz r10,264(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 264);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// lwz r8,164(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 164);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,172(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 172);
	// add r11,r8,r11
	ctx.r11.u64 = ctx.r8.u64 + ctx.r11.u64;
	// fsubs f12,f12,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f13.f64));
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r7,8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// rlwinm r3,r8,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r29,r7,1,0,30
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r3
	ctx.r8.u64 = ctx.r8.u64 + ctx.r3.u64;
	// add r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 + ctx.r29.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// rlwinm r28,r6,1,0,30
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r6,r6,r28
	ctx.r6.u64 = ctx.r6.u64 + ctx.r28.u64;
	// lfs f11,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,4(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f11,f0
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// lfs f10,8(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f7,f9,f13
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmuls f6,f10,f0
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f5,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// lfs f4,4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,8(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f9,f5,f13,f8
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f13.f64 + ctx.f8.f64));
	// lfs f1,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f8,f4,f0,f7
	ctx.f8.f64 = double(float(ctx.f4.f64 * ctx.f0.f64 + ctx.f7.f64));
	// lfs f10,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f7,f12,f2,f6
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f2.f64 + ctx.f6.f64));
	// fmadds f11,f1,f12,f9
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f12.f64 + ctx.f9.f64));
	// stfs f11,152(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmadds f10,f10,f12,f8
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f12.f64 + ctx.f8.f64));
	// stfs f10,156(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 156, temp.u32);
	// fmadds f9,f13,f3,f7
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f3.f64 + ctx.f7.f64));
	// stfs f9,160(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// beq cr6,0x830c56ec
	if (ctx.cr6.eq) goto loc_830C56EC;
	// lwz r6,280(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 280);
	// lwz r3,8(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// cmplw cr6,r6,r3
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, ctx.r3.u32, ctx.xer);
	// beq cr6,0x830c56ec
	if (ctx.cr6.eq) goto loc_830C56EC;
	// lfs f11,252(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 252);
	ctx.f11.f64 = double(temp.f32);
	// addi r6,r30,112
	ctx.r6.s64 = ctx.r30.s64 + 112;
	// lfs f10,112(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 112);
	ctx.f10.f64 = double(temp.f32);
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// lfs f6,248(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 248);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f7,f10,f11
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
	// lfs f8,244(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 244);
	ctx.f8.f64 = double(temp.f32);
	// fmr f4,f6
	ctx.f4.f64 = ctx.f6.f64;
	// fmr f5,f8
	ctx.f5.f64 = ctx.f8.f64;
	// lfs f2,120(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 120);
	ctx.f2.f64 = double(temp.f32);
	// lfs f29,124(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 124);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f1,f10,f8
	ctx.f1.f64 = double(float(ctx.f10.f64 * ctx.f8.f64));
	// lfs f3,256(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 256);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f27,f2,f6
	ctx.f27.f64 = double(float(ctx.f2.f64 * ctx.f6.f64));
	// lfs f26,116(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 116);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f25,f29,f11
	ctx.f25.f64 = double(float(ctx.f29.f64 * ctx.f11.f64));
	// lfs f23,132(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 132);
	ctx.f23.f64 = double(temp.f32);
	// fmsubs f28,f3,f3,f28
	ctx.f28.f64 = double(float(ctx.f3.f64 * ctx.f3.f64 - ctx.f28.f64));
	// lfs f24,136(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 136);
	ctx.f24.f64 = double(temp.f32);
	// addi r6,r10,244
	ctx.r6.s64 = ctx.r10.s64 + 244;
	// lfs f22,128(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 128);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f21,f23,f9
	ctx.f21.f64 = double(float(ctx.f23.f64 * ctx.f9.f64));
	// lfs f30,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f7,f26,f3,f7
	ctx.f7.f64 = double(float(ctx.f26.f64 * ctx.f3.f64 + ctx.f7.f64));
	// stfd f0,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.f0.u64);
	// fmuls f17,f23,f4
	ctx.f17.f64 = double(float(ctx.f23.f64 * ctx.f4.f64));
	// lfs f20,264(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 264);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f19,f24,f5
	ctx.f19.f64 = double(float(ctx.f24.f64 * ctx.f5.f64));
	// lfs f18,268(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 268);
	ctx.f18.f64 = double(temp.f32);
	// fmsubs f1,f29,f3,f1
	ctx.f1.f64 = double(float(ctx.f29.f64 * ctx.f3.f64 - ctx.f1.f64));
	// lfs f16,260(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 260);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f27,f10,f3,f27
	ctx.f27.f64 = double(float(ctx.f10.f64 * ctx.f3.f64 + ctx.f27.f64));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// fmadds f25,f2,f3,f25
	ctx.f25.f64 = double(float(ctx.f2.f64 * ctx.f3.f64 + ctx.f25.f64));
	// fmuls f15,f22,f4
	ctx.f15.f64 = double(float(ctx.f22.f64 * ctx.f4.f64));
	// fmuls f0,f24,f28
	ctx.f0.f64 = double(float(ctx.f24.f64 * ctx.f28.f64));
	// fmuls f14,f23,f28
	ctx.f14.f64 = double(float(ctx.f23.f64 * ctx.f28.f64));
	// fmsubs f21,f24,f4,f21
	ctx.f21.f64 = double(float(ctx.f24.f64 * ctx.f4.f64 - ctx.f21.f64));
	// fmadds f7,f29,f6,f7
	ctx.f7.f64 = double(float(ctx.f29.f64 * ctx.f6.f64 + ctx.f7.f64));
	// fmadds f17,f22,f5,f17
	ctx.f17.f64 = double(float(ctx.f22.f64 * ctx.f5.f64 + ctx.f17.f64));
	// fmsubs f19,f22,f9,f19
	ctx.f19.f64 = double(float(ctx.f22.f64 * ctx.f9.f64 - ctx.f19.f64));
	// fnmsubs f1,f26,f6,f1
	ctx.f1.f64 = double(float(-(ctx.f26.f64 * ctx.f6.f64 - ctx.f1.f64)));
	// fmadds f29,f29,f8,f27
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f8.f64 + ctx.f27.f64));
	// fmadds f27,f26,f8,f25
	ctx.f27.f64 = double(float(ctx.f26.f64 * ctx.f8.f64 + ctx.f25.f64));
	// fmsubs f25,f23,f5,f15
	ctx.f25.f64 = double(float(ctx.f23.f64 * ctx.f5.f64 - ctx.f15.f64));
	// fmuls f28,f22,f28
	ctx.f28.f64 = double(float(ctx.f22.f64 * ctx.f28.f64));
	// fmuls f23,f21,f3
	ctx.f23.f64 = double(float(ctx.f21.f64 * ctx.f3.f64));
	// fnmsubs f8,f2,f8,f7
	ctx.f8.f64 = double(float(-(ctx.f2.f64 * ctx.f8.f64 - ctx.f7.f64)));
	// fmadds f24,f24,f9,f17
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f9.f64 + ctx.f17.f64));
	// fmuls f7,f19,f3
	ctx.f7.f64 = double(float(ctx.f19.f64 * ctx.f3.f64));
	// fnmsubs f2,f2,f11,f1
	ctx.f2.f64 = double(float(-(ctx.f2.f64 * ctx.f11.f64 - ctx.f1.f64)));
	// fnmsubs f1,f26,f11,f29
	ctx.f1.f64 = double(float(-(ctx.f26.f64 * ctx.f11.f64 - ctx.f29.f64)));
	// fnmsubs f10,f10,f6,f27
	ctx.f10.f64 = double(float(-(ctx.f10.f64 * ctx.f6.f64 - ctx.f27.f64)));
	// fmuls f6,f25,f3
	ctx.f6.f64 = double(float(ctx.f25.f64 * ctx.f3.f64));
	// fadds f3,f28,f23
	ctx.f3.f64 = double(float(ctx.f28.f64 + ctx.f23.f64));
	// fmuls f11,f8,f8
	ctx.f11.f64 = double(float(ctx.f8.f64 * ctx.f8.f64));
	// fmuls f4,f4,f24
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f24.f64));
	// fadds f7,f14,f7
	ctx.f7.f64 = double(float(ctx.f14.f64 + ctx.f7.f64));
	// fmuls f5,f24,f5
	ctx.f5.f64 = double(float(ctx.f24.f64 * ctx.f5.f64));
	// fmuls f29,f8,f1
	ctx.f29.f64 = double(float(ctx.f8.f64 * ctx.f1.f64));
	// fmuls f28,f10,f10
	ctx.f28.f64 = double(float(ctx.f10.f64 * ctx.f10.f64));
	// fmuls f27,f2,f10
	ctx.f27.f64 = double(float(ctx.f2.f64 * ctx.f10.f64));
	// fmuls f9,f9,f24
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f24.f64));
	// fadds f6,f0,f6
	ctx.f6.f64 = double(float(ctx.f0.f64 + ctx.f6.f64));
	// fmuls f26,f11,f31
	ctx.f26.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// fadds f4,f7,f4
	ctx.f4.f64 = double(float(ctx.f7.f64 + ctx.f4.f64));
	// fadds f5,f3,f5
	ctx.f5.f64 = double(float(ctx.f3.f64 + ctx.f5.f64));
	// fmuls f7,f29,f31
	ctx.f7.f64 = double(float(ctx.f29.f64 * ctx.f31.f64));
	// fmuls f29,f28,f31
	ctx.f29.f64 = double(float(ctx.f28.f64 * ctx.f31.f64));
	// fmuls f28,f27,f31
	ctx.f28.f64 = double(float(ctx.f27.f64 * ctx.f31.f64));
	// fadds f6,f6,f9
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f9.f64));
	// fsubs f3,f30,f26
	ctx.f3.f64 = double(float(ctx.f30.f64 - ctx.f26.f64));
	// fmuls f11,f4,f31
	ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// fmuls f5,f5,f31
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// fsubs f9,f7,f28
	ctx.f9.f64 = double(float(ctx.f7.f64 - ctx.f28.f64));
	// stfs f9,100(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fmuls f6,f6,f31
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// fsubs f4,f3,f29
	ctx.f4.f64 = double(float(ctx.f3.f64 - ctx.f29.f64));
	// stfs f4,96(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fmuls f3,f10,f1
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f1.f64));
	// fadds f11,f20,f11
	ctx.f11.f64 = double(float(ctx.f20.f64 + ctx.f11.f64));
	// fmuls f4,f2,f8
	ctx.f4.f64 = double(float(ctx.f2.f64 * ctx.f8.f64));
	// mr r6,r31
	ctx.r6.u64 = ctx.r31.u64;
	// fmuls f27,f1,f1
	ctx.f27.f64 = double(float(ctx.f1.f64 * ctx.f1.f64));
	// li r3,9
	ctx.r3.s64 = 9;
	// fmuls f2,f2,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// fmuls f8,f10,f8
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f8.f64));
	// fadds f1,f28,f7
	ctx.f1.f64 = double(float(ctx.f28.f64 + ctx.f7.f64));
	// stfs f1,108(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fadds f10,f18,f6
	ctx.f10.f64 = double(float(ctx.f18.f64 + ctx.f6.f64));
	// fadds f9,f16,f5
	ctx.f9.f64 = double(float(ctx.f16.f64 + ctx.f5.f64));
	// fmuls f7,f3,f31
	ctx.f7.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// fmuls f6,f4,f31
	ctx.f6.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// fnmsubs f5,f27,f31,f30
	ctx.f5.f64 = double(float(-(ctx.f27.f64 * ctx.f31.f64 - ctx.f30.f64)));
	// fmuls f3,f2,f31
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f4,f8,f31
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fadds f2,f6,f7
	ctx.f2.f64 = double(float(ctx.f6.f64 + ctx.f7.f64));
	// stfs f2,104(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fsubs f8,f7,f6
	ctx.f8.f64 = double(float(ctx.f7.f64 - ctx.f6.f64));
	// stfs f8,120(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// fsubs f1,f5,f29
	ctx.f1.f64 = double(float(ctx.f5.f64 - ctx.f29.f64));
	// stfs f1,112(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fsubs f7,f4,f3
	ctx.f7.f64 = double(float(ctx.f4.f64 - ctx.f3.f64));
	// stfs f7,116(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fadds f6,f3,f4
	ctx.f6.f64 = double(float(ctx.f3.f64 + ctx.f4.f64));
	// stfs f6,124(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// fsubs f5,f5,f26
	ctx.f5.f64 = double(float(ctx.f5.f64 - ctx.f26.f64));
	// stfs f5,128(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// mtctr r3
	ctx.ctr.u64 = ctx.r3.u64;
	// lfd f0,136(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
loc_830C56B0:
	// lwz r3,0(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r3,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r3.u32);
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// bdnz 0x830c56b0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C56B0;
	// stfs f9,36(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r31.u32 + 36, temp.u32);
	// stfs f10,44(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r31.u32 + 44, temp.u32);
	// stfs f11,40(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r31.u32 + 40, temp.u32);
	// lfs f9,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f9.f64 = double(temp.f32);
	// lfs f10,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f11.f64 = double(temp.f32);
	// lfs f29,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f29.f64 = double(temp.f32);
	// lwz r10,264(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 264);
	// lwz r6,280(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 280);
	// stw r6,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r6.u32);
loc_830C56EC:
	// lfs f8,4(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// rlwinm r6,r26,0,25,25
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 0) & 0x40;
	// lfs f7,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f6,f8,f10
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f10.f64));
	// lfs f5,28(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f4,f7,f10
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
	// fmuls f3,f5,f10
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f10.f64));
	// lfs f1,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f2.f64 = double(temp.f32);
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// lfs f10,32(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	ctx.f10.f64 = double(temp.f32);
	// li r10,59
	ctx.r10.s64 = 59;
	// lfs f8,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f7.f64 = double(temp.f32);
	// lfs f5,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f5.f64 = double(temp.f32);
	// lfs f31,36(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	ctx.f31.f64 = double(temp.f32);
	// lfs f28,40(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f2,f2,f9,f6
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f9.f64 + ctx.f6.f64));
	// lfs f6,44(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f4,f1,f9,f4
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f9.f64 + ctx.f4.f64));
	// fmadds f3,f10,f9,f3
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f9.f64 + ctx.f3.f64));
	// fmadds f2,f8,f11,f2
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f11.f64 + ctx.f2.f64));
	// fmadds f1,f7,f11,f4
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f11.f64 + ctx.f4.f64));
	// fmadds f11,f5,f11,f3
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f11.f64 + ctx.f3.f64));
	// fadds f10,f2,f31
	ctx.f10.f64 = double(float(ctx.f2.f64 + ctx.f31.f64));
	// stfs f10,4(r27)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r27.u32 + 4, temp.u32);
	// fadds f9,f28,f1
	ctx.f9.f64 = double(float(ctx.f28.f64 + ctx.f1.f64));
	// stfs f9,8(r27)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r27.u32 + 8, temp.u32);
	// fadds f8,f6,f11
	ctx.f8.f64 = double(float(ctx.f6.f64 + ctx.f11.f64));
	// stfs f8,12(r27)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r27.u32 + 12, temp.u32);
	// lwz r3,144(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 144);
	// stw r3,0(r27)
	PPC_STORE_U32(ctx.r27.u32 + 0, ctx.r3.u32);
	// lwz r6,0(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// stw r6,28(r27)
	PPC_STORE_U32(ctx.r27.u32 + 28, ctx.r6.u32);
	// lwz r4,0(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// stfs f0,40(r27)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r27.u32 + 40, temp.u32);
	// stfs f13,44(r27)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r27.u32 + 44, temp.u32);
	// stw r4,32(r27)
	PPC_STORE_U32(ctx.r27.u32 + 32, ctx.r4.u32);
	// stw r10,52(r27)
	PPC_STORE_U32(ctx.r27.u32 + 52, ctx.r10.u32);
	// beq cr6,0x830c5884
	if (ctx.cr6.eq) goto loc_830C5884;
	// lfs f0,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r27,16
	ctx.r11.s64 = ctx.r27.s64 + 16;
	// lfs f12,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// lfs f8,8(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f6,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// fsubs f5,f8,f12
	ctx.f5.f64 = double(float(ctx.f8.f64 - ctx.f12.f64));
	// lfs f13,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f3,f6,f0
	ctx.f3.f64 = double(float(ctx.f6.f64 - ctx.f0.f64));
	// lfs f11,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lfs f4,4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	ctx.f4.f64 = double(temp.f32);
	// fsubs f9,f11,f0
	ctx.f9.f64 = double(float(ctx.f11.f64 - ctx.f0.f64));
	// lfs f10,4(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// fsubs f1,f4,f13
	ctx.f1.f64 = double(float(ctx.f4.f64 - ctx.f13.f64));
	// lfs f2,8(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	ctx.f2.f64 = double(temp.f32);
	// fsubs f7,f10,f13
	ctx.f7.f64 = double(float(ctx.f10.f64 - ctx.f13.f64));
	// fsubs f0,f2,f12
	ctx.f0.f64 = double(float(ctx.f2.f64 - ctx.f12.f64));
	// lfs f13,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,32(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,28(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f31,f5,f3
	ctx.f31.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// lfs f6,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// lfs f4,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f4.f64 = double(temp.f32);
	// lfs f28,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f27,f1,f9
	ctx.f27.f64 = double(float(ctx.f1.f64 * ctx.f9.f64));
	// lfs f2,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f26,f0,f7
	ctx.f26.f64 = double(float(ctx.f0.f64 * ctx.f7.f64));
	// fmsubs f0,f0,f9,f31
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f9.f64 - ctx.f31.f64));
	// fmsubs f9,f7,f3,f27
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f3.f64 - ctx.f27.f64));
	// fmsubs f7,f1,f5,f26
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f5.f64 - ctx.f26.f64));
	// fmuls f5,f0,f11
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// fmuls f3,f12,f9
	ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f9.f64));
	// fmuls f1,f7,f13
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// fmadds f13,f6,f7,f5
	ctx.f13.f64 = double(float(ctx.f6.f64 * ctx.f7.f64 + ctx.f5.f64));
	// fmadds f12,f0,f8,f3
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f3.f64));
	// fmadds f11,f9,f10,f1
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f10.f64 + ctx.f1.f64));
	// fmadds f10,f9,f28,f13
	ctx.f10.f64 = double(float(ctx.f9.f64 * ctx.f28.f64 + ctx.f13.f64));
	// stfs f10,16(r27)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r27.u32 + 16, temp.u32);
	// fmadds f9,f7,f2,f12
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f2.f64 + ctx.f12.f64));
	// stfs f9,24(r27)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r27.u32 + 24, temp.u32);
	// fmadds f8,f4,f0,f11
	ctx.f8.f64 = double(float(ctx.f4.f64 * ctx.f0.f64 + ctx.f11.f64));
	// stfs f8,20(r27)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r27.u32 + 20, temp.u32);
	// fmr f13,f10
	ctx.f13.f64 = ctx.f10.f64;
	// fmr f12,f9
	ctx.f12.f64 = ctx.f9.f64;
	// fmr f0,f8
	ctx.f0.f64 = ctx.f8.f64;
	// fmuls f7,f0,f0
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f0.f64));
	// fmadds f6,f13,f13,f7
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 + ctx.f7.f64));
	// fmadds f5,f12,f12,f6
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f12.f64 + ctx.f6.f64));
	// fsqrts f11,f5
	ctx.f11.f64 = double(float(sqrt(ctx.f5.f64)));
	// fcmpu cr6,f11,f29
	ctx.cr6.compare(ctx.f11.f64, ctx.f29.f64);
	// beq cr6,0x830c5878
	if (ctx.cr6.eq) goto loc_830C5878;
	// fdivs f11,f30,f11
	ctx.f11.f64 = double(float(ctx.f30.f64 / ctx.f11.f64));
	// fmuls f10,f11,f13
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// stfs f10,0(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fmuls f9,f11,f0
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f9,4(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fmuls f8,f11,f12
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// stfs f8,8(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
loc_830C5878:
	// lwz r11,52(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 52);
	// ori r10,r11,64
	ctx.r10.u64 = ctx.r11.u64 | 64;
	// b 0x830c59b8
	goto loc_830C59B8;
loc_830C5884:
	// rlwinm r10,r26,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830c59bc
	if (ctx.cr6.eq) goto loc_830C59BC;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lfs f11,12(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// lfs f9,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// lwz r8,176(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 176);
	// rlwinm r6,r10,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r5,r9,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// add r11,r10,r6
	ctx.r11.u64 = ctx.r10.u64 + ctx.r6.u64;
	// lfs f8,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f8.f64 = double(temp.f32);
	// add r10,r9,r5
	ctx.r10.u64 = ctx.r9.u64 + ctx.r5.u64;
	// lfs f10,32(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	ctx.f10.f64 = double(temp.f32);
	// rlwinm r6,r7,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// lfs f6,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f5,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f5.f64 = double(temp.f32);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// lfs f7,28(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	ctx.f7.f64 = double(temp.f32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lfs f3,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f4,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f4.f64 = double(temp.f32);
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r8
	ctx.r10.u64 = ctx.r11.u64 + ctx.r8.u64;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// lfs f1,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// addi r11,r27,16
	ctx.r11.s64 = ctx.r27.s64 + 16;
	// fmuls f1,f1,f13
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
	// lfs f28,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f27.f64 = double(temp.f32);
	// lfs f2,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f2,f2,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// lfs f25,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f31,f31,f0
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// lfs f26,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f1,f25,f12,f1
	ctx.f1.f64 = double(float(ctx.f25.f64 * ctx.f12.f64 + ctx.f1.f64));
	// lfs f25,8(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f2,f28,f13,f2
	ctx.f2.f64 = double(float(ctx.f28.f64 * ctx.f13.f64 + ctx.f2.f64));
	// lfs f28,4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f13,f27,f13,f31
	ctx.f13.f64 = double(float(ctx.f27.f64 * ctx.f13.f64 + ctx.f31.f64));
	// fmadds f1,f0,f26,f1
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f26.f64 + ctx.f1.f64));
	// fmadds f2,f28,f12,f2
	ctx.f2.f64 = double(float(ctx.f28.f64 * ctx.f12.f64 + ctx.f2.f64));
	// fmadds f0,f25,f12,f13
	ctx.f0.f64 = double(float(ctx.f25.f64 * ctx.f12.f64 + ctx.f13.f64));
	// fmuls f12,f1,f11
	ctx.f12.f64 = double(float(ctx.f1.f64 * ctx.f11.f64));
	// fmuls f13,f2,f9
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f9.f64));
	// fmuls f11,f10,f0
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmadds f9,f0,f8,f12
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f12.f64));
	// fmadds f10,f6,f1,f13
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f1.f64 + ctx.f13.f64));
	// fmadds f8,f2,f7,f11
	ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f7.f64 + ctx.f11.f64));
	// fmadds f6,f5,f2,f9
	ctx.f6.f64 = double(float(ctx.f5.f64 * ctx.f2.f64 + ctx.f9.f64));
	// stfs f6,20(r27)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r27.u32 + 20, temp.u32);
	// fmadds f7,f0,f3,f10
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f3.f64 + ctx.f10.f64));
	// stfs f7,16(r27)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r27.u32 + 16, temp.u32);
	// fmadds f5,f1,f4,f8
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f4.f64 + ctx.f8.f64));
	// stfs f5,24(r27)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r27.u32 + 24, temp.u32);
	// fmr f0,f6
	ctx.f0.f64 = ctx.f6.f64;
	// fmr f13,f7
	ctx.f13.f64 = ctx.f7.f64;
	// fmr f12,f5
	ctx.f12.f64 = ctx.f5.f64;
	// fmuls f4,f0,f0
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f0.f64));
	// fmadds f3,f13,f13,f4
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 + ctx.f4.f64));
	// fmadds f2,f12,f12,f3
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f12.f64 + ctx.f3.f64));
	// fsqrts f11,f2
	ctx.f11.f64 = double(float(sqrt(ctx.f2.f64)));
	// fcmpu cr6,f11,f29
	ctx.cr6.compare(ctx.f11.f64, ctx.f29.f64);
	// beq cr6,0x830c59b0
	if (ctx.cr6.eq) goto loc_830C59B0;
	// fdivs f11,f30,f11
	ctx.f11.f64 = double(float(ctx.f30.f64 / ctx.f11.f64));
	// fmuls f10,f11,f13
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// stfs f10,0(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fmuls f9,f11,f0
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f9,4(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fmuls f8,f11,f12
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// stfs f8,8(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
loc_830C59B0:
	// lwz r11,52(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 52);
	// ori r10,r11,4
	ctx.r10.u64 = ctx.r11.u64 | 4;
loc_830C59B8:
	// stw r10,52(r27)
	PPC_STORE_U32(ctx.r27.u32 + 52, ctx.r10.u32);
loc_830C59BC:
	// rlwinm r11,r26,0,24,24
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 0) & 0x80;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c59dc
	if (ctx.cr6.eq) goto loc_830C59DC;
	// lwz r11,52(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 52);
	// ori r10,r11,128
	ctx.r10.u64 = ctx.r11.u64 | 128;
	// stw r10,52(r27)
	PPC_STORE_U32(ctx.r27.u32 + 52, ctx.r10.u32);
	// lhz r9,306(r30)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r30.u32 + 306);
	// sth r9,48(r27)
	PPC_STORE_U16(ctx.r27.u32 + 48, ctx.r9.u16);
loc_830C59DC:
	// addi r3,r1,240
	ctx.r3.s64 = ctx.r1.s64 + 240;
	// bl 0x82d68dc8
	ctx.lr = 0x830C59E4;
	sub_82D68DC8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82d632a0
	ctx.lr = 0x830C59EC;
	sub_82D632A0(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,608
	ctx.r1.s64 = ctx.r1.s64 + 608;
	// addi r12,r1,-56
	ctx.r12.s64 = ctx.r1.s64 + -56;
	// bl 0x82cb6afc
	ctx.lr = 0x830C59FC;
	__restfpr_14(ctx, base);
	// b 0x82cb1130
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C5A00"))) PPC_WEAK_FUNC(sub_830C5A00);
PPC_FUNC_IMPL(__imp__sub_830C5A00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e4
	ctx.lr = 0x830C5A08;
	__savegprlr_27(ctx, base);
	// addi r12,r1,-48
	ctx.r12.s64 = ctx.r1.s64 + -48;
	// bl 0x82cb6ab4
	ctx.lr = 0x830C5A10;
	__savefpr_15(ctx, base);
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lfs f30,6140(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6140);
	ctx.f30.f64 = double(temp.f32);
	// lfs f31,7676(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 7676);
	ctx.f31.f64 = double(temp.f32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f29,6380(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 6380);
	ctx.f29.f64 = double(temp.f32);
	// beq cr6,0x830c5c2c
	if (ctx.cr6.eq) goto loc_830C5C2C;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c5c2c
	if (ctx.cr6.eq) goto loc_830C5C2C;
	// lfs f0,252(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// lfs f13,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f11,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f10,f13,f0
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f9,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f9.f64 = double(temp.f32);
	// fmr f8,f11
	ctx.f8.f64 = ctx.f11.f64;
	// fmr f7,f9
	ctx.f7.f64 = ctx.f9.f64;
	// lfs f5,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f5.f64 = double(temp.f32);
	// lfs f6,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f28,f5,f0
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f3,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f2,f5,f11
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// lfs f1,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f4,f13,f11
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// lfs f27,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f27.f64 = double(temp.f32);
	// fmsubs f26,f6,f6,f29
	ctx.f26.f64 = double(float(ctx.f6.f64 * ctx.f6.f64 - ctx.f29.f64));
	// lfs f25,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f25.f64 = double(temp.f32);
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// lfs f24,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f24.f64 = double(temp.f32);
	// addi r9,r31,12
	ctx.r9.s64 = ctx.r31.s64 + 12;
	// fmuls f23,f12,f1
	ctx.f23.f64 = double(float(ctx.f12.f64 * ctx.f1.f64));
	// lfs f22,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f10,f3,f6,f10
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f6.f64 + ctx.f10.f64));
	// lfs f21,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f20,f8,f1
	ctx.f20.f64 = double(float(ctx.f8.f64 * ctx.f1.f64));
	// lfs f19,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f18,f7,f27
	ctx.f18.f64 = double(float(ctx.f7.f64 * ctx.f27.f64));
	// fmadds f28,f25,f6,f28
	ctx.f28.f64 = double(float(ctx.f25.f64 * ctx.f6.f64 + ctx.f28.f64));
	// fmadds f2,f13,f6,f2
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f6.f64 + ctx.f2.f64));
	// fmsubs f4,f5,f6,f4
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f6.f64 - ctx.f4.f64));
	// fmuls f17,f12,f24
	ctx.f17.f64 = double(float(ctx.f12.f64 * ctx.f24.f64));
	// fmuls f16,f26,f24
	ctx.f16.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// fmuls f15,f26,f1
	ctx.f15.f64 = double(float(ctx.f26.f64 * ctx.f1.f64));
	// fmadds f23,f7,f24,f23
	ctx.f23.f64 = double(float(ctx.f7.f64 * ctx.f24.f64 + ctx.f23.f64));
	// fmadds f10,f5,f9,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f9.f64 + ctx.f10.f64));
	// fmsubs f5,f12,f27,f20
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f27.f64 - ctx.f20.f64));
	// fmsubs f24,f8,f24,f18
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f24.f64 - ctx.f18.f64));
	// fmadds f28,f3,f11,f28
	ctx.f28.f64 = double(float(ctx.f3.f64 * ctx.f11.f64 + ctx.f28.f64));
	// fmadds f2,f25,f9,f2
	ctx.f2.f64 = double(float(ctx.f25.f64 * ctx.f9.f64 + ctx.f2.f64));
	// fnmsubs f4,f3,f9,f4
	ctx.f4.f64 = double(float(-(ctx.f3.f64 * ctx.f9.f64 - ctx.f4.f64)));
	// fmuls f26,f26,f27
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f27.f64));
	// fmsubs f1,f7,f1,f17
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f1.f64 - ctx.f17.f64));
	// fmadds f27,f8,f27,f23
	ctx.f27.f64 = double(float(ctx.f8.f64 * ctx.f27.f64 + ctx.f23.f64));
	// fnmsubs f11,f25,f11,f10
	ctx.f11.f64 = double(float(-(ctx.f25.f64 * ctx.f11.f64 - ctx.f10.f64)));
	// fmuls f10,f5,f6
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmuls f5,f24,f6
	ctx.f5.f64 = double(float(ctx.f24.f64 * ctx.f6.f64));
	// fnmsubs f3,f3,f0,f2
	ctx.f3.f64 = double(float(-(ctx.f3.f64 * ctx.f0.f64 - ctx.f2.f64)));
	// fnmsubs f2,f13,f9,f28
	ctx.f2.f64 = double(float(-(ctx.f13.f64 * ctx.f9.f64 - ctx.f28.f64)));
	// fnmsubs f4,f25,f0,f4
	ctx.f4.f64 = double(float(-(ctx.f25.f64 * ctx.f0.f64 - ctx.f4.f64)));
	// fmuls f1,f1,f6
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f6.f64));
	// fmuls f0,f7,f27
	ctx.f0.f64 = double(float(ctx.f7.f64 * ctx.f27.f64));
	// fmuls f13,f11,f11
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f11.f64));
	// fmuls f12,f12,f27
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f27.f64));
	// fadds f9,f15,f5
	ctx.f9.f64 = double(float(ctx.f15.f64 + ctx.f5.f64));
	// fadds f10,f16,f10
	ctx.f10.f64 = double(float(ctx.f16.f64 + ctx.f10.f64));
	// fmuls f7,f11,f3
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f3.f64));
	// fmuls f6,f2,f2
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f2.f64));
	// fmuls f5,f4,f2
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f2.f64));
	// fmuls f8,f27,f8
	ctx.f8.f64 = double(float(ctx.f27.f64 * ctx.f8.f64));
	// fadds f1,f26,f1
	ctx.f1.f64 = double(float(ctx.f26.f64 + ctx.f1.f64));
	// fmuls f28,f13,f31
	ctx.f28.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// fadds f13,f9,f12
	ctx.f13.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
	// fadds f0,f10,f0
	ctx.f0.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// fmuls f12,f7,f31
	ctx.f12.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// fmuls f10,f6,f31
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// fmuls f9,f5,f31
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// fadds f8,f1,f8
	ctx.f8.f64 = double(float(ctx.f1.f64 + ctx.f8.f64));
	// fsubs f7,f30,f28
	ctx.f7.f64 = double(float(ctx.f30.f64 - ctx.f28.f64));
	// fmuls f5,f13,f31
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// fmuls f6,f0,f31
	ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fsubs f1,f12,f9
	ctx.f1.f64 = double(float(ctx.f12.f64 - ctx.f9.f64));
	// stfs f1,100(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fmuls f8,f8,f31
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fsubs f7,f7,f10
	ctx.f7.f64 = double(float(ctx.f7.f64 - ctx.f10.f64));
	// stfs f7,96(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f13,f21,f5
	ctx.f13.f64 = double(float(ctx.f21.f64 + ctx.f5.f64));
	// fadds f0,f22,f6
	ctx.f0.f64 = double(float(ctx.f22.f64 + ctx.f6.f64));
	// fmuls f6,f2,f3
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f3.f64));
	// fmuls f5,f4,f11
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmuls f7,f4,f3
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// fmuls f1,f3,f3
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f11,f2,f11
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fadds f4,f9,f12
	ctx.f4.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
	// stfs f4,108(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fmuls f3,f6,f31
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// fmuls f2,f5,f31
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// fadds f12,f19,f8
	ctx.f12.f64 = double(float(ctx.f19.f64 + ctx.f8.f64));
	// fmuls f9,f7,f31
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// fnmsubs f1,f1,f31,f30
	ctx.f1.f64 = double(float(-(ctx.f1.f64 * ctx.f31.f64 - ctx.f30.f64)));
	// fmuls f11,f11,f31
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// fadds f8,f2,f3
	ctx.f8.f64 = double(float(ctx.f2.f64 + ctx.f3.f64));
	// stfs f8,104(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fsubs f6,f3,f2
	ctx.f6.f64 = double(float(ctx.f3.f64 - ctx.f2.f64));
	// stfs f6,120(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// fsubs f7,f1,f10
	ctx.f7.f64 = double(float(ctx.f1.f64 - ctx.f10.f64));
	// stfs f7,112(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fsubs f5,f11,f9
	ctx.f5.f64 = double(float(ctx.f11.f64 - ctx.f9.f64));
	// stfs f5,116(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fadds f4,f9,f11
	ctx.f4.f64 = double(float(ctx.f9.f64 + ctx.f11.f64));
	// stfs f4,124(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// fsubs f3,f1,f28
	ctx.f3.f64 = double(float(ctx.f1.f64 - ctx.f28.f64));
	// stfs f3,128(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830C5C00:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c5c00
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C5C00;
	// stfs f12,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f0,40(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f13,44(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
loc_830C5C2C:
	// lfs f0,4(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// lwz r11,336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// lfs f13,52(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 52);
	ctx.f13.f64 = double(temp.f32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// fsubs f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// lfs f11,8(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,56(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 56);
	ctx.f10.f64 = double(temp.f32);
	// addi r3,r11,156
	ctx.r3.s64 = ctx.r11.s64 + 156;
	// lfs f9,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f9.f64 = double(temp.f32);
	// fsubs f8,f11,f10
	ctx.f8.f64 = double(float(ctx.f11.f64 - ctx.f10.f64));
	// lfs f7,28(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	ctx.f7.f64 = double(temp.f32);
	// addi r30,r31,12
	ctx.r30.s64 = ctx.r31.s64 + 12;
	// lfs f6,32(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,0(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,48(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,36(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	ctx.f3.f64 = double(temp.f32);
	// fsubs f2,f5,f4
	ctx.f2.f64 = double(float(ctx.f5.f64 - ctx.f4.f64));
	// lfs f1,40(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,44(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f13,f9,f12
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// lfs f11,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f10,f7,f12
	ctx.f10.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// lfs f9,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f7,f6,f12
	ctx.f7.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// lfs f6,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f5,f3,f8,f13
	ctx.f5.f64 = double(float(ctx.f3.f64 * ctx.f8.f64 + ctx.f13.f64));
	// fmadds f4,f1,f8,f10
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f8.f64 + ctx.f10.f64));
	// fmadds f3,f0,f8,f7
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f7.f64));
	// fmadds f1,f11,f2,f5
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f2.f64 + ctx.f5.f64));
	// stfs f1,80(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmadds f0,f9,f2,f4
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f2.f64 + ctx.f4.f64));
	// stfs f0,84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmadds f13,f6,f2,f3
	ctx.f13.f64 = double(float(ctx.f6.f64 * ctx.f2.f64 + ctx.f3.f64));
	// stfs f13,88(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// bl 0x82d54480
	ctx.lr = 0x830C5CB8;
	sub_82D54480(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c5cd8
	if (ctx.cr6.eq) goto loc_830C5CD8;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// addi r12,r1,-48
	ctx.r12.s64 = ctx.r1.s64 + -48;
	// bl 0x82cb6b00
	ctx.lr = 0x830C5CD4;
	__restfpr_15(ctx, base);
	// b 0x82cb1134
	__restgprlr_27(ctx, base);
	return;
loc_830C5CD8:
	// lwz r28,268(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// lwz r3,308(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 308);
	// bl 0x83073d18
	ctx.lr = 0x830C5CE4;
	sub_83073D18(ctx, base);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c5edc
	if (ctx.cr6.eq) goto loc_830C5EDC;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c5edc
	if (ctx.cr6.eq) goto loc_830C5EDC;
	// lfs f0,252(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// lfs f13,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f11,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f10,f13,f0
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f9,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f9.f64 = double(temp.f32);
	// fmr f8,f11
	ctx.f8.f64 = ctx.f11.f64;
	// fmr f7,f9
	ctx.f7.f64 = ctx.f9.f64;
	// lfs f5,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f5.f64 = double(temp.f32);
	// lfs f3,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f13,f11
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fmuls f2,f5,f9
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// lfs f6,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f6.f64 = double(temp.f32);
	// lfs f1,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f28,f3,f0
	ctx.f28.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// lfs f27,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f27.f64 = double(temp.f32);
	// fmsubs f29,f6,f6,f29
	ctx.f29.f64 = double(float(ctx.f6.f64 * ctx.f6.f64 - ctx.f29.f64));
	// lfs f26,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f26.f64 = double(temp.f32);
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// lfs f25,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f24,f12,f27
	ctx.f24.f64 = double(float(ctx.f12.f64 * ctx.f27.f64));
	// lfs f23,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f10,f1,f6,f10
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f6.f64 + ctx.f10.f64));
	// lfs f22,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f21,f8,f27
	ctx.f21.f64 = double(float(ctx.f8.f64 * ctx.f27.f64));
	// lfs f20,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f19,f7,f26
	ctx.f19.f64 = double(float(ctx.f7.f64 * ctx.f26.f64));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// fmsubs f4,f3,f6,f4
	ctx.f4.f64 = double(float(ctx.f3.f64 * ctx.f6.f64 - ctx.f4.f64));
	// fmadds f2,f13,f6,f2
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f6.f64 + ctx.f2.f64));
	// fmadds f28,f5,f6,f28
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f6.f64 + ctx.f28.f64));
	// fmuls f18,f12,f25
	ctx.f18.f64 = double(float(ctx.f12.f64 * ctx.f25.f64));
	// fmuls f17,f29,f25
	ctx.f17.f64 = double(float(ctx.f29.f64 * ctx.f25.f64));
	// fmuls f16,f29,f27
	ctx.f16.f64 = double(float(ctx.f29.f64 * ctx.f27.f64));
	// fmadds f24,f7,f25,f24
	ctx.f24.f64 = double(float(ctx.f7.f64 * ctx.f25.f64 + ctx.f24.f64));
	// fmadds f10,f3,f9,f10
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f9.f64 + ctx.f10.f64));
	// fmsubs f21,f12,f26,f21
	ctx.f21.f64 = double(float(ctx.f12.f64 * ctx.f26.f64 - ctx.f21.f64));
	// fmsubs f25,f8,f25,f19
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f25.f64 - ctx.f19.f64));
	// fnmsubs f4,f1,f9,f4
	ctx.f4.f64 = double(float(-(ctx.f1.f64 * ctx.f9.f64 - ctx.f4.f64)));
	// fmadds f3,f3,f11,f2
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f11.f64 + ctx.f2.f64));
	// fmadds f2,f1,f11,f28
	ctx.f2.f64 = double(float(ctx.f1.f64 * ctx.f11.f64 + ctx.f28.f64));
	// fmsubs f28,f7,f27,f18
	ctx.f28.f64 = double(float(ctx.f7.f64 * ctx.f27.f64 - ctx.f18.f64));
	// fmuls f29,f29,f26
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f26.f64));
	// fmadds f27,f26,f8,f24
	ctx.f27.f64 = double(float(ctx.f26.f64 * ctx.f8.f64 + ctx.f24.f64));
	// fnmsubs f11,f5,f11,f10
	ctx.f11.f64 = double(float(-(ctx.f5.f64 * ctx.f11.f64 - ctx.f10.f64)));
	// fmuls f10,f21,f6
	ctx.f10.f64 = double(float(ctx.f21.f64 * ctx.f6.f64));
	// fmuls f26,f25,f6
	ctx.f26.f64 = double(float(ctx.f25.f64 * ctx.f6.f64));
	// fnmsubs f5,f5,f0,f4
	ctx.f5.f64 = double(float(-(ctx.f5.f64 * ctx.f0.f64 - ctx.f4.f64)));
	// fnmsubs f4,f1,f0,f3
	ctx.f4.f64 = double(float(-(ctx.f1.f64 * ctx.f0.f64 - ctx.f3.f64)));
	// fnmsubs f3,f13,f9,f2
	ctx.f3.f64 = double(float(-(ctx.f13.f64 * ctx.f9.f64 - ctx.f2.f64)));
	// fmuls f2,f28,f6
	ctx.f2.f64 = double(float(ctx.f28.f64 * ctx.f6.f64));
	// fmuls f13,f12,f27
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f27.f64));
	// fmuls f0,f11,f11
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f11.f64));
	// fadds f12,f17,f10
	ctx.f12.f64 = double(float(ctx.f17.f64 + ctx.f10.f64));
	// fmuls f1,f7,f27
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f27.f64));
	// fadds f10,f16,f26
	ctx.f10.f64 = double(float(ctx.f16.f64 + ctx.f26.f64));
	// fmuls f9,f11,f4
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// fmuls f7,f3,f3
	ctx.f7.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// fadds f2,f29,f2
	ctx.f2.f64 = double(float(ctx.f29.f64 + ctx.f2.f64));
	// fmuls f6,f5,f3
	ctx.f6.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fmuls f8,f27,f8
	ctx.f8.f64 = double(float(ctx.f27.f64 * ctx.f8.f64));
	// fmuls f29,f0,f31
	ctx.f29.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fadds f1,f12,f1
	ctx.f1.f64 = double(float(ctx.f12.f64 + ctx.f1.f64));
	// fadds f0,f10,f13
	ctx.f0.f64 = double(float(ctx.f10.f64 + ctx.f13.f64));
	// fmuls f12,f9,f31
	ctx.f12.f64 = double(float(ctx.f9.f64 * ctx.f31.f64));
	// fmuls f10,f7,f31
	ctx.f10.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// fmuls f9,f6,f31
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// fadds f8,f2,f8
	ctx.f8.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// fsubs f7,f30,f29
	ctx.f7.f64 = double(float(ctx.f30.f64 - ctx.f29.f64));
	// fmuls f6,f1,f31
	ctx.f6.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// fmuls f2,f0,f31
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fsubs f1,f12,f9
	ctx.f1.f64 = double(float(ctx.f12.f64 - ctx.f9.f64));
	// stfs f1,100(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fmuls f8,f8,f31
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fsubs f7,f7,f10
	ctx.f7.f64 = double(float(ctx.f7.f64 - ctx.f10.f64));
	// stfs f7,96(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f0,f23,f6
	ctx.f0.f64 = double(float(ctx.f23.f64 + ctx.f6.f64));
	// fadds f13,f22,f2
	ctx.f13.f64 = double(float(ctx.f22.f64 + ctx.f2.f64));
	// fmuls f6,f3,f4
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f4.f64));
	// fmuls f2,f5,f11
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// fmuls f1,f4,f4
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f4.f64));
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// fmuls f11,f3,f11
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// li r9,9
	ctx.r9.s64 = 9;
	// fmuls f7,f5,f4
	ctx.f7.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// fmuls f3,f2,f31
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f4,f6,f31
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// fadds f5,f9,f12
	ctx.f5.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
	// stfs f5,108(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fadds f12,f8,f20
	ctx.f12.f64 = double(float(ctx.f8.f64 + ctx.f20.f64));
	// fnmsubs f2,f1,f31,f30
	ctx.f2.f64 = double(float(-(ctx.f1.f64 * ctx.f31.f64 - ctx.f30.f64)));
	// fmuls f1,f11,f31
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// fmuls f11,f7,f31
	ctx.f11.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// fadds f9,f3,f4
	ctx.f9.f64 = double(float(ctx.f3.f64 + ctx.f4.f64));
	// stfs f9,104(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fsubs f7,f4,f3
	ctx.f7.f64 = double(float(ctx.f4.f64 - ctx.f3.f64));
	// stfs f7,120(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// fsubs f8,f2,f10
	ctx.f8.f64 = double(float(ctx.f2.f64 - ctx.f10.f64));
	// stfs f8,112(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fsubs f4,f2,f29
	ctx.f4.f64 = double(float(ctx.f2.f64 - ctx.f29.f64));
	// stfs f4,128(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// fsubs f6,f1,f11
	ctx.f6.f64 = double(float(ctx.f1.f64 - ctx.f11.f64));
	// stfs f6,116(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fadds f5,f11,f1
	ctx.f5.f64 = double(float(ctx.f11.f64 + ctx.f1.f64));
	// stfs f5,124(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_830C5EB0:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c5eb0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C5EB0;
	// stfs f12,36(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r30.u32 + 36, temp.u32);
	// stfs f0,40(r30)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r30.u32 + 40, temp.u32);
	// stfs f13,44(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r30.u32 + 44, temp.u32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
loc_830C5EDC:
	// lwz r11,336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// addi r4,r11,8
	ctx.r4.s64 = ctx.r11.s64 + 8;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x830ff210
	ctx.lr = 0x830C5EF4;
	sub_830FF210(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r3,308(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 308);
	// bl 0x83073de8
	ctx.lr = 0x830C5F04;
	sub_83073DE8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// addi r12,r1,-48
	ctx.r12.s64 = ctx.r1.s64 + -48;
	// bl 0x82cb6b00
	ctx.lr = 0x830C5F14;
	__restfpr_15(ctx, base);
	// b 0x82cb1134
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C5F18"))) PPC_WEAK_FUNC(sub_830C5F18);
PPC_FUNC_IMPL(__imp__sub_830C5F18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e4
	ctx.lr = 0x830C5F20;
	__savegprlr_27(ctx, base);
	// addi r12,r1,-48
	ctx.r12.s64 = ctx.r1.s64 + -48;
	// bl 0x82cb6ab4
	ctx.lr = 0x830C5F28;
	__savefpr_15(ctx, base);
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lfs f30,6140(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6140);
	ctx.f30.f64 = double(temp.f32);
	// lfs f31,7676(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 7676);
	ctx.f31.f64 = double(temp.f32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f29,6380(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 6380);
	ctx.f29.f64 = double(temp.f32);
	// beq cr6,0x830c6144
	if (ctx.cr6.eq) goto loc_830C6144;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c6144
	if (ctx.cr6.eq) goto loc_830C6144;
	// lfs f0,252(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// lfs f13,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f11,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f10,f13,f0
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f9,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f9.f64 = double(temp.f32);
	// fmr f8,f11
	ctx.f8.f64 = ctx.f11.f64;
	// fmr f7,f9
	ctx.f7.f64 = ctx.f9.f64;
	// lfs f5,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f5.f64 = double(temp.f32);
	// lfs f6,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f28,f5,f0
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f3,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f2,f5,f11
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// lfs f1,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f4,f13,f11
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// lfs f27,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f27.f64 = double(temp.f32);
	// fmsubs f26,f6,f6,f29
	ctx.f26.f64 = double(float(ctx.f6.f64 * ctx.f6.f64 - ctx.f29.f64));
	// lfs f25,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f25.f64 = double(temp.f32);
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// lfs f24,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f24.f64 = double(temp.f32);
	// addi r9,r31,12
	ctx.r9.s64 = ctx.r31.s64 + 12;
	// fmuls f23,f12,f1
	ctx.f23.f64 = double(float(ctx.f12.f64 * ctx.f1.f64));
	// lfs f22,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f10,f3,f6,f10
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f6.f64 + ctx.f10.f64));
	// lfs f21,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f20,f8,f1
	ctx.f20.f64 = double(float(ctx.f8.f64 * ctx.f1.f64));
	// lfs f19,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f18,f7,f27
	ctx.f18.f64 = double(float(ctx.f7.f64 * ctx.f27.f64));
	// fmadds f28,f25,f6,f28
	ctx.f28.f64 = double(float(ctx.f25.f64 * ctx.f6.f64 + ctx.f28.f64));
	// fmadds f2,f13,f6,f2
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f6.f64 + ctx.f2.f64));
	// fmsubs f4,f5,f6,f4
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f6.f64 - ctx.f4.f64));
	// fmuls f17,f12,f24
	ctx.f17.f64 = double(float(ctx.f12.f64 * ctx.f24.f64));
	// fmuls f16,f26,f24
	ctx.f16.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// fmuls f15,f26,f1
	ctx.f15.f64 = double(float(ctx.f26.f64 * ctx.f1.f64));
	// fmadds f23,f7,f24,f23
	ctx.f23.f64 = double(float(ctx.f7.f64 * ctx.f24.f64 + ctx.f23.f64));
	// fmadds f10,f5,f9,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f9.f64 + ctx.f10.f64));
	// fmsubs f5,f12,f27,f20
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f27.f64 - ctx.f20.f64));
	// fmsubs f24,f8,f24,f18
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f24.f64 - ctx.f18.f64));
	// fmadds f28,f3,f11,f28
	ctx.f28.f64 = double(float(ctx.f3.f64 * ctx.f11.f64 + ctx.f28.f64));
	// fmadds f2,f25,f9,f2
	ctx.f2.f64 = double(float(ctx.f25.f64 * ctx.f9.f64 + ctx.f2.f64));
	// fnmsubs f4,f3,f9,f4
	ctx.f4.f64 = double(float(-(ctx.f3.f64 * ctx.f9.f64 - ctx.f4.f64)));
	// fmuls f26,f26,f27
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f27.f64));
	// fmsubs f1,f7,f1,f17
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f1.f64 - ctx.f17.f64));
	// fmadds f27,f8,f27,f23
	ctx.f27.f64 = double(float(ctx.f8.f64 * ctx.f27.f64 + ctx.f23.f64));
	// fnmsubs f11,f25,f11,f10
	ctx.f11.f64 = double(float(-(ctx.f25.f64 * ctx.f11.f64 - ctx.f10.f64)));
	// fmuls f10,f5,f6
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmuls f5,f24,f6
	ctx.f5.f64 = double(float(ctx.f24.f64 * ctx.f6.f64));
	// fnmsubs f3,f3,f0,f2
	ctx.f3.f64 = double(float(-(ctx.f3.f64 * ctx.f0.f64 - ctx.f2.f64)));
	// fnmsubs f2,f13,f9,f28
	ctx.f2.f64 = double(float(-(ctx.f13.f64 * ctx.f9.f64 - ctx.f28.f64)));
	// fnmsubs f4,f25,f0,f4
	ctx.f4.f64 = double(float(-(ctx.f25.f64 * ctx.f0.f64 - ctx.f4.f64)));
	// fmuls f1,f1,f6
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f6.f64));
	// fmuls f0,f7,f27
	ctx.f0.f64 = double(float(ctx.f7.f64 * ctx.f27.f64));
	// fmuls f13,f11,f11
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f11.f64));
	// fmuls f12,f12,f27
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f27.f64));
	// fadds f9,f15,f5
	ctx.f9.f64 = double(float(ctx.f15.f64 + ctx.f5.f64));
	// fadds f10,f16,f10
	ctx.f10.f64 = double(float(ctx.f16.f64 + ctx.f10.f64));
	// fmuls f7,f11,f3
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f3.f64));
	// fmuls f6,f2,f2
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f2.f64));
	// fmuls f5,f4,f2
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f2.f64));
	// fmuls f8,f27,f8
	ctx.f8.f64 = double(float(ctx.f27.f64 * ctx.f8.f64));
	// fadds f1,f26,f1
	ctx.f1.f64 = double(float(ctx.f26.f64 + ctx.f1.f64));
	// fmuls f28,f13,f31
	ctx.f28.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// fadds f13,f9,f12
	ctx.f13.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
	// fadds f0,f10,f0
	ctx.f0.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// fmuls f12,f7,f31
	ctx.f12.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// fmuls f10,f6,f31
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// fmuls f9,f5,f31
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// fadds f8,f1,f8
	ctx.f8.f64 = double(float(ctx.f1.f64 + ctx.f8.f64));
	// fsubs f7,f30,f28
	ctx.f7.f64 = double(float(ctx.f30.f64 - ctx.f28.f64));
	// fmuls f5,f13,f31
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// fmuls f6,f0,f31
	ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fsubs f1,f12,f9
	ctx.f1.f64 = double(float(ctx.f12.f64 - ctx.f9.f64));
	// stfs f1,100(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fmuls f8,f8,f31
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fsubs f7,f7,f10
	ctx.f7.f64 = double(float(ctx.f7.f64 - ctx.f10.f64));
	// stfs f7,96(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f13,f21,f5
	ctx.f13.f64 = double(float(ctx.f21.f64 + ctx.f5.f64));
	// fadds f0,f22,f6
	ctx.f0.f64 = double(float(ctx.f22.f64 + ctx.f6.f64));
	// fmuls f6,f2,f3
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f3.f64));
	// fmuls f5,f4,f11
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmuls f7,f4,f3
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// fmuls f1,f3,f3
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f11,f2,f11
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fadds f4,f9,f12
	ctx.f4.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
	// stfs f4,108(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fmuls f3,f6,f31
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// fmuls f2,f5,f31
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// fadds f12,f19,f8
	ctx.f12.f64 = double(float(ctx.f19.f64 + ctx.f8.f64));
	// fmuls f9,f7,f31
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// fnmsubs f1,f1,f31,f30
	ctx.f1.f64 = double(float(-(ctx.f1.f64 * ctx.f31.f64 - ctx.f30.f64)));
	// fmuls f11,f11,f31
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// fadds f8,f2,f3
	ctx.f8.f64 = double(float(ctx.f2.f64 + ctx.f3.f64));
	// stfs f8,104(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fsubs f6,f3,f2
	ctx.f6.f64 = double(float(ctx.f3.f64 - ctx.f2.f64));
	// stfs f6,120(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// fsubs f7,f1,f10
	ctx.f7.f64 = double(float(ctx.f1.f64 - ctx.f10.f64));
	// stfs f7,112(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fsubs f5,f11,f9
	ctx.f5.f64 = double(float(ctx.f11.f64 - ctx.f9.f64));
	// stfs f5,116(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fadds f4,f9,f11
	ctx.f4.f64 = double(float(ctx.f9.f64 + ctx.f11.f64));
	// stfs f4,124(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// fsubs f3,f1,f28
	ctx.f3.f64 = double(float(ctx.f1.f64 - ctx.f28.f64));
	// stfs f3,128(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830C6118:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c6118
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C6118;
	// stfs f12,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f0,40(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f13,44(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
loc_830C6144:
	// lfs f0,4(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// lwz r11,336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// lfs f13,52(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 52);
	ctx.f13.f64 = double(temp.f32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// fsubs f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// lfs f11,8(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,56(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 56);
	ctx.f10.f64 = double(temp.f32);
	// addi r3,r11,156
	ctx.r3.s64 = ctx.r11.s64 + 156;
	// lfs f9,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f9.f64 = double(temp.f32);
	// fsubs f8,f11,f10
	ctx.f8.f64 = double(float(ctx.f11.f64 - ctx.f10.f64));
	// lfs f7,28(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	ctx.f7.f64 = double(temp.f32);
	// addi r30,r31,12
	ctx.r30.s64 = ctx.r31.s64 + 12;
	// lfs f6,32(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,0(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,48(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,36(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	ctx.f3.f64 = double(temp.f32);
	// fsubs f2,f5,f4
	ctx.f2.f64 = double(float(ctx.f5.f64 - ctx.f4.f64));
	// lfs f1,40(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,44(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f13,f9,f12
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// lfs f11,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f10,f7,f12
	ctx.f10.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// lfs f9,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f7,f6,f12
	ctx.f7.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// lfs f6,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f5,f3,f8,f13
	ctx.f5.f64 = double(float(ctx.f3.f64 * ctx.f8.f64 + ctx.f13.f64));
	// fmadds f4,f1,f8,f10
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f8.f64 + ctx.f10.f64));
	// fmadds f3,f0,f8,f7
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f7.f64));
	// fmadds f1,f11,f2,f5
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f2.f64 + ctx.f5.f64));
	// stfs f1,80(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmadds f0,f9,f2,f4
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f2.f64 + ctx.f4.f64));
	// stfs f0,84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmadds f13,f6,f2,f3
	ctx.f13.f64 = double(float(ctx.f6.f64 * ctx.f2.f64 + ctx.f3.f64));
	// stfs f13,88(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// bl 0x82d54480
	ctx.lr = 0x830C61D0;
	sub_82D54480(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c61f0
	if (ctx.cr6.eq) goto loc_830C61F0;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// addi r12,r1,-48
	ctx.r12.s64 = ctx.r1.s64 + -48;
	// bl 0x82cb6b00
	ctx.lr = 0x830C61EC;
	__restfpr_15(ctx, base);
	// b 0x82cb1134
	__restgprlr_27(ctx, base);
	return;
loc_830C61F0:
	// lwz r28,268(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// lwz r3,308(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 308);
	// bl 0x83073d18
	ctx.lr = 0x830C61FC;
	sub_83073D18(ctx, base);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c63f4
	if (ctx.cr6.eq) goto loc_830C63F4;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c63f4
	if (ctx.cr6.eq) goto loc_830C63F4;
	// lfs f0,252(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// lfs f13,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f11,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f10,f13,f0
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f9,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f9.f64 = double(temp.f32);
	// fmr f8,f11
	ctx.f8.f64 = ctx.f11.f64;
	// fmr f7,f9
	ctx.f7.f64 = ctx.f9.f64;
	// lfs f5,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f5.f64 = double(temp.f32);
	// lfs f3,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f13,f11
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fmuls f2,f5,f9
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// lfs f6,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f6.f64 = double(temp.f32);
	// lfs f1,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f28,f3,f0
	ctx.f28.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// lfs f27,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f27.f64 = double(temp.f32);
	// fmsubs f29,f6,f6,f29
	ctx.f29.f64 = double(float(ctx.f6.f64 * ctx.f6.f64 - ctx.f29.f64));
	// lfs f26,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f26.f64 = double(temp.f32);
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// lfs f25,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f24,f12,f27
	ctx.f24.f64 = double(float(ctx.f12.f64 * ctx.f27.f64));
	// lfs f23,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f10,f1,f6,f10
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f6.f64 + ctx.f10.f64));
	// lfs f22,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f21,f8,f27
	ctx.f21.f64 = double(float(ctx.f8.f64 * ctx.f27.f64));
	// lfs f20,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f19,f7,f26
	ctx.f19.f64 = double(float(ctx.f7.f64 * ctx.f26.f64));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// fmsubs f4,f3,f6,f4
	ctx.f4.f64 = double(float(ctx.f3.f64 * ctx.f6.f64 - ctx.f4.f64));
	// fmadds f2,f13,f6,f2
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f6.f64 + ctx.f2.f64));
	// fmadds f28,f5,f6,f28
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f6.f64 + ctx.f28.f64));
	// fmuls f18,f12,f25
	ctx.f18.f64 = double(float(ctx.f12.f64 * ctx.f25.f64));
	// fmuls f17,f29,f25
	ctx.f17.f64 = double(float(ctx.f29.f64 * ctx.f25.f64));
	// fmuls f16,f29,f27
	ctx.f16.f64 = double(float(ctx.f29.f64 * ctx.f27.f64));
	// fmadds f24,f7,f25,f24
	ctx.f24.f64 = double(float(ctx.f7.f64 * ctx.f25.f64 + ctx.f24.f64));
	// fmadds f10,f3,f9,f10
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f9.f64 + ctx.f10.f64));
	// fmsubs f21,f12,f26,f21
	ctx.f21.f64 = double(float(ctx.f12.f64 * ctx.f26.f64 - ctx.f21.f64));
	// fmsubs f25,f8,f25,f19
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f25.f64 - ctx.f19.f64));
	// fnmsubs f4,f1,f9,f4
	ctx.f4.f64 = double(float(-(ctx.f1.f64 * ctx.f9.f64 - ctx.f4.f64)));
	// fmadds f3,f3,f11,f2
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f11.f64 + ctx.f2.f64));
	// fmadds f2,f1,f11,f28
	ctx.f2.f64 = double(float(ctx.f1.f64 * ctx.f11.f64 + ctx.f28.f64));
	// fmsubs f28,f7,f27,f18
	ctx.f28.f64 = double(float(ctx.f7.f64 * ctx.f27.f64 - ctx.f18.f64));
	// fmuls f29,f29,f26
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f26.f64));
	// fmadds f27,f26,f8,f24
	ctx.f27.f64 = double(float(ctx.f26.f64 * ctx.f8.f64 + ctx.f24.f64));
	// fnmsubs f11,f5,f11,f10
	ctx.f11.f64 = double(float(-(ctx.f5.f64 * ctx.f11.f64 - ctx.f10.f64)));
	// fmuls f10,f21,f6
	ctx.f10.f64 = double(float(ctx.f21.f64 * ctx.f6.f64));
	// fmuls f26,f25,f6
	ctx.f26.f64 = double(float(ctx.f25.f64 * ctx.f6.f64));
	// fnmsubs f5,f5,f0,f4
	ctx.f5.f64 = double(float(-(ctx.f5.f64 * ctx.f0.f64 - ctx.f4.f64)));
	// fnmsubs f4,f1,f0,f3
	ctx.f4.f64 = double(float(-(ctx.f1.f64 * ctx.f0.f64 - ctx.f3.f64)));
	// fnmsubs f3,f13,f9,f2
	ctx.f3.f64 = double(float(-(ctx.f13.f64 * ctx.f9.f64 - ctx.f2.f64)));
	// fmuls f2,f28,f6
	ctx.f2.f64 = double(float(ctx.f28.f64 * ctx.f6.f64));
	// fmuls f13,f12,f27
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f27.f64));
	// fmuls f0,f11,f11
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f11.f64));
	// fadds f12,f17,f10
	ctx.f12.f64 = double(float(ctx.f17.f64 + ctx.f10.f64));
	// fmuls f1,f7,f27
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f27.f64));
	// fadds f10,f16,f26
	ctx.f10.f64 = double(float(ctx.f16.f64 + ctx.f26.f64));
	// fmuls f9,f11,f4
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// fmuls f7,f3,f3
	ctx.f7.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// fadds f2,f29,f2
	ctx.f2.f64 = double(float(ctx.f29.f64 + ctx.f2.f64));
	// fmuls f6,f5,f3
	ctx.f6.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fmuls f8,f27,f8
	ctx.f8.f64 = double(float(ctx.f27.f64 * ctx.f8.f64));
	// fmuls f29,f0,f31
	ctx.f29.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fadds f1,f12,f1
	ctx.f1.f64 = double(float(ctx.f12.f64 + ctx.f1.f64));
	// fadds f0,f10,f13
	ctx.f0.f64 = double(float(ctx.f10.f64 + ctx.f13.f64));
	// fmuls f12,f9,f31
	ctx.f12.f64 = double(float(ctx.f9.f64 * ctx.f31.f64));
	// fmuls f10,f7,f31
	ctx.f10.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// fmuls f9,f6,f31
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// fadds f8,f2,f8
	ctx.f8.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// fsubs f7,f30,f29
	ctx.f7.f64 = double(float(ctx.f30.f64 - ctx.f29.f64));
	// fmuls f6,f1,f31
	ctx.f6.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// fmuls f2,f0,f31
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fsubs f1,f12,f9
	ctx.f1.f64 = double(float(ctx.f12.f64 - ctx.f9.f64));
	// stfs f1,100(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fmuls f8,f8,f31
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fsubs f7,f7,f10
	ctx.f7.f64 = double(float(ctx.f7.f64 - ctx.f10.f64));
	// stfs f7,96(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f0,f23,f6
	ctx.f0.f64 = double(float(ctx.f23.f64 + ctx.f6.f64));
	// fadds f13,f22,f2
	ctx.f13.f64 = double(float(ctx.f22.f64 + ctx.f2.f64));
	// fmuls f6,f3,f4
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f4.f64));
	// fmuls f2,f5,f11
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// fmuls f1,f4,f4
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f4.f64));
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// fmuls f11,f3,f11
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// li r9,9
	ctx.r9.s64 = 9;
	// fmuls f7,f5,f4
	ctx.f7.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// fmuls f3,f2,f31
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f4,f6,f31
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// fadds f5,f9,f12
	ctx.f5.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
	// stfs f5,108(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fadds f12,f8,f20
	ctx.f12.f64 = double(float(ctx.f8.f64 + ctx.f20.f64));
	// fnmsubs f2,f1,f31,f30
	ctx.f2.f64 = double(float(-(ctx.f1.f64 * ctx.f31.f64 - ctx.f30.f64)));
	// fmuls f1,f11,f31
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// fmuls f11,f7,f31
	ctx.f11.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// fadds f9,f3,f4
	ctx.f9.f64 = double(float(ctx.f3.f64 + ctx.f4.f64));
	// stfs f9,104(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fsubs f7,f4,f3
	ctx.f7.f64 = double(float(ctx.f4.f64 - ctx.f3.f64));
	// stfs f7,120(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// fsubs f8,f2,f10
	ctx.f8.f64 = double(float(ctx.f2.f64 - ctx.f10.f64));
	// stfs f8,112(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fsubs f4,f2,f29
	ctx.f4.f64 = double(float(ctx.f2.f64 - ctx.f29.f64));
	// stfs f4,128(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// fsubs f6,f1,f11
	ctx.f6.f64 = double(float(ctx.f1.f64 - ctx.f11.f64));
	// stfs f6,116(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fadds f5,f11,f1
	ctx.f5.f64 = double(float(ctx.f11.f64 + ctx.f1.f64));
	// stfs f5,124(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_830C63C8:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c63c8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C63C8;
	// stfs f12,36(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r30.u32 + 36, temp.u32);
	// stfs f0,40(r30)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r30.u32 + 40, temp.u32);
	// stfs f13,44(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r30.u32 + 44, temp.u32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
loc_830C63F4:
	// lwz r11,336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// addi r4,r11,8
	ctx.r4.s64 = ctx.r11.s64 + 8;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x830f9ae0
	ctx.lr = 0x830C640C;
	sub_830F9AE0(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r3,308(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 308);
	// bl 0x83073de8
	ctx.lr = 0x830C641C;
	sub_83073DE8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// addi r12,r1,-48
	ctx.r12.s64 = ctx.r1.s64 + -48;
	// bl 0x82cb6b00
	ctx.lr = 0x830C642C;
	__restfpr_15(ctx, base);
	// b 0x82cb1134
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C6430"))) PPC_WEAK_FUNC(sub_830C6430);
PPC_FUNC_IMPL(__imp__sub_830C6430) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e8
	ctx.lr = 0x830C6438;
	__savegprlr_28(ctx, base);
	// addi r12,r1,-40
	ctx.r12.s64 = ctx.r1.s64 + -40;
	// bl 0x82cb6ab0
	ctx.lr = 0x830C6440;
	__savefpr_14(ctx, base);
	// stwu r1,-400(r1)
	ea = -400 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lfs f10,16(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f11,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// fmr f4,f10
	ctx.f4.f64 = ctx.f10.f64;
	// lfs f8,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f8.f64 = double(temp.f32);
	// fmr f6,f11
	ctx.f6.f64 = ctx.f11.f64;
	// fmr f2,f8
	ctx.f2.f64 = ctx.f8.f64;
	// lfs f0,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fadds f9,f11,f0
	ctx.f9.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// lfs f12,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fadds f7,f10,f13
	ctx.f7.f64 = double(float(ctx.f10.f64 + ctx.f13.f64));
	// fadds f5,f8,f12
	ctx.f5.f64 = double(float(ctx.f8.f64 + ctx.f12.f64));
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f30,6380(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6380);
	ctx.f30.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// fsubs f1,f4,f13
	ctx.f1.f64 = double(float(ctx.f4.f64 - ctx.f13.f64));
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// fsubs f3,f6,f0
	ctx.f3.f64 = double(float(ctx.f6.f64 - ctx.f0.f64));
	// lfs f29,6140(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6140);
	ctx.f29.f64 = double(temp.f32);
	// fsubs f13,f2,f12
	ctx.f13.f64 = double(float(ctx.f2.f64 - ctx.f12.f64));
	// lfs f0,6048(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6048);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f11,f9,f30
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f30.f64));
	// lfs f31,7676(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 7676);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f12,f7,f30
	ctx.f12.f64 = double(float(ctx.f7.f64 * ctx.f30.f64));
	// stfs f29,168(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 168, temp.u32);
	// fmuls f10,f5,f30
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f30.f64));
	// stfs f11,144(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// stfs f12,148(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// addi r10,r4,12
	ctx.r10.s64 = ctx.r4.s64 + 12;
	// stfs f0,172(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stfs f10,152(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// stfs f0,176(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// fmuls f8,f1,f30
	ctx.f8.f64 = double(float(ctx.f1.f64 * ctx.f30.f64));
	// stfs f0,180(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f9,f3,f30
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f30.f64));
	// stfs f29,184(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// fmuls f7,f13,f30
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f30.f64));
	// stfs f0,188(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 188, temp.u32);
	// stfs f0,192(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 192, temp.u32);
	// stfs f0,196(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 196, temp.u32);
	// stfs f29,200(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 200, temp.u32);
	// stfs f9,156(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 156, temp.u32);
	// stfs f8,160(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// stfs f7,164(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// beq cr6,0x830c66f4
	if (ctx.cr6.eq) goto loc_830C66F4;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c66f4
	if (ctx.cr6.eq) goto loc_830C66F4;
	// lfs f0,252(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// lfs f13,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f10,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f13,f0
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f8,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f8.f64 = double(temp.f32);
	// fmr f7,f10
	ctx.f7.f64 = ctx.f10.f64;
	// fmr f6,f8
	ctx.f6.f64 = ctx.f8.f64;
	// lfs f4,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f1,f4,f10
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f10.f64));
	// lfs f5,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f27,f4,f0
	ctx.f27.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// lfs f2,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f3,f13,f10
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// lfs f28,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f28.f64 = double(temp.f32);
	// lfs f26,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f26.f64 = double(temp.f32);
	// fmsubs f25,f5,f5,f30
	ctx.f25.f64 = double(float(ctx.f5.f64 * ctx.f5.f64 - ctx.f30.f64));
	// lfs f24,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f24.f64 = double(temp.f32);
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// lfs f23,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f23.f64 = double(temp.f32);
	// addi r9,r31,12
	ctx.r9.s64 = ctx.r31.s64 + 12;
	// fmuls f22,f12,f28
	ctx.f22.f64 = double(float(ctx.f12.f64 * ctx.f28.f64));
	// lfs f21,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f9,f2,f5,f9
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f5.f64 + ctx.f9.f64));
	// lfs f20,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f19,f7,f28
	ctx.f19.f64 = double(float(ctx.f7.f64 * ctx.f28.f64));
	// lfs f18,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f17,f6,f26
	ctx.f17.f64 = double(float(ctx.f6.f64 * ctx.f26.f64));
	// fmadds f1,f13,f5,f1
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f5.f64 + ctx.f1.f64));
	// fmadds f27,f24,f5,f27
	ctx.f27.f64 = double(float(ctx.f24.f64 * ctx.f5.f64 + ctx.f27.f64));
	// fmsubs f3,f4,f5,f3
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f5.f64 - ctx.f3.f64));
	// fmuls f16,f12,f23
	ctx.f16.f64 = double(float(ctx.f12.f64 * ctx.f23.f64));
	// fmuls f15,f25,f23
	ctx.f15.f64 = double(float(ctx.f25.f64 * ctx.f23.f64));
	// fmuls f14,f25,f28
	ctx.f14.f64 = double(float(ctx.f25.f64 * ctx.f28.f64));
	// fmadds f22,f6,f23,f22
	ctx.f22.f64 = double(float(ctx.f6.f64 * ctx.f23.f64 + ctx.f22.f64));
	// fmadds f9,f4,f8,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f9.f64));
	// fmsubs f4,f12,f26,f19
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f26.f64 - ctx.f19.f64));
	// fmsubs f23,f7,f23,f17
	ctx.f23.f64 = double(float(ctx.f7.f64 * ctx.f23.f64 - ctx.f17.f64));
	// fmadds f1,f24,f8,f1
	ctx.f1.f64 = double(float(ctx.f24.f64 * ctx.f8.f64 + ctx.f1.f64));
	// fmadds f27,f2,f10,f27
	ctx.f27.f64 = double(float(ctx.f2.f64 * ctx.f10.f64 + ctx.f27.f64));
	// fnmsubs f3,f2,f8,f3
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f8.f64 - ctx.f3.f64)));
	// fmsubs f28,f6,f28,f16
	ctx.f28.f64 = double(float(ctx.f6.f64 * ctx.f28.f64 - ctx.f16.f64));
	// fmuls f25,f25,f26
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f26.f64));
	// fmadds f26,f26,f7,f22
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f7.f64 + ctx.f22.f64));
	// fnmsubs f10,f24,f10,f9
	ctx.f10.f64 = double(float(-(ctx.f24.f64 * ctx.f10.f64 - ctx.f9.f64)));
	// fmuls f9,f4,f5
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f5.f64));
	// fmuls f4,f23,f5
	ctx.f4.f64 = double(float(ctx.f23.f64 * ctx.f5.f64));
	// fnmsubs f2,f2,f0,f1
	ctx.f2.f64 = double(float(-(ctx.f2.f64 * ctx.f0.f64 - ctx.f1.f64)));
	// fnmsubs f1,f13,f8,f27
	ctx.f1.f64 = double(float(-(ctx.f13.f64 * ctx.f8.f64 - ctx.f27.f64)));
	// fnmsubs f3,f24,f0,f3
	ctx.f3.f64 = double(float(-(ctx.f24.f64 * ctx.f0.f64 - ctx.f3.f64)));
	// fmuls f0,f28,f5
	ctx.f0.f64 = double(float(ctx.f28.f64 * ctx.f5.f64));
	// fmuls f13,f6,f26
	ctx.f13.f64 = double(float(ctx.f6.f64 * ctx.f26.f64));
	// fmuls f8,f10,f10
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f10.f64));
	// fadds f5,f15,f9
	ctx.f5.f64 = double(float(ctx.f15.f64 + ctx.f9.f64));
	// fmuls f6,f12,f26
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f26.f64));
	// fadds f4,f14,f4
	ctx.f4.f64 = double(float(ctx.f14.f64 + ctx.f4.f64));
	// fmuls f12,f10,f2
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f2.f64));
	// fmuls f28,f3,f1
	ctx.f28.f64 = double(float(ctx.f3.f64 * ctx.f1.f64));
	// fmuls f7,f26,f7
	ctx.f7.f64 = double(float(ctx.f26.f64 * ctx.f7.f64));
	// fadds f0,f25,f0
	ctx.f0.f64 = double(float(ctx.f25.f64 + ctx.f0.f64));
	// fmuls f9,f1,f1
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f1.f64));
	// fmuls f8,f8,f31
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fadds f5,f5,f13
	ctx.f5.f64 = double(float(ctx.f5.f64 + ctx.f13.f64));
	// fadds f4,f4,f6
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f6.f64));
	// fmuls f12,f12,f31
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// fmuls f6,f28,f31
	ctx.f6.f64 = double(float(ctx.f28.f64 * ctx.f31.f64));
	// fadds f0,f0,f7
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f7.f64));
	// fmuls f9,f9,f31
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f31.f64));
	// fsubs f13,f29,f8
	ctx.f13.f64 = double(float(ctx.f29.f64 - ctx.f8.f64));
	// fmuls f7,f5,f31
	ctx.f7.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// fmuls f5,f4,f31
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// fsubs f4,f12,f6
	ctx.f4.f64 = double(float(ctx.f12.f64 - ctx.f6.f64));
	// stfs f4,100(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fmuls f4,f0,f31
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fsubs f0,f13,f9
	ctx.f0.f64 = double(float(ctx.f13.f64 - ctx.f9.f64));
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f0,f21,f7
	ctx.f0.f64 = double(float(ctx.f21.f64 + ctx.f7.f64));
	// fmuls f7,f1,f2
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f2.f64));
	// fadds f13,f20,f5
	ctx.f13.f64 = double(float(ctx.f20.f64 + ctx.f5.f64));
	// fmuls f5,f3,f10
	ctx.f5.f64 = double(float(ctx.f3.f64 * ctx.f10.f64));
	// fmuls f1,f1,f10
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f10.f64));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// fmuls f28,f2,f2
	ctx.f28.f64 = double(float(ctx.f2.f64 * ctx.f2.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f10,f3,f2
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f2.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fadds f6,f6,f12
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f12.f64));
	// stfs f6,108(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fadds f12,f18,f4
	ctx.f12.f64 = double(float(ctx.f18.f64 + ctx.f4.f64));
	// fmuls f4,f7,f31
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// fmuls f3,f5,f31
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// fmuls f1,f1,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// fnmsubs f2,f28,f31,f29
	ctx.f2.f64 = double(float(-(ctx.f28.f64 * ctx.f31.f64 - ctx.f29.f64)));
	// fmuls f10,f10,f31
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f31.f64));
	// fadds f7,f3,f4
	ctx.f7.f64 = double(float(ctx.f3.f64 + ctx.f4.f64));
	// stfs f7,104(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fsubs f5,f4,f3
	ctx.f5.f64 = double(float(ctx.f4.f64 - ctx.f3.f64));
	// stfs f5,120(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// fsubs f6,f2,f9
	ctx.f6.f64 = double(float(ctx.f2.f64 - ctx.f9.f64));
	// stfs f6,112(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fsubs f4,f1,f10
	ctx.f4.f64 = double(float(ctx.f1.f64 - ctx.f10.f64));
	// stfs f4,116(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fadds f3,f10,f1
	ctx.f3.f64 = double(float(ctx.f10.f64 + ctx.f1.f64));
	// stfs f3,124(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// fsubs f2,f2,f8
	ctx.f2.f64 = double(float(ctx.f2.f64 - ctx.f8.f64));
	// stfs f2,128(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830C66C8:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c66c8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C66C8;
	// stfs f12,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f0,40(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f13,44(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
loc_830C66F4:
	// lfs f0,52(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 52);
	ctx.f0.f64 = double(temp.f32);
	// lwz r11,336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// lfs f13,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f13.f64 = double(temp.f32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// lfs f4,48(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	ctx.f4.f64 = double(temp.f32);
	// lfs f9,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f9.f64 = double(temp.f32);
	// fsubs f2,f11,f4
	ctx.f2.f64 = double(float(ctx.f11.f64 - ctx.f4.f64));
	// lfs f10,56(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 56);
	ctx.f10.f64 = double(temp.f32);
	// addi r3,r11,156
	ctx.r3.s64 = ctx.r11.s64 + 156;
	// lfs f8,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f8.f64 = double(temp.f32);
	// addi r30,r31,12
	ctx.r30.s64 = ctx.r31.s64 + 12;
	// lfs f7,28(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	ctx.f7.f64 = double(temp.f32);
	// fsubs f6,f8,f10
	ctx.f6.f64 = double(float(ctx.f8.f64 - ctx.f10.f64));
	// lfs f5,32(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	ctx.f5.f64 = double(temp.f32);
	// lfs f3,36(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,40(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,44(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f11,f9,f12
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// lfs f10,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f7,f12
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// lfs f8,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f5,f12
	ctx.f7.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// fmadds f5,f3,f6,f11
	ctx.f5.f64 = double(float(ctx.f3.f64 * ctx.f6.f64 + ctx.f11.f64));
	// fmadds f4,f1,f6,f9
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f6.f64 + ctx.f9.f64));
	// fmadds f3,f0,f6,f7
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f6.f64 + ctx.f7.f64));
	// fmadds f1,f2,f13,f5
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f13.f64 + ctx.f5.f64));
	// stfs f1,80(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmadds f0,f10,f2,f4
	ctx.f0.f64 = double(float(ctx.f10.f64 * ctx.f2.f64 + ctx.f4.f64));
	// stfs f0,84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmadds f13,f8,f2,f3
	ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f2.f64 + ctx.f3.f64));
	// stfs f13,88(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// bl 0x82d54480
	ctx.lr = 0x830C677C;
	sub_82D54480(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c679c
	if (ctx.cr6.eq) goto loc_830C679C;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// addi r12,r1,-40
	ctx.r12.s64 = ctx.r1.s64 + -40;
	// bl 0x82cb6afc
	ctx.lr = 0x830C6798;
	__restfpr_14(ctx, base);
	// b 0x82cb1138
	__restgprlr_28(ctx, base);
	return;
loc_830C679C:
	// lwz r28,268(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// lwz r3,308(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 308);
	// bl 0x83073d18
	ctx.lr = 0x830C67A8;
	sub_83073D18(ctx, base);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c69a0
	if (ctx.cr6.eq) goto loc_830C69A0;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c69a0
	if (ctx.cr6.eq) goto loc_830C69A0;
	// lfs f0,252(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// lfs f13,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f11,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f10,f13,f0
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f9,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f9.f64 = double(temp.f32);
	// fmr f8,f11
	ctx.f8.f64 = ctx.f11.f64;
	// fmr f7,f9
	ctx.f7.f64 = ctx.f9.f64;
	// lfs f5,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f5.f64 = double(temp.f32);
	// lfs f6,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f28,f5,f0
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f3,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f2,f5,f11
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// lfs f1,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f4,f13,f11
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// lfs f27,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f27.f64 = double(temp.f32);
	// fmsubs f30,f6,f6,f30
	ctx.f30.f64 = double(float(ctx.f6.f64 * ctx.f6.f64 - ctx.f30.f64));
	// lfs f26,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f26.f64 = double(temp.f32);
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// lfs f25,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f24,f12,f1
	ctx.f24.f64 = double(float(ctx.f12.f64 * ctx.f1.f64));
	// lfs f23,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f10,f3,f6,f10
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f6.f64 + ctx.f10.f64));
	// lfs f22,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f21,f1,f8
	ctx.f21.f64 = double(float(ctx.f1.f64 * ctx.f8.f64));
	// lfs f20,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f19,f7,f27
	ctx.f19.f64 = double(float(ctx.f7.f64 * ctx.f27.f64));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// fmadds f28,f26,f6,f28
	ctx.f28.f64 = double(float(ctx.f26.f64 * ctx.f6.f64 + ctx.f28.f64));
	// fmadds f2,f13,f6,f2
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f6.f64 + ctx.f2.f64));
	// fmsubs f4,f5,f6,f4
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f6.f64 - ctx.f4.f64));
	// fmuls f18,f12,f25
	ctx.f18.f64 = double(float(ctx.f12.f64 * ctx.f25.f64));
	// fmuls f17,f25,f30
	ctx.f17.f64 = double(float(ctx.f25.f64 * ctx.f30.f64));
	// fmuls f16,f1,f30
	ctx.f16.f64 = double(float(ctx.f1.f64 * ctx.f30.f64));
	// fmadds f24,f7,f25,f24
	ctx.f24.f64 = double(float(ctx.f7.f64 * ctx.f25.f64 + ctx.f24.f64));
	// fmadds f10,f5,f9,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f9.f64 + ctx.f10.f64));
	// fmsubs f5,f12,f27,f21
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f27.f64 - ctx.f21.f64));
	// fmsubs f25,f25,f8,f19
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f8.f64 - ctx.f19.f64));
	// fmadds f28,f3,f11,f28
	ctx.f28.f64 = double(float(ctx.f3.f64 * ctx.f11.f64 + ctx.f28.f64));
	// fmadds f2,f26,f9,f2
	ctx.f2.f64 = double(float(ctx.f26.f64 * ctx.f9.f64 + ctx.f2.f64));
	// fnmsubs f4,f3,f9,f4
	ctx.f4.f64 = double(float(-(ctx.f3.f64 * ctx.f9.f64 - ctx.f4.f64)));
	// fmsubs f1,f7,f1,f18
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f1.f64 - ctx.f18.f64));
	// fmuls f30,f27,f30
	ctx.f30.f64 = double(float(ctx.f27.f64 * ctx.f30.f64));
	// fmadds f27,f27,f8,f24
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f8.f64 + ctx.f24.f64));
	// fnmsubs f11,f26,f11,f10
	ctx.f11.f64 = double(float(-(ctx.f26.f64 * ctx.f11.f64 - ctx.f10.f64)));
	// fmuls f10,f5,f6
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmuls f5,f25,f6
	ctx.f5.f64 = double(float(ctx.f25.f64 * ctx.f6.f64));
	// fnmsubs f3,f3,f0,f2
	ctx.f3.f64 = double(float(-(ctx.f3.f64 * ctx.f0.f64 - ctx.f2.f64)));
	// fnmsubs f2,f13,f9,f28
	ctx.f2.f64 = double(float(-(ctx.f13.f64 * ctx.f9.f64 - ctx.f28.f64)));
	// fmuls f1,f1,f6
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f6.f64));
	// fnmsubs f4,f26,f0,f4
	ctx.f4.f64 = double(float(-(ctx.f26.f64 * ctx.f0.f64 - ctx.f4.f64)));
	// fmuls f0,f7,f27
	ctx.f0.f64 = double(float(ctx.f7.f64 * ctx.f27.f64));
	// fmuls f13,f11,f11
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f11.f64));
	// fmuls f12,f12,f27
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f27.f64));
	// fadds f9,f16,f5
	ctx.f9.f64 = double(float(ctx.f16.f64 + ctx.f5.f64));
	// fadds f10,f17,f10
	ctx.f10.f64 = double(float(ctx.f17.f64 + ctx.f10.f64));
	// fmuls f7,f11,f3
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f3.f64));
	// fmuls f6,f2,f2
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f2.f64));
	// fadds f1,f30,f1
	ctx.f1.f64 = double(float(ctx.f30.f64 + ctx.f1.f64));
	// fmuls f5,f4,f2
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f2.f64));
	// fmuls f8,f27,f8
	ctx.f8.f64 = double(float(ctx.f27.f64 * ctx.f8.f64));
	// fmuls f30,f13,f31
	ctx.f30.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// fadds f13,f9,f12
	ctx.f13.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
	// fadds f0,f10,f0
	ctx.f0.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// fmuls f12,f7,f31
	ctx.f12.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// fmuls f10,f6,f31
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// fmuls f9,f5,f31
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// fadds f8,f1,f8
	ctx.f8.f64 = double(float(ctx.f1.f64 + ctx.f8.f64));
	// fsubs f7,f29,f30
	ctx.f7.f64 = double(float(ctx.f29.f64 - ctx.f30.f64));
	// fmuls f5,f13,f31
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// fmuls f6,f0,f31
	ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fsubs f1,f12,f9
	ctx.f1.f64 = double(float(ctx.f12.f64 - ctx.f9.f64));
	// stfs f1,100(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fmuls f8,f8,f31
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fsubs f7,f7,f10
	ctx.f7.f64 = double(float(ctx.f7.f64 - ctx.f10.f64));
	// stfs f7,96(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f13,f22,f5
	ctx.f13.f64 = double(float(ctx.f22.f64 + ctx.f5.f64));
	// fadds f0,f23,f6
	ctx.f0.f64 = double(float(ctx.f23.f64 + ctx.f6.f64));
	// fmuls f6,f2,f3
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f3.f64));
	// fmuls f5,f4,f11
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmuls f7,f4,f3
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// fmuls f1,f3,f3
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// li r9,9
	ctx.r9.s64 = 9;
	// fmuls f11,f2,f11
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// fadds f4,f9,f12
	ctx.f4.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
	// stfs f4,108(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fmuls f3,f6,f31
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// fmuls f2,f5,f31
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// fadds f12,f8,f20
	ctx.f12.f64 = double(float(ctx.f8.f64 + ctx.f20.f64));
	// fmuls f9,f7,f31
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// fnmsubs f1,f1,f31,f29
	ctx.f1.f64 = double(float(-(ctx.f1.f64 * ctx.f31.f64 - ctx.f29.f64)));
	// fmuls f11,f11,f31
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// fadds f8,f2,f3
	ctx.f8.f64 = double(float(ctx.f2.f64 + ctx.f3.f64));
	// stfs f8,104(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fsubs f6,f3,f2
	ctx.f6.f64 = double(float(ctx.f3.f64 - ctx.f2.f64));
	// stfs f6,120(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// fsubs f7,f1,f10
	ctx.f7.f64 = double(float(ctx.f1.f64 - ctx.f10.f64));
	// stfs f7,112(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fsubs f5,f11,f9
	ctx.f5.f64 = double(float(ctx.f11.f64 - ctx.f9.f64));
	// stfs f5,116(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fadds f4,f9,f11
	ctx.f4.f64 = double(float(ctx.f9.f64 + ctx.f11.f64));
	// stfs f4,124(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// fsubs f3,f1,f30
	ctx.f3.f64 = double(float(ctx.f1.f64 - ctx.f30.f64));
	// stfs f3,128(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_830C6974:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c6974
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C6974;
	// stfs f0,40(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r30.u32 + 40, temp.u32);
	// stfs f13,44(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r30.u32 + 44, temp.u32);
	// stfs f12,36(r30)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r30.u32 + 36, temp.u32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
loc_830C69A0:
	// lwz r11,336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// addi r4,r11,8
	ctx.r4.s64 = ctx.r11.s64 + 8;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// bl 0x830f9ae0
	ctx.lr = 0x830C69B8;
	sub_830F9AE0(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r3,308(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 308);
	// bl 0x83073de8
	ctx.lr = 0x830C69C8;
	sub_83073DE8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// addi r12,r1,-40
	ctx.r12.s64 = ctx.r1.s64 + -40;
	// bl 0x82cb6afc
	ctx.lr = 0x830C69D8;
	__restfpr_14(ctx, base);
	// b 0x82cb1138
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C69DC"))) PPC_WEAK_FUNC(sub_830C69DC);
PPC_FUNC_IMPL(__imp__sub_830C69DC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C69E0"))) PPC_WEAK_FUNC(sub_830C69E0);
PPC_FUNC_IMPL(__imp__sub_830C69E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e4
	ctx.lr = 0x830C69E8;
	__savegprlr_27(ctx, base);
	// addi r12,r1,-48
	ctx.r12.s64 = ctx.r1.s64 + -48;
	// bl 0x82cb6ab0
	ctx.lr = 0x830C69F0;
	__savefpr_14(ctx, base);
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f0,0(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,12(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lfs f12,16(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 16);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f0,f13
	ctx.f11.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// lfs f10,4(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f9,20(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 20);
	ctx.f9.f64 = double(temp.f32);
	// fadds f8,f12,f10
	ctx.f8.f64 = double(float(ctx.f12.f64 + ctx.f10.f64));
	// lfs f7,8(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 8);
	ctx.f7.f64 = double(temp.f32);
	// fadds f6,f9,f7
	ctx.f6.f64 = double(float(ctx.f9.f64 + ctx.f7.f64));
	// lfs f30,6380(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6380);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,6140(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6140);
	ctx.f29.f64 = double(temp.f32);
	// lfs f31,7676(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 7676);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f11,f11,f30
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f30.f64));
	// stfs f11,80(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmuls f5,f8,f30
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f30.f64));
	// stfs f5,84(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmuls f4,f6,f30
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f30.f64));
	// stfs f4,88(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// beq cr6,0x830c6c48
	if (ctx.cr6.eq) goto loc_830C6C48;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c6c48
	if (ctx.cr6.eq) goto loc_830C6C48;
	// lfs f0,252(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// lfs f13,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f10,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f13,f0
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f8,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f8.f64 = double(temp.f32);
	// fmr f7,f10
	ctx.f7.f64 = ctx.f10.f64;
	// fmr f6,f8
	ctx.f6.f64 = ctx.f8.f64;
	// lfs f4,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f1,f4,f10
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f10.f64));
	// lfs f5,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f27,f4,f0
	ctx.f27.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// lfs f2,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f3,f13,f10
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// lfs f28,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f28.f64 = double(temp.f32);
	// lfs f26,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f26.f64 = double(temp.f32);
	// fmsubs f25,f5,f5,f30
	ctx.f25.f64 = double(float(ctx.f5.f64 * ctx.f5.f64 - ctx.f30.f64));
	// lfs f24,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f24.f64 = double(temp.f32);
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// lfs f23,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f23.f64 = double(temp.f32);
	// addi r9,r31,12
	ctx.r9.s64 = ctx.r31.s64 + 12;
	// fmuls f22,f12,f28
	ctx.f22.f64 = double(float(ctx.f12.f64 * ctx.f28.f64));
	// lfs f21,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f9,f2,f5,f9
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f5.f64 + ctx.f9.f64));
	// lfs f20,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f19,f7,f28
	ctx.f19.f64 = double(float(ctx.f7.f64 * ctx.f28.f64));
	// lfs f18,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f17,f6,f26
	ctx.f17.f64 = double(float(ctx.f6.f64 * ctx.f26.f64));
	// fmadds f1,f13,f5,f1
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f5.f64 + ctx.f1.f64));
	// fmadds f27,f24,f5,f27
	ctx.f27.f64 = double(float(ctx.f24.f64 * ctx.f5.f64 + ctx.f27.f64));
	// fmsubs f3,f4,f5,f3
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f5.f64 - ctx.f3.f64));
	// fmuls f16,f12,f23
	ctx.f16.f64 = double(float(ctx.f12.f64 * ctx.f23.f64));
	// fmuls f15,f25,f23
	ctx.f15.f64 = double(float(ctx.f25.f64 * ctx.f23.f64));
	// fmuls f14,f25,f28
	ctx.f14.f64 = double(float(ctx.f25.f64 * ctx.f28.f64));
	// fmadds f22,f6,f23,f22
	ctx.f22.f64 = double(float(ctx.f6.f64 * ctx.f23.f64 + ctx.f22.f64));
	// fmadds f9,f4,f8,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f9.f64));
	// fmsubs f4,f12,f26,f19
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f26.f64 - ctx.f19.f64));
	// fmsubs f23,f7,f23,f17
	ctx.f23.f64 = double(float(ctx.f7.f64 * ctx.f23.f64 - ctx.f17.f64));
	// fmadds f1,f24,f8,f1
	ctx.f1.f64 = double(float(ctx.f24.f64 * ctx.f8.f64 + ctx.f1.f64));
	// fmadds f27,f2,f10,f27
	ctx.f27.f64 = double(float(ctx.f2.f64 * ctx.f10.f64 + ctx.f27.f64));
	// fnmsubs f3,f2,f8,f3
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f8.f64 - ctx.f3.f64)));
	// fmsubs f28,f6,f28,f16
	ctx.f28.f64 = double(float(ctx.f6.f64 * ctx.f28.f64 - ctx.f16.f64));
	// fmuls f25,f25,f26
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f26.f64));
	// fmadds f26,f7,f26,f22
	ctx.f26.f64 = double(float(ctx.f7.f64 * ctx.f26.f64 + ctx.f22.f64));
	// fnmsubs f10,f24,f10,f9
	ctx.f10.f64 = double(float(-(ctx.f24.f64 * ctx.f10.f64 - ctx.f9.f64)));
	// fmuls f9,f4,f5
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f5.f64));
	// fmuls f4,f23,f5
	ctx.f4.f64 = double(float(ctx.f23.f64 * ctx.f5.f64));
	// fnmsubs f2,f2,f0,f1
	ctx.f2.f64 = double(float(-(ctx.f2.f64 * ctx.f0.f64 - ctx.f1.f64)));
	// fnmsubs f1,f13,f8,f27
	ctx.f1.f64 = double(float(-(ctx.f13.f64 * ctx.f8.f64 - ctx.f27.f64)));
	// fnmsubs f3,f24,f0,f3
	ctx.f3.f64 = double(float(-(ctx.f24.f64 * ctx.f0.f64 - ctx.f3.f64)));
	// fmuls f0,f28,f5
	ctx.f0.f64 = double(float(ctx.f28.f64 * ctx.f5.f64));
	// fmuls f13,f6,f26
	ctx.f13.f64 = double(float(ctx.f6.f64 * ctx.f26.f64));
	// fmuls f8,f10,f10
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f10.f64));
	// fadds f5,f15,f9
	ctx.f5.f64 = double(float(ctx.f15.f64 + ctx.f9.f64));
	// fmuls f6,f12,f26
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f26.f64));
	// fadds f4,f14,f4
	ctx.f4.f64 = double(float(ctx.f14.f64 + ctx.f4.f64));
	// fmuls f12,f10,f2
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f2.f64));
	// fmuls f28,f3,f1
	ctx.f28.f64 = double(float(ctx.f3.f64 * ctx.f1.f64));
	// fmuls f7,f26,f7
	ctx.f7.f64 = double(float(ctx.f26.f64 * ctx.f7.f64));
	// fadds f0,f25,f0
	ctx.f0.f64 = double(float(ctx.f25.f64 + ctx.f0.f64));
	// fmuls f9,f1,f1
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f1.f64));
	// fmuls f8,f8,f31
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// fadds f5,f5,f13
	ctx.f5.f64 = double(float(ctx.f5.f64 + ctx.f13.f64));
	// fadds f4,f4,f6
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f6.f64));
	// fmuls f12,f12,f31
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// fmuls f6,f28,f31
	ctx.f6.f64 = double(float(ctx.f28.f64 * ctx.f31.f64));
	// fadds f0,f0,f7
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f7.f64));
	// fmuls f9,f9,f31
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f31.f64));
	// fsubs f13,f29,f8
	ctx.f13.f64 = double(float(ctx.f29.f64 - ctx.f8.f64));
	// fmuls f7,f5,f31
	ctx.f7.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// fmuls f5,f4,f31
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// fsubs f4,f12,f6
	ctx.f4.f64 = double(float(ctx.f12.f64 - ctx.f6.f64));
	// stfs f4,100(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fmuls f4,f0,f31
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fsubs f0,f13,f9
	ctx.f0.f64 = double(float(ctx.f13.f64 - ctx.f9.f64));
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f0,f21,f7
	ctx.f0.f64 = double(float(ctx.f21.f64 + ctx.f7.f64));
	// fmuls f7,f1,f2
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f2.f64));
	// fadds f13,f20,f5
	ctx.f13.f64 = double(float(ctx.f20.f64 + ctx.f5.f64));
	// fmuls f5,f3,f10
	ctx.f5.f64 = double(float(ctx.f3.f64 * ctx.f10.f64));
	// fmuls f1,f1,f10
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f10.f64));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// fmuls f28,f2,f2
	ctx.f28.f64 = double(float(ctx.f2.f64 * ctx.f2.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f10,f3,f2
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f2.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fadds f6,f6,f12
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f12.f64));
	// stfs f6,108(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fadds f12,f18,f4
	ctx.f12.f64 = double(float(ctx.f18.f64 + ctx.f4.f64));
	// fmuls f4,f7,f31
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// fmuls f3,f5,f31
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// fmuls f1,f1,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// fnmsubs f2,f28,f31,f29
	ctx.f2.f64 = double(float(-(ctx.f28.f64 * ctx.f31.f64 - ctx.f29.f64)));
	// fmuls f10,f10,f31
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f31.f64));
	// fadds f7,f3,f4
	ctx.f7.f64 = double(float(ctx.f3.f64 + ctx.f4.f64));
	// stfs f7,104(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fsubs f5,f4,f3
	ctx.f5.f64 = double(float(ctx.f4.f64 - ctx.f3.f64));
	// stfs f5,120(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// fsubs f6,f2,f9
	ctx.f6.f64 = double(float(ctx.f2.f64 - ctx.f9.f64));
	// stfs f6,112(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fsubs f4,f1,f10
	ctx.f4.f64 = double(float(ctx.f1.f64 - ctx.f10.f64));
	// stfs f4,116(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fadds f3,f10,f1
	ctx.f3.f64 = double(float(ctx.f10.f64 + ctx.f1.f64));
	// stfs f3,124(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// fsubs f2,f2,f8
	ctx.f2.f64 = double(float(ctx.f2.f64 - ctx.f8.f64));
	// stfs f2,128(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830C6C1C:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c6c1c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C6C1C;
	// stfs f12,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f0,40(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f13,44(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
loc_830C6C48:
	// lfs f0,84(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// lwz r11,336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// lfs f13,52(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 52);
	ctx.f13.f64 = double(temp.f32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// fsubs f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// lfs f10,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,56(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 56);
	ctx.f9.f64 = double(temp.f32);
	// addi r3,r11,156
	ctx.r3.s64 = ctx.r11.s64 + 156;
	// lfs f4,48(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	ctx.f4.f64 = double(temp.f32);
	// fsubs f7,f10,f9
	ctx.f7.f64 = double(float(ctx.f10.f64 - ctx.f9.f64));
	// lfs f8,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f8.f64 = double(temp.f32);
	// fsubs f2,f11,f4
	ctx.f2.f64 = double(float(ctx.f11.f64 - ctx.f4.f64));
	// lfs f6,28(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	ctx.f6.f64 = double(temp.f32);
	// addi r30,r31,12
	ctx.r30.s64 = ctx.r31.s64 + 12;
	// lfs f5,32(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	ctx.f5.f64 = double(temp.f32);
	// lfs f3,36(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,40(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,44(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f11,f8,f12
	ctx.f11.f64 = double(float(ctx.f8.f64 * ctx.f12.f64));
	// lfs f10,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f6,f12
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// lfs f8,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f6,f5,f12
	ctx.f6.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// fmadds f5,f3,f7,f11
	ctx.f5.f64 = double(float(ctx.f3.f64 * ctx.f7.f64 + ctx.f11.f64));
	// fmadds f4,f1,f7,f9
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f7.f64 + ctx.f9.f64));
	// fmadds f3,f0,f7,f6
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f7.f64 + ctx.f6.f64));
	// fmadds f1,f2,f13,f5
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f13.f64 + ctx.f5.f64));
	// stfs f1,80(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmadds f0,f10,f2,f4
	ctx.f0.f64 = double(float(ctx.f10.f64 * ctx.f2.f64 + ctx.f4.f64));
	// stfs f0,84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmadds f13,f8,f2,f3
	ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f2.f64 + ctx.f3.f64));
	// stfs f13,88(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// bl 0x82d54480
	ctx.lr = 0x830C6CD0;
	sub_82D54480(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c6cf0
	if (ctx.cr6.eq) goto loc_830C6CF0;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// addi r12,r1,-48
	ctx.r12.s64 = ctx.r1.s64 + -48;
	// bl 0x82cb6afc
	ctx.lr = 0x830C6CEC;
	__restfpr_14(ctx, base);
	// b 0x82cb1134
	__restgprlr_27(ctx, base);
	return;
loc_830C6CF0:
	// lwz r28,268(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// lwz r3,308(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 308);
	// bl 0x83073d18
	ctx.lr = 0x830C6CFC;
	sub_83073D18(ctx, base);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c6ef4
	if (ctx.cr6.eq) goto loc_830C6EF4;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c6ef4
	if (ctx.cr6.eq) goto loc_830C6EF4;
	// lfs f0,248(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// lfs f13,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f11,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f10,f13,f0
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// lfs f7,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f7.f64 = double(temp.f32);
	// lfs f5,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f4,f7,f0
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f3,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f2,f5,f11
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// lfs f8,252(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f28,f3,f11
	ctx.f28.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// lfs f1,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f1.f64 = double(temp.f32);
	// fmr f6,f8
	ctx.f6.f64 = ctx.f8.f64;
	// lfs f27,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f27.f64 = double(temp.f32);
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// lfs f26,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f19,f12,f27
	ctx.f19.f64 = double(float(ctx.f12.f64 * ctx.f27.f64));
	// lfs f23,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f10,f5,f8,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f8.f64 + ctx.f10.f64));
	// lfs f22,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f21,f9,f1
	ctx.f21.f64 = double(float(ctx.f9.f64 * ctx.f1.f64));
	// lfs f20,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f24,f12,f26
	ctx.f24.f64 = double(float(ctx.f12.f64 * ctx.f26.f64));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// fmadds f4,f13,f11,f4
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f4.f64));
	// fmsubs f2,f13,f25,f2
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f25.f64 - ctx.f2.f64));
	// fmadds f13,f13,f8,f28
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f8.f64 + ctx.f28.f64));
	// fmuls f28,f6,f27
	ctx.f28.f64 = double(float(ctx.f6.f64 * ctx.f27.f64));
	// fmsubs f30,f25,f25,f30
	ctx.f30.f64 = double(float(ctx.f25.f64 * ctx.f25.f64 - ctx.f30.f64));
	// fmadds f19,f9,f26,f19
	ctx.f19.f64 = double(float(ctx.f9.f64 * ctx.f26.f64 + ctx.f19.f64));
	// fmadds f10,f3,f25,f10
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f25.f64 + ctx.f10.f64));
	// fmsubs f21,f6,f26,f21
	ctx.f21.f64 = double(float(ctx.f6.f64 * ctx.f26.f64 - ctx.f21.f64));
	// fmsubs f24,f9,f27,f24
	ctx.f24.f64 = double(float(ctx.f9.f64 * ctx.f27.f64 - ctx.f24.f64));
	// fmadds f4,f5,f25,f4
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f25.f64 + ctx.f4.f64));
	// fnmsubs f2,f3,f0,f2
	ctx.f2.f64 = double(float(-(ctx.f3.f64 * ctx.f0.f64 - ctx.f2.f64)));
	// fmadds f13,f7,f25,f13
	ctx.f13.f64 = double(float(ctx.f7.f64 * ctx.f25.f64 + ctx.f13.f64));
	// fmsubs f28,f12,f1,f28
	ctx.f28.f64 = double(float(ctx.f12.f64 * ctx.f1.f64 - ctx.f28.f64));
	// fmuls f18,f30,f1
	ctx.f18.f64 = double(float(ctx.f30.f64 * ctx.f1.f64));
	// fmuls f27,f30,f27
	ctx.f27.f64 = double(float(ctx.f30.f64 * ctx.f27.f64));
	// fmadds f1,f6,f1,f19
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f1.f64 + ctx.f19.f64));
	// fnmsubs f11,f7,f11,f10
	ctx.f11.f64 = double(float(-(ctx.f7.f64 * ctx.f11.f64 - ctx.f10.f64)));
	// fmuls f10,f21,f25
	ctx.f10.f64 = double(float(ctx.f21.f64 * ctx.f25.f64));
	// fmuls f24,f24,f25
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f25.f64));
	// fnmsubs f4,f3,f8,f4
	ctx.f4.f64 = double(float(-(ctx.f3.f64 * ctx.f8.f64 - ctx.f4.f64)));
	// fnmsubs f3,f7,f8,f2
	ctx.f3.f64 = double(float(-(ctx.f7.f64 * ctx.f8.f64 - ctx.f2.f64)));
	// fnmsubs f2,f5,f0,f13
	ctx.f2.f64 = double(float(-(ctx.f5.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f13,f28,f25
	ctx.f13.f64 = double(float(ctx.f28.f64 * ctx.f25.f64));
	// fmuls f0,f30,f26
	ctx.f0.f64 = double(float(ctx.f30.f64 * ctx.f26.f64));
	// fmuls f12,f12,f1
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f1.f64));
	// fmuls f7,f11,f11
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f11.f64));
	// fadds f5,f27,f10
	ctx.f5.f64 = double(float(ctx.f27.f64 + ctx.f10.f64));
	// fmuls f10,f6,f1
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f1.f64));
	// fadds f8,f18,f24
	ctx.f8.f64 = double(float(ctx.f18.f64 + ctx.f24.f64));
	// fmuls f1,f1,f9
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f9.f64));
	// fmuls f6,f11,f4
	ctx.f6.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// fmuls f28,f3,f2
	ctx.f28.f64 = double(float(ctx.f3.f64 * ctx.f2.f64));
	// fadds f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// fmuls f30,f2,f2
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f2.f64));
	// fmuls f9,f7,f31
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// fadds f7,f5,f12
	ctx.f7.f64 = double(float(ctx.f5.f64 + ctx.f12.f64));
	// fadds f5,f8,f10
	ctx.f5.f64 = double(float(ctx.f8.f64 + ctx.f10.f64));
	// fmuls f12,f6,f31
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// fmuls f8,f28,f31
	ctx.f8.f64 = double(float(ctx.f28.f64 * ctx.f31.f64));
	// fadds f6,f0,f1
	ctx.f6.f64 = double(float(ctx.f0.f64 + ctx.f1.f64));
	// fmuls f10,f30,f31
	ctx.f10.f64 = double(float(ctx.f30.f64 * ctx.f31.f64));
	// fsubs f1,f29,f9
	ctx.f1.f64 = double(float(ctx.f29.f64 - ctx.f9.f64));
	// fmuls f0,f7,f31
	ctx.f0.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// fmuls f13,f5,f31
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// fsubs f7,f12,f8
	ctx.f7.f64 = double(float(ctx.f12.f64 - ctx.f8.f64));
	// stfs f7,100(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fmuls f6,f6,f31
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// fmuls f7,f3,f11
	ctx.f7.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// fsubs f5,f1,f10
	ctx.f5.f64 = double(float(ctx.f1.f64 - ctx.f10.f64));
	// stfs f5,96(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f0,f23,f0
	ctx.f0.f64 = double(float(ctx.f23.f64 + ctx.f0.f64));
	// fmuls f1,f2,f4
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f4.f64));
	// fadds f13,f22,f13
	ctx.f13.f64 = double(float(ctx.f22.f64 + ctx.f13.f64));
	// fmuls f2,f2,f11
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// fmuls f11,f3,f4
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f4.f64));
	// li r9,9
	ctx.r9.s64 = 9;
	// fmuls f5,f4,f4
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f4.f64));
	// fadds f8,f8,f12
	ctx.f8.f64 = double(float(ctx.f8.f64 + ctx.f12.f64));
	// stfs f8,108(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fadds f12,f20,f6
	ctx.f12.f64 = double(float(ctx.f20.f64 + ctx.f6.f64));
	// fmuls f6,f1,f31
	ctx.f6.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// fmuls f4,f7,f31
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// fmuls f2,f2,f31
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmuls f1,f11,f31
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// fnmsubs f3,f5,f31,f29
	ctx.f3.f64 = double(float(-(ctx.f5.f64 * ctx.f31.f64 - ctx.f29.f64)));
	// fadds f11,f4,f6
	ctx.f11.f64 = double(float(ctx.f4.f64 + ctx.f6.f64));
	// stfs f11,104(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fsubs f8,f6,f4
	ctx.f8.f64 = double(float(ctx.f6.f64 - ctx.f4.f64));
	// stfs f8,120(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// fsubs f7,f2,f1
	ctx.f7.f64 = double(float(ctx.f2.f64 - ctx.f1.f64));
	// stfs f7,116(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fsubs f10,f3,f10
	ctx.f10.f64 = double(float(ctx.f3.f64 - ctx.f10.f64));
	// stfs f10,112(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fadds f6,f1,f2
	ctx.f6.f64 = double(float(ctx.f1.f64 + ctx.f2.f64));
	// stfs f6,124(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// fsubs f5,f3,f9
	ctx.f5.f64 = double(float(ctx.f3.f64 - ctx.f9.f64));
	// stfs f5,128(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_830C6EC8:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c6ec8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C6EC8;
	// stfs f0,40(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r30.u32 + 40, temp.u32);
	// stfs f13,44(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r30.u32 + 44, temp.u32);
	// stfs f12,36(r30)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r30.u32 + 36, temp.u32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
loc_830C6EF4:
	// lwz r11,336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// addi r4,r11,8
	ctx.r4.s64 = ctx.r11.s64 + 8;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x830f2ee8
	ctx.lr = 0x830C6F0C;
	sub_830F2EE8(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r3,308(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 308);
	// bl 0x83073de8
	ctx.lr = 0x830C6F1C;
	sub_83073DE8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// addi r12,r1,-48
	ctx.r12.s64 = ctx.r1.s64 + -48;
	// bl 0x82cb6afc
	ctx.lr = 0x830C6F2C;
	__restfpr_14(ctx, base);
	// b 0x82cb1134
	__restgprlr_27(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C6F30"))) PPC_WEAK_FUNC(sub_830C6F30);
PPC_FUNC_IMPL(__imp__sub_830C6F30) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C6F34"))) PPC_WEAK_FUNC(sub_830C6F34);
PPC_FUNC_IMPL(__imp__sub_830C6F34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C6F38"))) PPC_WEAK_FUNC(sub_830C6F38);
PPC_FUNC_IMPL(__imp__sub_830C6F38) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C6F3C"))) PPC_WEAK_FUNC(sub_830C6F3C);
PPC_FUNC_IMPL(__imp__sub_830C6F3C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C6F40"))) PPC_WEAK_FUNC(sub_830C6F40);
PPC_FUNC_IMPL(__imp__sub_830C6F40) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C6F44"))) PPC_WEAK_FUNC(sub_830C6F44);
PPC_FUNC_IMPL(__imp__sub_830C6F44) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C6F48"))) PPC_WEAK_FUNC(sub_830C6F48);
PPC_FUNC_IMPL(__imp__sub_830C6F48) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,340(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C6F50"))) PPC_WEAK_FUNC(sub_830C6F50);
PPC_FUNC_IMPL(__imp__sub_830C6F50) {
	PPC_FUNC_PROLOGUE();
	// stw r4,340(r3)
	PPC_STORE_U32(ctx.r3.u32 + 340, ctx.r4.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C6F58"))) PPC_WEAK_FUNC(sub_830C6F58);
PPC_FUNC_IMPL(__imp__sub_830C6F58) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// rlwinm r11,r3,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// lfs f0,4(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r3,9
	ctx.r10.s64 = ctx.r3.s64 + 9;
	// add r11,r3,r11
	ctx.r11.u64 = ctx.r3.u64 + ctx.r11.u64;
	// lfs f12,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r5
	ctx.r11.u64 = ctx.r11.u64 + ctx.r5.u64;
	// lfsx f11,r9,r5
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r5.u32);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f8,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f6,f8,f13,f9
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f13.f64 + ctx.f9.f64));
	// fadds f5,f6,f11
	ctx.f5.f64 = double(float(ctx.f6.f64 + ctx.f11.f64));
	// fmadds f1,f7,f12,f5
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f12.f64 + ctx.f5.f64));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C6FA0"))) PPC_WEAK_FUNC(sub_830C6FA0);
PPC_FUNC_IMPL(__imp__sub_830C6FA0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10cc
	ctx.lr = 0x830C6FA8;
	__savegprlr_21(ctx, base);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r23,16(r6)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r6.u32 + 16);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// mr r24,r5
	ctx.r24.u64 = ctx.r5.u64;
	// li r22,2340
	ctx.r22.s64 = 2340;
	// li r30,0
	ctx.r30.s64 = 0;
	// addi r21,r6,88
	ctx.r21.s64 = ctx.r6.s64 + 88;
	// addi r29,r6,108
	ctx.r29.s64 = ctx.r6.s64 + 108;
loc_830C6FCC:
	// clrlwi r27,r22,30
	ctx.r27.u64 = ctx.r22.u32 & 0x3;
	// rlwinm r22,r22,30,2,31
	ctx.r22.u64 = __builtin_rotateleft64(ctx.r22.u32 | (ctx.r22.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r11,r27,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r27.u32 | (ctx.r27.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplwi cr6,r30,3
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 3, ctx.xer);
	// add r11,r27,r11
	ctx.r11.u64 = ctx.r27.u64 + ctx.r11.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r11,r24
	ctx.r31.u64 = ctx.r11.u64 + ctx.r24.u64;
	// lfsx f0,r11,r24
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r24.u32);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lfs f13,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// stfs f13,84(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// stfs f12,88(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// bge cr6,0x830c701c
	if (!ctx.cr6.lt) goto loc_830C701C;
	// fneg f0,f0
	ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fneg f13,f13
	ctx.f13.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// stfs f13,84(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fneg f12,f12
	ctx.f12.u64 = ctx.f12.u64 ^ 0x8000000000000000;
	// stfs f12,88(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
loc_830C701C:
	// lwz r11,36(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 36);
	// lwz r25,32(r26)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r26.u32 + 32);
	// addic. r11,r11,1
	ctx.xer.ca = ctx.r11.u32 > 4294967294;
	ctx.r11.s64 = ctx.r11.s64 + 1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,36(r26)
	PPC_STORE_U32(ctx.r26.u32 + 36, ctx.r11.u32);
	// bne 0x830c7054
	if (!ctx.cr0.eq) goto loc_830C7054;
	// cmplwi cr6,r25,0
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, 0, ctx.xer);
	// beq cr6,0x830c704c
	if (ctx.cr6.eq) goto loc_830C704C;
	// lwz r11,28(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 28);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82cb16f0
	ctx.lr = 0x830C704C;
	sub_82CB16F0(ctx, base);
loc_830C704C:
	// lwz r11,28(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 28);
	// stw r11,36(r26)
	PPC_STORE_U32(ctx.r26.u32 + 36, ctx.r11.u32);
loc_830C7054:
	// mr r8,r25
	ctx.r8.u64 = ctx.r25.u64;
	// lwz r7,36(r26)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r26.u32 + 36);
	// mr r6,r21
	ctx.r6.u64 = ctx.r21.u64;
	// mr r5,r23
	ctx.r5.u64 = ctx.r23.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82d56708
	ctx.lr = 0x830C7070;
	sub_82D56708(ctx, base);
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// addi r9,r27,9
	ctx.r9.s64 = ctx.r27.s64 + 9;
	// lfs f0,8(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lfs f10,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f8,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// add r7,r11,r10
	ctx.r7.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplwi cr6,r30,6
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 6, ctx.xer);
	// add r11,r11,r23
	ctx.r11.u64 = ctx.r11.u64 + ctx.r23.u64;
	// lfsx f13,r8,r24
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r24.u32);
	ctx.f13.f64 = double(temp.f32);
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// lfs f12,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f11,f12,f0,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f9,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// lfs f7,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f6,f9,f10,f11
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f10.f64 + ctx.f11.f64));
	// fmadds f5,f8,f7,f6
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f7.f64 + ctx.f6.f64));
	// stfs f5,0(r28)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r28.u32 + 0, temp.u32);
	// addi r28,r28,4
	ctx.r28.s64 = ctx.r28.s64 + 4;
	// blt cr6,0x830c6fcc
	if (ctx.cr6.lt) goto loc_830C6FCC;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x82cb111c
	__restgprlr_21(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C70D4"))) PPC_WEAK_FUNC(sub_830C70D4);
PPC_FUNC_IMPL(__imp__sub_830C70D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C70D8"))) PPC_WEAK_FUNC(sub_830C70D8);
PPC_FUNC_IMPL(__imp__sub_830C70D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6ac0
	ctx.lr = 0x830C70E8;
	__savefpr_18(ctx, base);
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c72f8
	if (ctx.cr6.eq) goto loc_830C72F8;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c72f8
	if (ctx.cr6.eq) goto loc_830C72F8;
	// lfs f12,252(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f12.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f11,112(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 112);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f12
	ctx.f10.f64 = ctx.f12.f64;
	// fmuls f9,f11,f12
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f7,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,124(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 124);
	ctx.f6.f64 = double(temp.f32);
	// fmr f30,f7
	ctx.f30.f64 = ctx.f7.f64;
	// lfs f1,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f3,f6,f7
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// fmuls f2,f6,f12
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// lfs f8,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f8.f64 = double(temp.f32);
	// fmr f29,f1
	ctx.f29.f64 = ctx.f1.f64;
	// lfs f4,116(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 116);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f11,f7
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// lfs f31,120(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 120);
	ctx.f31.f64 = double(temp.f32);
	// lfs f28,136(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	ctx.f28.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f27,128(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 128);
	ctx.f27.f64 = double(temp.f32);
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f13,6380(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6380);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r3,112
	ctx.r10.s64 = ctx.r3.s64 + 112;
	// lfs f26,132(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 132);
	ctx.f26.f64 = double(temp.f32);
	// fmsubs f24,f8,f8,f13
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f8.f64 - ctx.f13.f64));
	// fmadds f9,f4,f8,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f9.f64));
	// lfs f23,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f21,f28,f30
	ctx.f21.f64 = double(float(ctx.f28.f64 * ctx.f30.f64));
	// lfs f0,7676(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f3,f11,f8,f3
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f8.f64 + ctx.f3.f64));
	// lfs f13,6140(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 6140);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f2,f31,f8,f2
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f8.f64 + ctx.f2.f64));
	// lfs f22,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f19,f27,f29
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// lfs f20,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fmsubs f5,f6,f8,f5
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f8.f64 - ctx.f5.f64));
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// fmuls f18,f27,f30
	ctx.f18.f64 = double(float(ctx.f27.f64 * ctx.f30.f64));
	// addi r9,r3,12
	ctx.r9.s64 = ctx.r3.s64 + 12;
	// fmuls f25,f26,f10
	ctx.f25.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// fmadds f9,f6,f1,f9
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f1.f64 + ctx.f9.f64));
	// fmsubs f21,f27,f10,f21
	ctx.f21.f64 = double(float(ctx.f27.f64 * ctx.f10.f64 - ctx.f21.f64));
	// fmadds f3,f31,f1,f3
	ctx.f3.f64 = double(float(ctx.f31.f64 * ctx.f1.f64 + ctx.f3.f64));
	// fmadds f2,f4,f7,f2
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f7.f64 + ctx.f2.f64));
	// fmsubs f19,f26,f30,f19
	ctx.f19.f64 = double(float(ctx.f26.f64 * ctx.f30.f64 - ctx.f19.f64));
	// fnmsubs f5,f4,f1,f5
	ctx.f5.f64 = double(float(-(ctx.f4.f64 * ctx.f1.f64 - ctx.f5.f64)));
	// fmuls f6,f26,f24
	ctx.f6.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// fmadds f26,f26,f29,f18
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f18.f64));
	// fmsubs f25,f28,f29,f25
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f29.f64 - ctx.f25.f64));
	// fmuls f18,f28,f24
	ctx.f18.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// fnmsubs f9,f31,f7,f9
	ctx.f9.f64 = double(float(-(ctx.f31.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// fmuls f27,f27,f24
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// fnmsubs f4,f4,f12,f3
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f12.f64 - ctx.f3.f64)));
	// fnmsubs f3,f11,f1,f2
	ctx.f3.f64 = double(float(-(ctx.f11.f64 * ctx.f1.f64 - ctx.f2.f64)));
	// fmuls f2,f21,f8
	ctx.f2.f64 = double(float(ctx.f21.f64 * ctx.f8.f64));
	// fnmsubs f5,f31,f12,f5
	ctx.f5.f64 = double(float(-(ctx.f31.f64 * ctx.f12.f64 - ctx.f5.f64)));
	// fmuls f1,f19,f8
	ctx.f1.f64 = double(float(ctx.f19.f64 * ctx.f8.f64));
	// fmadds f12,f28,f10,f26
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f10.f64 + ctx.f26.f64));
	// fmuls f7,f25,f8
	ctx.f7.f64 = double(float(ctx.f25.f64 * ctx.f8.f64));
	// fmuls f11,f9,f9
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f9.f64));
	// fmuls f31,f3,f3
	ctx.f31.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// fadds f6,f6,f2
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f2.f64));
	// fmuls f28,f5,f3
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fadds f1,f18,f1
	ctx.f1.f64 = double(float(ctx.f18.f64 + ctx.f1.f64));
	// fmuls f2,f29,f12
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f12.f64));
	// fmuls f10,f12,f10
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// fadds f8,f27,f7
	ctx.f8.f64 = double(float(ctx.f27.f64 + ctx.f7.f64));
	// fmuls f7,f9,f4
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f12,f12,f30
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f30.f64));
	// fmuls f31,f31,f0
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fmuls f30,f28,f0
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// fadds f6,f6,f2
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f2.f64));
	// fadds f2,f1,f10
	ctx.f2.f64 = double(float(ctx.f1.f64 + ctx.f10.f64));
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fsubs f1,f13,f11
	ctx.f1.f64 = double(float(ctx.f13.f64 - ctx.f11.f64));
	// fsubs f10,f7,f30
	ctx.f10.f64 = double(float(ctx.f7.f64 - ctx.f30.f64));
	// stfs f10,-156(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -156, temp.u32);
	// fsubs f1,f1,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 - ctx.f31.f64));
	// stfs f1,-160(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -160, temp.u32);
	// fmuls f10,f3,f4
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f4.f64));
	// fmuls f1,f5,f9
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// fmuls f9,f3,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f9.f64));
	// addi r11,r1,-160
	ctx.r11.s64 = ctx.r1.s64 + -160;
	// fmuls f5,f5,f4
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f29,f4,f4
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f4.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fadds f4,f8,f12
	ctx.f4.f64 = double(float(ctx.f8.f64 + ctx.f12.f64));
	// fmuls f3,f2,f0
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmuls f2,f6,f0
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fadds f12,f30,f7
	ctx.f12.f64 = double(float(ctx.f30.f64 + ctx.f7.f64));
	// stfs f12,-148(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -148, temp.u32);
	// fmuls f8,f1,f0
	ctx.f8.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f6,f9,f0
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fnmsubs f7,f29,f0,f13
	ctx.f7.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fadds f13,f20,f3
	ctx.f13.f64 = double(float(ctx.f20.f64 + ctx.f3.f64));
	// fadds f0,f22,f2
	ctx.f0.f64 = double(float(ctx.f22.f64 + ctx.f2.f64));
	// fadds f3,f8,f10
	ctx.f3.f64 = double(float(ctx.f8.f64 + ctx.f10.f64));
	// stfs f3,-152(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + -152, temp.u32);
	// fsubs f1,f10,f8
	ctx.f1.f64 = double(float(ctx.f10.f64 - ctx.f8.f64));
	// stfs f1,-136(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -136, temp.u32);
	// fsubs f12,f6,f5
	ctx.f12.f64 = double(float(ctx.f6.f64 - ctx.f5.f64));
	// stfs f12,-140(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -140, temp.u32);
	// fsubs f2,f7,f31
	ctx.f2.f64 = double(float(ctx.f7.f64 - ctx.f31.f64));
	// stfs f2,-144(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + -144, temp.u32);
	// fadds f10,f5,f6
	ctx.f10.f64 = double(float(ctx.f5.f64 + ctx.f6.f64));
	// stfs f10,-132(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -132, temp.u32);
	// fsubs f9,f7,f11
	ctx.f9.f64 = double(float(ctx.f7.f64 - ctx.f11.f64));
	// stfs f9,-128(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -128, temp.u32);
	// fadds f12,f23,f4
	ctx.f12.f64 = double(float(ctx.f23.f64 + ctx.f4.f64));
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830C72CC:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c72cc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C72CC;
	// stfs f12,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f0,40(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f13,44(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
loc_830C72F8:
	// lfs f0,8(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r3,12
	ctx.r11.s64 = ctx.r3.s64 + 12;
	// lfs f12,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f10,f12,f0
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f11,28(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f11,f13
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fmuls f7,f9,f13
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// lfs f5,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f5.f64 = double(temp.f32);
	// lfs f6,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// lfs f4,24(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,36(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,12(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,32(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	ctx.f1.f64 = double(temp.f32);
	// lfs f12,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f10,f5,f13,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f13.f64 + ctx.f10.f64));
	// lfs f11,48(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f8,f4,f6,f8
	ctx.f8.f64 = double(float(ctx.f4.f64 * ctx.f6.f64 + ctx.f8.f64));
	// lfs f5,56(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f4,f3,f6,f7
	ctx.f4.f64 = double(float(ctx.f3.f64 * ctx.f6.f64 + ctx.f7.f64));
	// fmadds f3,f6,f2,f10
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f2.f64 + ctx.f10.f64));
	// fmadds f2,f1,f0,f8
	ctx.f2.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 + ctx.f8.f64));
	// fmadds f1,f12,f0,f4
	ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 + ctx.f4.f64));
	// fadds f0,f11,f3
	ctx.f0.f64 = double(float(ctx.f11.f64 + ctx.f3.f64));
	// stfs f0,0(r5)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// fadds f13,f9,f2
	ctx.f13.f64 = double(float(ctx.f9.f64 + ctx.f2.f64));
	// stfs f13,4(r5)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// fadds f12,f5,f1
	ctx.f12.f64 = double(float(ctx.f5.f64 + ctx.f1.f64));
	// stfs f12,8(r5)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6b0c
	ctx.lr = 0x830C737C;
	__restfpr_18(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C7388"))) PPC_WEAK_FUNC(sub_830C7388);
PPC_FUNC_IMPL(__imp__sub_830C7388) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// addi r10,r11,5504
	ctx.r10.s64 = ctx.r11.s64 + 5504;
	// stw r10,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r10.u32);
	// bl 0x83088de0
	ctx.lr = 0x830C73B4;
	sub_83088DE0(ctx, base);
	// clrlwi r9,r30,31
	ctx.r9.u64 = ctx.r30.u32 & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x830c73dc
	if (ctx.cr6.eq) goto loc_830C73DC;
	// lis r11,-31901
	ctx.r11.s64 = -2090663936;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lwz r3,-32308(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -32308);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,20(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830C73DC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830C73DC:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C73F8"))) PPC_WEAK_FUNC(sub_830C73F8);
PPC_FUNC_IMPL(__imp__sub_830C73F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32222
	ctx.r11.s64 = -2111700992;
	// lfs f0,-17488(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -17488);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f1,f1,f0
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C7408"))) PPC_WEAK_FUNC(sub_830C7408);
PPC_FUNC_IMPL(__imp__sub_830C7408) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f0,0(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f11,12(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f9,f11,f0
	ctx.f9.f64 = double(float(ctx.f11.f64 - ctx.f0.f64));
	// lfs f12,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// lfs f10,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f8.f64 = double(temp.f32);
	// fsubs f7,f10,f13
	ctx.f7.f64 = double(float(ctx.f10.f64 - ctx.f13.f64));
	// fsubs f6,f8,f12
	ctx.f6.f64 = double(float(ctx.f8.f64 - ctx.f12.f64));
	// fmuls f5,f9,f1
	ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f1.f64));
	// fmuls f4,f7,f1
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f1.f64));
	// fmuls f3,f6,f1
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f1.f64));
	// fadds f2,f0,f5
	ctx.f2.f64 = double(float(ctx.f0.f64 + ctx.f5.f64));
	// stfs f2,0(r4)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fadds f1,f13,f4
	ctx.f1.f64 = double(float(ctx.f13.f64 + ctx.f4.f64));
	// stfs f1,4(r4)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// fadds f0,f12,f3
	ctx.f0.f64 = double(float(ctx.f12.f64 + ctx.f3.f64));
	// stfs f0,8(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C7454"))) PPC_WEAK_FUNC(sub_830C7454);
PPC_FUNC_IMPL(__imp__sub_830C7454) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C7458"))) PPC_WEAK_FUNC(sub_830C7458);
PPC_FUNC_IMPL(__imp__sub_830C7458) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lfs f0,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lis r10,-32222
	ctx.r10.s64 = -2111700992;
	// stfs f0,0(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 0, temp.u32);
	// lfs f11,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f11
	ctx.f10.f64 = ctx.f11.f64;
	// fmuls f9,f10,f10
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f10.f64));
	// stfs f11,4(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r31.u32 + 4, temp.u32);
	// lfs f8,8(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// fmr f6,f8
	ctx.f6.f64 = ctx.f8.f64;
	// fmadds f7,f12,f12,f9
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f12.f64 + ctx.f9.f64));
	// lfs f13,6140(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6140);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,-17340(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -17340);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f31,f1,f0
	ctx.f31.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// stfs f8,8(r31)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r31.u32 + 8, temp.u32);
	// fmadds f5,f8,f8,f7
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f8.f64 + ctx.f7.f64));
	// fmr f1,f31
	ctx.f1.f64 = ctx.f31.f64;
	// fsqrts f4,f5
	ctx.f4.f64 = double(float(sqrt(ctx.f5.f64)));
	// fdivs f3,f13,f4
	ctx.f3.f64 = double(float(ctx.f13.f64 / ctx.f4.f64));
	// fmuls f2,f12,f3
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f3.f64));
	// stfs f2,0(r31)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r31.u32 + 0, temp.u32);
	// fmuls f0,f10,f3
	ctx.f0.f64 = double(float(ctx.f10.f64 * ctx.f3.f64));
	// stfs f0,4(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 4, temp.u32);
	// fmuls f13,f6,f3
	ctx.f13.f64 = double(float(ctx.f6.f64 * ctx.f3.f64));
	// stfs f13,8(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r31.u32 + 8, temp.u32);
	// bl 0x82cb4940
	ctx.lr = 0x830C74DC;
	sub_82CB4940(ctx, base);
	// frsp f12,f1
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f1.f64));
	// stfs f12,12(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + 12, temp.u32);
	// fmr f1,f31
	ctx.f1.f64 = ctx.f31.f64;
	// bl 0x82cb4860
	ctx.lr = 0x830C74EC;
	sub_82CB4860(ctx, base);
	// frsp f11,f1
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(ctx.f1.f64));
	// lfs f10,0(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,8(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f10,f11
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
	// stfs f7,0(r31)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r31.u32 + 0, temp.u32);
	// fmuls f6,f11,f9
	ctx.f6.f64 = double(float(ctx.f11.f64 * ctx.f9.f64));
	// stfs f6,4(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r31.u32 + 4, temp.u32);
	// fmuls f5,f8,f11
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f11.f64));
	// stfs f5,8(r31)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r31.u32 + 8, temp.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-24(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C752C"))) PPC_WEAK_FUNC(sub_830C752C);
PPC_FUNC_IMPL(__imp__sub_830C752C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C7530"))) PPC_WEAK_FUNC(sub_830C7530);
PPC_FUNC_IMPL(__imp__sub_830C7530) {
	PPC_FUNC_PROLOGUE();
	// mr r7,r5
	ctx.r7.u64 = ctx.r5.u64;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
	// addi r6,r4,24
	ctx.r6.s64 = ctx.r4.s64 + 24;
	// addi r5,r4,12
	ctx.r5.s64 = ctx.r4.s64 + 12;
	// b 0x83137248
	sub_83137248(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C7544"))) PPC_WEAK_FUNC(sub_830C7544);
PPC_FUNC_IMPL(__imp__sub_830C7544) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C7548"))) PPC_WEAK_FUNC(sub_830C7548);
PPC_FUNC_IMPL(__imp__sub_830C7548) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// addi r6,r3,24
	ctx.r6.s64 = ctx.r3.s64 + 24;
	// addi r5,r3,12
	ctx.r5.s64 = ctx.r3.s64 + 12;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x83137248
	ctx.lr = 0x830C7578;
	sub_83137248(ctx, base);
	// lfs f0,24(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f13,f0,f0
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f0.f64));
	// li r11,1
	ctx.r11.s64 = 1;
	// fcmpu cr6,f1,f13
	ctx.cr6.compare(ctx.f1.f64, ctx.f13.f64);
	// blt cr6,0x830c7590
	if (ctx.cr6.lt) goto loc_830C7590;
	// li r11,0
	ctx.r11.s64 = 0;
loc_830C7590:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C75A8"))) PPC_WEAK_FUNC(sub_830C75A8);
PPC_FUNC_IMPL(__imp__sub_830C75A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e8
	ctx.lr = 0x830C75B0;
	__savegprlr_28(ctx, base);
	// addi r12,r1,-40
	ctx.r12.s64 = ctx.r1.s64 + -40;
	// bl 0x82cb6abc
	ctx.lr = 0x830C75B8;
	__savefpr_17(ctx, base);
	// stwu r1,-400(r1)
	ea = -400 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// mr r29,r5
	ctx.r29.u64 = ctx.r5.u64;
	// bl 0x8308a278
	ctx.lr = 0x830C75CC;
	sub_8308A278(ctx, base);
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// addi r9,r11,6264
	ctx.r9.s64 = ctx.r11.s64 + 6264;
	// li r30,3
	ctx.r30.s64 = 3;
	// stw r9,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r9.u32);
	// lfs f0,92(r28)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 92);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,336(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 336, temp.u32);
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// lfs f31,6380(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6380);
	ctx.f31.f64 = double(temp.f32);
	// lfs f13,96(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f13,f31
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// stfs f12,340(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + 340, temp.u32);
	// lwz r8,100(r28)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r28.u32 + 100);
	// stw r8,344(r31)
	PPC_STORE_U32(ctx.r31.u32 + 344, ctx.r8.u32);
	// stw r30,288(r31)
	PPC_STORE_U32(ctx.r31.u32 + 288, ctx.r30.u32);
	// bl 0x8315fb70
	ctx.lr = 0x830C760C;
	sub_8315FB70(ctx, base);
	// lfs f11,96(r28)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 96);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,92(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 92);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f11,f31
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// stfs f10,184(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// stw r30,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r30.u32);
	// stfs f9,188(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 188, temp.u32);
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// lwz r3,1412(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1412);
	// bl 0x8315fb80
	ctx.lr = 0x830C7630;
	sub_8315FB80(ctx, base);
	// lis r7,-32222
	ctx.r7.s64 = -2111700992;
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// stw r3,152(r31)
	PPC_STORE_U32(ctx.r31.u32 + 152, ctx.r3.u32);
	// lis r5,-32256
	ctx.r5.s64 = -2113929216;
	// lis r4,-32248
	ctx.r4.s64 = -2113404928;
	// addi r30,r31,160
	ctx.r30.s64 = ctx.r31.s64 + 160;
	// lfd f0,-18352(r7)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r7.u32 + -18352);
	// fsqrts f8,f0
	ctx.f8.f64 = double(float(sqrt(ctx.f0.f64)));
	// lfs f0,6140(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 6140);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,6048(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 6048);
	ctx.f13.f64 = double(temp.f32);
	// lfd f30,6256(r4)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r4.u32 + 6256);
	// fdivs f7,f0,f8
	ctx.f7.f64 = double(float(ctx.f0.f64 / ctx.f8.f64));
	// stfs f7,168(r31)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r31.u32 + 168, temp.u32);
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// fmuls f6,f7,f13
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// stfs f6,160(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r31.u32 + 160, temp.u32);
	// stfs f6,164(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r31.u32 + 164, temp.u32);
	// bl 0x82cb4940
	ctx.lr = 0x830C7678;
	sub_82CB4940(ctx, base);
	// frsp f5,f1
	ctx.fpscr.disableFlushMode();
	ctx.f5.f64 = double(float(ctx.f1.f64));
	// stfs f5,172(r31)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r31.u32 + 172, temp.u32);
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// bl 0x82cb4860
	ctx.lr = 0x830C7688;
	sub_82CB4860(ctx, base);
	// frsp f4,f1
	ctx.fpscr.disableFlushMode();
	ctx.f4.f64 = double(float(ctx.f1.f64));
	// lfs f3,160(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,164(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 164);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,168(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 168);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f0,f3,f4
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f4.f64));
	// stfs f0,160(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 160, temp.u32);
	// fmuls f13,f2,f4
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f4.f64));
	// stfs f13,164(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r31.u32 + 164, temp.u32);
	// fmuls f12,f4,f1
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f1.f64));
	// stfs f12,168(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + 168, temp.u32);
	// lwz r3,152(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 152);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x830c7b48
	if (ctx.cr6.eq) goto loc_830C7B48;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x8315fb70
	ctx.lr = 0x830C76C4;
	sub_8315FB70(ctx, base);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// li r10,10
	ctx.r10.s64 = 10;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r10,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r10.u32);
	// beq cr6,0x830c79ac
	if (ctx.cr6.eq) goto loc_830C79AC;
	// lfs f0,216(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 216);
	ctx.f0.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f13,220(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 220);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f11,224(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 224);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// lfs f9,232(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 232);
	ctx.f9.f64 = double(temp.f32);
	// fmr f8,f11
	ctx.f8.f64 = ctx.f11.f64;
	// lfs f7,236(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 236);
	ctx.f7.f64 = double(temp.f32);
	// fneg f6,f9
	ctx.f6.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// lfs f5,240(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 240);
	ctx.f5.f64 = double(temp.f32);
	// fneg f4,f7
	ctx.f4.u64 = ctx.f7.u64 ^ 0x8000000000000000;
	// fneg f3,f5
	ctx.f3.u64 = ctx.f5.u64 ^ 0x8000000000000000;
	// lfs f2,228(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 228);
	ctx.f2.f64 = double(temp.f32);
	// fneg f1,f0
	ctx.f1.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lfs f9,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f9.f64 = double(temp.f32);
	// fneg f7,f13
	ctx.f7.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// lfs f5,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f5.f64 = double(temp.f32);
	// fneg f13,f11
	ctx.f13.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// lfs f11,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f11.f64 = double(temp.f32);
	// fmsubs f29,f2,f2,f31
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f2.f64 - ctx.f31.f64));
	// lfs f0,7676(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmr f30,f2
	ctx.f30.f64 = ctx.f2.f64;
	// addi r10,r11,216
	ctx.r10.s64 = ctx.r11.s64 + 216;
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// fmuls f28,f10,f6
	ctx.f28.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// fmuls f27,f8,f4
	ctx.f27.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// fmuls f26,f3,f12
	ctx.f26.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// fmuls f24,f8,f3
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f3.f64));
	// fmuls f23,f7,f5
	ctx.f23.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// fmuls f25,f9,f1
	ctx.f25.f64 = double(float(ctx.f9.f64 * ctx.f1.f64));
	// fmuls f21,f13,f5
	ctx.f21.f64 = double(float(ctx.f13.f64 * ctx.f5.f64));
	// fmuls f19,f29,f6
	ctx.f19.f64 = double(float(ctx.f29.f64 * ctx.f6.f64));
	// fmuls f20,f7,f11
	ctx.f20.f64 = double(float(ctx.f7.f64 * ctx.f11.f64));
	// fmsubs f22,f30,f30,f31
	ctx.f22.f64 = double(float(ctx.f30.f64 * ctx.f30.f64 - ctx.f31.f64));
	// fmsubs f28,f4,f12,f28
	ctx.f28.f64 = double(float(ctx.f4.f64 * ctx.f12.f64 - ctx.f28.f64));
	// fmsubs f27,f10,f3,f27
	ctx.f27.f64 = double(float(ctx.f10.f64 * ctx.f3.f64 - ctx.f27.f64));
	// fmsubs f26,f8,f6,f26
	ctx.f26.f64 = double(float(ctx.f8.f64 * ctx.f6.f64 - ctx.f26.f64));
	// fmadds f6,f12,f6,f24
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f6.f64 + ctx.f24.f64));
	// fmadds f24,f11,f1,f23
	ctx.f24.f64 = double(float(ctx.f11.f64 * ctx.f1.f64 + ctx.f23.f64));
	// fmsubs f25,f13,f11,f25
	ctx.f25.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 - ctx.f25.f64));
	// fmsubs f21,f7,f9,f21
	ctx.f21.f64 = double(float(ctx.f7.f64 * ctx.f9.f64 - ctx.f21.f64));
	// fmuls f23,f4,f29
	ctx.f23.f64 = double(float(ctx.f4.f64 * ctx.f29.f64));
	// fmuls f3,f3,f29
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f29.f64));
	// fmsubs f29,f5,f1,f20
	ctx.f29.f64 = double(float(ctx.f5.f64 * ctx.f1.f64 - ctx.f20.f64));
	// fmuls f28,f28,f2
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f2.f64));
	// fmuls f27,f27,f2
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f2.f64));
	// fmuls f2,f26,f2
	ctx.f2.f64 = double(float(ctx.f26.f64 * ctx.f2.f64));
	// fmadds f6,f10,f4,f6
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f4.f64 + ctx.f6.f64));
	// fmadds f4,f13,f9,f24
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f9.f64 + ctx.f24.f64));
	// fmuls f5,f5,f22
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f22.f64));
	// fmuls f26,f25,f30
	ctx.f26.f64 = double(float(ctx.f25.f64 * ctx.f30.f64));
	// fmuls f11,f11,f22
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f22.f64));
	// fmuls f25,f30,f21
	ctx.f25.f64 = double(float(ctx.f30.f64 * ctx.f21.f64));
	// fmuls f9,f9,f22
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f22.f64));
	// fsubs f3,f3,f28
	ctx.f3.f64 = double(float(ctx.f3.f64 - ctx.f28.f64));
	// fsubs f28,f19,f27
	ctx.f28.f64 = double(float(ctx.f19.f64 - ctx.f27.f64));
	// fsubs f2,f23,f2
	ctx.f2.f64 = double(float(ctx.f23.f64 - ctx.f2.f64));
	// fmuls f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// fmuls f10,f10,f6
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// fmuls f8,f8,f6
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f6.f64));
	// fmuls f6,f7,f4
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f4.f64));
	// fadds f5,f5,f26
	ctx.f5.f64 = double(float(ctx.f5.f64 + ctx.f26.f64));
	// fmuls f29,f29,f30
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f30.f64));
	// fmuls f27,f4,f1
	ctx.f27.f64 = double(float(ctx.f4.f64 * ctx.f1.f64));
	// fadds f11,f11,f25
	ctx.f11.f64 = double(float(ctx.f11.f64 + ctx.f25.f64));
	// fmuls f4,f13,f4
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f4.f64));
	// fadds f12,f28,f12
	ctx.f12.f64 = double(float(ctx.f28.f64 + ctx.f12.f64));
	// fadds f10,f2,f10
	ctx.f10.f64 = double(float(ctx.f2.f64 + ctx.f10.f64));
	// fadds f8,f3,f8
	ctx.f8.f64 = double(float(ctx.f3.f64 + ctx.f8.f64));
	// fadds f6,f5,f6
	ctx.f6.f64 = double(float(ctx.f5.f64 + ctx.f6.f64));
	// fadds f9,f9,f29
	ctx.f9.f64 = double(float(ctx.f9.f64 + ctx.f29.f64));
	// fadds f5,f11,f27
	ctx.f5.f64 = double(float(ctx.f11.f64 + ctx.f27.f64));
	// fmuls f3,f12,f0
	ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f2,f10,f0
	ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f10,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f12,f8,f0
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfs f8,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f11,f6,f0
	ctx.f11.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fadds f4,f9,f4
	ctx.f4.f64 = double(float(ctx.f9.f64 + ctx.f4.f64));
	// fmuls f9,f5,f0
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fmuls f6,f30,f10
	ctx.f6.f64 = double(float(ctx.f30.f64 * ctx.f10.f64));
	// lfs f29,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f28,f13,f8
	ctx.f28.f64 = double(float(ctx.f13.f64 * ctx.f8.f64));
	// lfs f27,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f5,f30,f8
	ctx.f5.f64 = double(float(ctx.f30.f64 * ctx.f8.f64));
	// lfs f24,20(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f26,f8,f1
	ctx.f26.f64 = double(float(ctx.f8.f64 * ctx.f1.f64));
	// lfs f25,24(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 24);
	ctx.f25.f64 = double(temp.f32);
	// fadds f9,f9,f3
	ctx.f9.f64 = double(float(ctx.f9.f64 + ctx.f3.f64));
	// lfs f23,16(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// lfs f22,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f22.f64 = double(temp.f32);
	// fadds f2,f11,f2
	ctx.f2.f64 = double(float(ctx.f11.f64 + ctx.f2.f64));
	// lfs f11,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f21,12(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f21.f64 = double(temp.f32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lfs f20,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f6,f29,f1,f6
	ctx.f6.f64 = double(float(ctx.f29.f64 * ctx.f1.f64 + ctx.f6.f64));
	// fmadds f3,f30,f29,f28
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f29.f64 + ctx.f28.f64));
	// fmadds f5,f27,f1,f5
	ctx.f5.f64 = double(float(ctx.f27.f64 * ctx.f1.f64 + ctx.f5.f64));
	// fmsubs f30,f30,f27,f26
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f27.f64 - ctx.f26.f64));
	// fadds f12,f4,f12
	ctx.f12.f64 = double(float(ctx.f4.f64 + ctx.f12.f64));
	// fmadds f6,f13,f27,f6
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f27.f64 + ctx.f6.f64));
	// fmadds f4,f7,f27,f3
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f27.f64 + ctx.f3.f64));
	// fmadds f5,f7,f10,f5
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f10.f64 + ctx.f5.f64));
	// fnmsubs f3,f7,f29,f30
	ctx.f3.f64 = double(float(-(ctx.f7.f64 * ctx.f29.f64 - ctx.f30.f64)));
	// fnmsubs f8,f7,f8,f6
	ctx.f8.f64 = double(float(-(ctx.f7.f64 * ctx.f8.f64 - ctx.f6.f64)));
	// fnmsubs f6,f10,f1,f4
	ctx.f6.f64 = double(float(-(ctx.f10.f64 * ctx.f1.f64 - ctx.f4.f64)));
	// fnmsubs f7,f13,f29,f5
	ctx.f7.f64 = double(float(-(ctx.f13.f64 * ctx.f29.f64 - ctx.f5.f64)));
	// fnmsubs f5,f13,f10,f3
	ctx.f5.f64 = double(float(-(ctx.f13.f64 * ctx.f10.f64 - ctx.f3.f64)));
	// fmuls f4,f8,f24
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f24.f64));
	// fmuls f3,f8,f25
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f25.f64));
	// fmuls f1,f25,f7
	ctx.f1.f64 = double(float(ctx.f25.f64 * ctx.f7.f64));
	// fmuls f13,f6,f23
	ctx.f13.f64 = double(float(ctx.f6.f64 * ctx.f23.f64));
	// fmsubs f10,f5,f5,f31
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f5.f64 - ctx.f31.f64));
	// fmuls f31,f6,f11
	ctx.f31.f64 = double(float(ctx.f6.f64 * ctx.f11.f64));
	// fmuls f30,f8,f22
	ctx.f30.f64 = double(float(ctx.f8.f64 * ctx.f22.f64));
	// fmuls f29,f8,f21
	ctx.f29.f64 = double(float(ctx.f8.f64 * ctx.f21.f64));
	// fmuls f28,f22,f7
	ctx.f28.f64 = double(float(ctx.f22.f64 * ctx.f7.f64));
	// fmsubs f4,f6,f25,f4
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f25.f64 - ctx.f4.f64));
	// fmadds f3,f6,f24,f3
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f24.f64 + ctx.f3.f64));
	// fmsubs f1,f8,f23,f1
	ctx.f1.f64 = double(float(ctx.f8.f64 * ctx.f23.f64 - ctx.f1.f64));
	// fmsubs f13,f24,f7,f13
	ctx.f13.f64 = double(float(ctx.f24.f64 * ctx.f7.f64 - ctx.f13.f64));
	// fmuls f27,f23,f10
	ctx.f27.f64 = double(float(ctx.f23.f64 * ctx.f10.f64));
	// fmadds f31,f5,f22,f31
	ctx.f31.f64 = double(float(ctx.f5.f64 * ctx.f22.f64 + ctx.f31.f64));
	// fmuls f26,f24,f10
	ctx.f26.f64 = double(float(ctx.f24.f64 * ctx.f10.f64));
	// fmuls f10,f25,f10
	ctx.f10.f64 = double(float(ctx.f25.f64 * ctx.f10.f64));
	// fmadds f29,f5,f11,f29
	ctx.f29.f64 = double(float(ctx.f5.f64 * ctx.f11.f64 + ctx.f29.f64));
	// fmadds f30,f5,f20,f30
	ctx.f30.f64 = double(float(ctx.f5.f64 * ctx.f20.f64 + ctx.f30.f64));
	// fmuls f4,f5,f4
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// fmadds f3,f23,f7,f3
	ctx.f3.f64 = double(float(ctx.f23.f64 * ctx.f7.f64 + ctx.f3.f64));
	// fmuls f1,f1,f5
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f5.f64));
	// fmuls f13,f13,f5
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f5.f64));
	// fmsubs f5,f5,f21,f28
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f21.f64 - ctx.f28.f64));
	// fmadds f31,f21,f7,f31
	ctx.f31.f64 = double(float(ctx.f21.f64 * ctx.f7.f64 + ctx.f31.f64));
	// fmadds f29,f20,f7,f29
	ctx.f29.f64 = double(float(ctx.f20.f64 * ctx.f7.f64 + ctx.f29.f64));
	// fmadds f30,f6,f21,f30
	ctx.f30.f64 = double(float(ctx.f6.f64 * ctx.f21.f64 + ctx.f30.f64));
	// fadds f4,f27,f4
	ctx.f4.f64 = double(float(ctx.f27.f64 + ctx.f4.f64));
	// fmuls f28,f3,f7
	ctx.f28.f64 = double(float(ctx.f3.f64 * ctx.f7.f64));
	// fmuls f27,f6,f3
	ctx.f27.f64 = double(float(ctx.f6.f64 * ctx.f3.f64));
	// fadds f13,f10,f13
	ctx.f13.f64 = double(float(ctx.f10.f64 + ctx.f13.f64));
	// fmuls f3,f8,f3
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f3.f64));
	// fadds f1,f26,f1
	ctx.f1.f64 = double(float(ctx.f26.f64 + ctx.f1.f64));
	// fnmsubs f10,f6,f20,f5
	ctx.f10.f64 = double(float(-(ctx.f6.f64 * ctx.f20.f64 - ctx.f5.f64)));
	// fnmsubs f5,f8,f20,f31
	ctx.f5.f64 = double(float(-(ctx.f8.f64 * ctx.f20.f64 - ctx.f31.f64)));
	// stfs f5,80(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fnmsubs f6,f6,f22,f29
	ctx.f6.f64 = double(float(-(ctx.f6.f64 * ctx.f22.f64 - ctx.f29.f64)));
	// stfs f6,88(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fnmsubs f7,f11,f7,f30
	ctx.f7.f64 = double(float(-(ctx.f11.f64 * ctx.f7.f64 - ctx.f30.f64)));
	// fadds f5,f4,f28
	ctx.f5.f64 = double(float(ctx.f4.f64 + ctx.f28.f64));
	// fadds f3,f13,f3
	ctx.f3.f64 = double(float(ctx.f13.f64 + ctx.f3.f64));
	// fadds f4,f1,f27
	ctx.f4.f64 = double(float(ctx.f1.f64 + ctx.f27.f64));
	// fnmsubs f1,f8,f11,f10
	ctx.f1.f64 = double(float(-(ctx.f8.f64 * ctx.f11.f64 - ctx.f10.f64)));
	// stfs f1,92(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fmuls f13,f5,f0
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fmuls f10,f3,f0
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmuls f11,f4,f0
	ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fadds f9,f13,f9
	ctx.f9.f64 = double(float(ctx.f13.f64 + ctx.f9.f64));
	// stfs f9,96(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f5,f10,f12
	ctx.f5.f64 = double(float(ctx.f10.f64 + ctx.f12.f64));
	// fadds f8,f11,f2
	ctx.f8.f64 = double(float(ctx.f11.f64 + ctx.f2.f64));
	// stfs f8,100(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// lwz r7,184(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 184);
	// stfs f7,84(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lwz r6,100(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// stfs f5,104(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// lwz r11,4(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r10,0(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r4,r1,140
	ctx.r4.s64 = ctx.r1.s64 + 140;
	// lwz r5,104(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stw r7,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r7.u32);
	// stw r11,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r11.u32);
	// lwz r9,12(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lwz r8,8(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
	// stw r9,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r9.u32);
	// stw r8,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r8.u32);
	// stw r6,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r6.u32);
	// stw r5,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r5.u32);
	// b 0x830c7b14
	goto loc_830C7B14;
loc_830C79AC:
	// lfs f13,120(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f12,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f12.f64 = double(temp.f32);
	// fmr f11,f13
	ctx.f11.f64 = ctx.f13.f64;
	// lfs f9,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f9.f64 = double(temp.f32);
	// fmr f10,f12
	ctx.f10.f64 = ctx.f12.f64;
	// fmr f7,f9
	ctx.f7.f64 = ctx.f9.f64;
	// lfs f5,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f5.f64 = double(temp.f32);
	// lfs f8,24(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 24);
	ctx.f8.f64 = double(temp.f32);
	// fmsubs f3,f5,f5,f31
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f5.f64 - ctx.f31.f64));
	// lfs f1,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// lfs f6,16(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f30,f1,f13
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
	// lfs f4,20(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f19,f1,f9
	ctx.f19.f64 = double(float(ctx.f1.f64 * ctx.f9.f64));
	// lfs f2,12(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f2.f64 = double(temp.f32);
	// addi r9,r1,140
	ctx.r9.s64 = ctx.r1.s64 + 140;
	// lfs f26,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f29,f2,f13
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// fmuls f31,f2,f9
	ctx.f31.f64 = double(float(ctx.f2.f64 * ctx.f9.f64));
	// lfs f28,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f27,f11,f8
	ctx.f27.f64 = double(float(ctx.f11.f64 * ctx.f8.f64));
	// lfs f0,7676(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f23,f11,f4
	ctx.f23.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// lfs f24,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f25,f10,f6
	ctx.f25.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// lfs f22,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f21,f8,f7
	ctx.f21.f64 = double(float(ctx.f8.f64 * ctx.f7.f64));
	// lfs f20,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f18,f8,f3
	ctx.f18.f64 = double(float(ctx.f8.f64 * ctx.f3.f64));
	// li r8,0
	ctx.r8.s64 = 0;
	// fmuls f17,f6,f3
	ctx.f17.f64 = double(float(ctx.f6.f64 * ctx.f3.f64));
	// addi r11,r31,112
	ctx.r11.s64 = ctx.r31.s64 + 112;
	// fmuls f3,f4,f3
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// stw r8,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r8.u32);
	// fmadds f30,f26,f5,f30
	ctx.f30.f64 = double(float(ctx.f26.f64 * ctx.f5.f64 + ctx.f30.f64));
	// fmadds f29,f28,f5,f29
	ctx.f29.f64 = double(float(ctx.f28.f64 * ctx.f5.f64 + ctx.f29.f64));
	// fmadds f31,f1,f5,f31
	ctx.f31.f64 = double(float(ctx.f1.f64 * ctx.f5.f64 + ctx.f31.f64));
	// fmadds f27,f10,f4,f27
	ctx.f27.f64 = double(float(ctx.f10.f64 * ctx.f4.f64 + ctx.f27.f64));
	// fmsubs f8,f10,f8,f23
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f8.f64 - ctx.f23.f64));
	// fmsubs f4,f4,f7,f25
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f7.f64 - ctx.f25.f64));
	// fmsubs f25,f11,f6,f21
	ctx.f25.f64 = double(float(ctx.f11.f64 * ctx.f6.f64 - ctx.f21.f64));
	// fmsubs f23,f2,f5,f19
	ctx.f23.f64 = double(float(ctx.f2.f64 * ctx.f5.f64 - ctx.f19.f64));
	// fmadds f2,f2,f12,f30
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f12.f64 + ctx.f30.f64));
	// fmadds f30,f26,f9,f29
	ctx.f30.f64 = double(float(ctx.f26.f64 * ctx.f9.f64 + ctx.f29.f64));
	// fmadds f31,f28,f12,f31
	ctx.f31.f64 = double(float(ctx.f28.f64 * ctx.f12.f64 + ctx.f31.f64));
	// fmadds f6,f6,f7,f27
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f7.f64 + ctx.f27.f64));
	// fmuls f8,f8,f5
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// fmuls f4,f4,f5
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f5.f64));
	// fmuls f5,f25,f5
	ctx.f5.f64 = double(float(ctx.f25.f64 * ctx.f5.f64));
	// fnmsubs f29,f26,f12,f23
	ctx.f29.f64 = double(float(-(ctx.f26.f64 * ctx.f12.f64 - ctx.f23.f64)));
	// fnmsubs f2,f28,f9,f2
	ctx.f2.f64 = double(float(-(ctx.f28.f64 * ctx.f9.f64 - ctx.f2.f64)));
	// stfs f2,84(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f31,f26,f13,f31
	ctx.f31.f64 = double(float(-(ctx.f26.f64 * ctx.f13.f64 - ctx.f31.f64)));
	// stfs f31,80(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmuls f11,f11,f6
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f6.f64));
	// fmuls f7,f6,f7
	ctx.f7.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// fadds f9,f18,f4
	ctx.f9.f64 = double(float(ctx.f18.f64 + ctx.f4.f64));
	// fadds f8,f17,f8
	ctx.f8.f64 = double(float(ctx.f17.f64 + ctx.f8.f64));
	// fmuls f6,f10,f6
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// fadds f5,f3,f5
	ctx.f5.f64 = double(float(ctx.f3.f64 + ctx.f5.f64));
	// fnmsubs f4,f1,f12,f30
	ctx.f4.f64 = double(float(-(ctx.f1.f64 * ctx.f12.f64 - ctx.f30.f64)));
	// stfs f4,88(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fnmsubs f3,f28,f13,f29
	ctx.f3.f64 = double(float(-(ctx.f28.f64 * ctx.f13.f64 - ctx.f29.f64)));
	// stfs f3,92(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fadds f2,f9,f11
	ctx.f2.f64 = double(float(ctx.f9.f64 + ctx.f11.f64));
	// fadds f1,f8,f7
	ctx.f1.f64 = double(float(ctx.f8.f64 + ctx.f7.f64));
	// fadds f13,f5,f6
	ctx.f13.f64 = double(float(ctx.f5.f64 + ctx.f6.f64));
	// fmuls f12,f2,f0
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmuls f11,f1,f0
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f10,f13,f0
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fadds f9,f24,f12
	ctx.f9.f64 = double(float(ctx.f24.f64 + ctx.f12.f64));
	// stfs f9,104(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fadds f8,f11,f22
	ctx.f8.f64 = double(float(ctx.f11.f64 + ctx.f22.f64));
	// stfs f8,96(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f7,f20,f10
	ctx.f7.f64 = double(float(ctx.f20.f64 + ctx.f10.f64));
	// stfs f7,100(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lwz r6,4(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// lwz r5,8(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lwz r4,12(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r10,104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// stw r6,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r6.u32);
	// stw r5,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r5.u32);
	// stw r4,12(r9)
	PPC_STORE_U32(ctx.r9.u32 + 12, ctx.r4.u32);
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r11.u32);
	// stw r10,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r10.u32);
loc_830C7B14:
	// lwz r3,96(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r11,152(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 152);
	// stw r3,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r3.u32);
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r11.u32);
	// lwz r3,1412(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1412);
	// bl 0x8315fb80
	ctx.lr = 0x830C7B30;
	sub_8315FB80(ctx, base);
	// stw r3,156(r31)
	PPC_STORE_U32(ctx.r31.u32 + 156, ctx.r3.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// addi r12,r1,-40
	ctx.r12.s64 = ctx.r1.s64 + -40;
	// bl 0x82cb6b08
	ctx.lr = 0x830C7B44;
	__restfpr_17(ctx, base);
	// b 0x82cb1138
	__restgprlr_28(ctx, base);
	return;
loc_830C7B48:
	// li r11,0
	ctx.r11.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r11,156(r31)
	PPC_STORE_U32(ctx.r31.u32 + 156, ctx.r11.u32);
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// addi r12,r1,-40
	ctx.r12.s64 = ctx.r1.s64 + -40;
	// bl 0x82cb6b08
	ctx.lr = 0x830C7B60;
	__restfpr_17(ctx, base);
	// b 0x82cb1138
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C7B64"))) PPC_WEAK_FUNC(sub_830C7B64);
PPC_FUNC_IMPL(__imp__sub_830C7B64) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C7B68"))) PPC_WEAK_FUNC(sub_830C7B68);
PPC_FUNC_IMPL(__imp__sub_830C7B68) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// addi r10,r11,6264
	ctx.r10.s64 = ctx.r11.s64 + 6264;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// b 0x83088de0
	sub_83088DE0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C7B78"))) PPC_WEAK_FUNC(sub_830C7B78);
PPC_FUNC_IMPL(__imp__sub_830C7B78) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// fmr f31,f1
	ctx.f31.f64 = ctx.f1.f64;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lfs f0,6380(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6380);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,6048(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6048);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f2,f0
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f31,336(r31)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r31.u32 + 336, temp.u32);
	// stfs f0,340(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 340, temp.u32);
	// fcmpu cr6,f31,f13
	ctx.cr6.compare(ctx.f31.f64, ctx.f13.f64);
	// bgt cr6,0x830c7bd4
	if (ctx.cr6.gt) goto loc_830C7BD4;
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r4,r11,6152
	ctx.r4.s64 = ctx.r11.s64 + 6152;
	// li r5,72
	ctx.r5.s64 = 72;
	// addi r7,r4,-56
	ctx.r7.s64 = ctx.r4.s64 + -56;
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x82d17988
	ctx.lr = 0x830C7BD4;
	sub_82D17988(ctx, base);
loc_830C7BD4:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x830886e0
	ctx.lr = 0x830C7BDC;
	sub_830886E0(ctx, base);
	// li r4,1
	ctx.r4.s64 = 1;
	// lfs f1,340(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 340);
	ctx.f1.f64 = double(temp.f32);
	// lwz r3,152(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 152);
	// bl 0x8315fde0
	ctx.lr = 0x830C7BEC;
	sub_8315FDE0(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,152(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 152);
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f31.f64;
	// bl 0x8315fde0
	ctx.lr = 0x830C7BFC;
	sub_8315FDE0(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C7C14"))) PPC_WEAK_FUNC(sub_830C7C14);
PPC_FUNC_IMPL(__imp__sub_830C7C14) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C7C18"))) PPC_WEAK_FUNC(sub_830C7C18);
PPC_FUNC_IMPL(__imp__sub_830C7C18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// fmr f31,f1
	ctx.f31.f64 = ctx.f1.f64;
	// stfs f31,336(r31)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r31.u32 + 336, temp.u32);
	// bl 0x830886e0
	ctx.lr = 0x830C7C3C;
	sub_830886E0(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,152(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 152);
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f31.f64;
	// bl 0x8315fde0
	ctx.lr = 0x830C7C4C;
	sub_8315FDE0(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C7C64"))) PPC_WEAK_FUNC(sub_830C7C64);
PPC_FUNC_IMPL(__imp__sub_830C7C64) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C7C68"))) PPC_WEAK_FUNC(sub_830C7C68);
PPC_FUNC_IMPL(__imp__sub_830C7C68) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lfs f0,6380(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6380);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f1,f0
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// stfs f0,340(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 340, temp.u32);
	// bl 0x830886e0
	ctx.lr = 0x830C7C90;
	sub_830886E0(ctx, base);
	// li r4,1
	ctx.r4.s64 = 1;
	// lfs f1,340(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 340);
	ctx.f1.f64 = double(temp.f32);
	// lwz r3,152(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 152);
	// bl 0x8315fde0
	ctx.lr = 0x830C7CA0;
	sub_8315FDE0(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C7CB4"))) PPC_WEAK_FUNC(sub_830C7CB4);
PPC_FUNC_IMPL(__imp__sub_830C7CB4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C7CB8"))) PPC_WEAK_FUNC(sub_830C7CB8);
PPC_FUNC_IMPL(__imp__sub_830C7CB8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f0,6048(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6048);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,8(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// stfs f0,4(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// stfs f0,0(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// lfs f0,340(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,336(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// stfs f12,12(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C7CE0"))) PPC_WEAK_FUNC(sub_830C7CE0);
PPC_FUNC_IMPL(__imp__sub_830C7CE0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f0,340(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	ctx.f0.f64 = double(temp.f32);
	// fneg f13,f0
	ctx.f13.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lfs f12,336(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	ctx.f12.f64 = double(temp.f32);
	// fneg f11,f12
	ctx.f11.u64 = ctx.f12.u64 ^ 0x8000000000000000;
	// stfs f11,0(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// stfs f11,8(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// fadds f10,f0,f12
	ctx.f10.f64 = double(float(ctx.f0.f64 + ctx.f12.f64));
	// fsubs f9,f13,f12
	ctx.f9.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// stfs f9,4(r4)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// stfs f12,12(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// stfs f10,16(r4)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// stfs f12,20(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C7D14"))) PPC_WEAK_FUNC(sub_830C7D14);
PPC_FUNC_IMPL(__imp__sub_830C7D14) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C7D18"))) PPC_WEAK_FUNC(sub_830C7D18);
PPC_FUNC_IMPL(__imp__sub_830C7D18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6ac0
	ctx.lr = 0x830C7D28;
	__savefpr_18(ctx, base);
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c7f38
	if (ctx.cr6.eq) goto loc_830C7F38;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c7f38
	if (ctx.cr6.eq) goto loc_830C7F38;
	// lfs f12,252(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f12.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f11,112(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 112);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f12
	ctx.f10.f64 = ctx.f12.f64;
	// fmuls f9,f11,f12
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f7,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,124(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 124);
	ctx.f6.f64 = double(temp.f32);
	// fmr f30,f7
	ctx.f30.f64 = ctx.f7.f64;
	// fmuls f3,f6,f7
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// lfs f1,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f2,f6,f12
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// lfs f8,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f5,f11,f7
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// lfs f4,116(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 116);
	ctx.f4.f64 = double(temp.f32);
	// fmr f29,f1
	ctx.f29.f64 = ctx.f1.f64;
	// lfs f31,120(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 120);
	ctx.f31.f64 = double(temp.f32);
	// lfs f28,136(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	ctx.f28.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f27,128(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 128);
	ctx.f27.f64 = double(temp.f32);
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f26,132(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 132);
	ctx.f26.f64 = double(temp.f32);
	// addi r10,r3,112
	ctx.r10.s64 = ctx.r3.s64 + 112;
	// fmuls f25,f26,f10
	ctx.f25.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// lfs f13,6380(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6380);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f9,f4,f8,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f9.f64));
	// lfs f23,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f21,f28,f30
	ctx.f21.f64 = double(float(ctx.f28.f64 * ctx.f30.f64));
	// lfs f0,7676(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f3,f11,f8,f3
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f8.f64 + ctx.f3.f64));
	// lfs f22,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f2,f31,f8,f2
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f8.f64 + ctx.f2.f64));
	// lfs f20,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fmsubs f5,f6,f8,f5
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f8.f64 - ctx.f5.f64));
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// fmuls f18,f27,f30
	ctx.f18.f64 = double(float(ctx.f27.f64 * ctx.f30.f64));
	// addi r9,r3,12
	ctx.r9.s64 = ctx.r3.s64 + 12;
	// fmuls f19,f27,f29
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// fmsubs f24,f8,f8,f13
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f8.f64 - ctx.f13.f64));
	// lfs f13,6140(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 6140);
	ctx.f13.f64 = double(temp.f32);
	// fmsubs f25,f28,f29,f25
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f29.f64 - ctx.f25.f64));
	// fmadds f9,f6,f1,f9
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f1.f64 + ctx.f9.f64));
	// fmsubs f21,f27,f10,f21
	ctx.f21.f64 = double(float(ctx.f27.f64 * ctx.f10.f64 - ctx.f21.f64));
	// fmadds f3,f31,f1,f3
	ctx.f3.f64 = double(float(ctx.f31.f64 * ctx.f1.f64 + ctx.f3.f64));
	// fmadds f2,f4,f7,f2
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f7.f64 + ctx.f2.f64));
	// fnmsubs f5,f4,f1,f5
	ctx.f5.f64 = double(float(-(ctx.f4.f64 * ctx.f1.f64 - ctx.f5.f64)));
	// fmadds f18,f28,f10,f18
	ctx.f18.f64 = double(float(ctx.f28.f64 * ctx.f10.f64 + ctx.f18.f64));
	// fmsubs f19,f26,f30,f19
	ctx.f19.f64 = double(float(ctx.f26.f64 * ctx.f30.f64 - ctx.f19.f64));
	// fmuls f6,f26,f24
	ctx.f6.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// fmuls f27,f27,f24
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// fmuls f28,f28,f24
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// fnmsubs f9,f31,f7,f9
	ctx.f9.f64 = double(float(-(ctx.f31.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// fmuls f7,f25,f8
	ctx.f7.f64 = double(float(ctx.f25.f64 * ctx.f8.f64));
	// fnmsubs f4,f4,f12,f3
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f12.f64 - ctx.f3.f64)));
	// fnmsubs f3,f11,f1,f2
	ctx.f3.f64 = double(float(-(ctx.f11.f64 * ctx.f1.f64 - ctx.f2.f64)));
	// fnmsubs f5,f31,f12,f5
	ctx.f5.f64 = double(float(-(ctx.f31.f64 * ctx.f12.f64 - ctx.f5.f64)));
	// fmuls f2,f21,f8
	ctx.f2.f64 = double(float(ctx.f21.f64 * ctx.f8.f64));
	// fmadds f12,f26,f29,f18
	ctx.f12.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f18.f64));
	// fmuls f1,f19,f8
	ctx.f1.f64 = double(float(ctx.f19.f64 * ctx.f8.f64));
	// fmuls f11,f9,f9
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f9.f64));
	// fadds f8,f27,f7
	ctx.f8.f64 = double(float(ctx.f27.f64 + ctx.f7.f64));
	// fmuls f7,f9,f4
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// fmuls f31,f3,f3
	ctx.f31.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// fmuls f27,f5,f3
	ctx.f27.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fadds f6,f6,f2
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f2.f64));
	// fmuls f2,f12,f29
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f29.f64));
	// fmuls f10,f10,f12
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
	// fadds f1,f28,f1
	ctx.f1.f64 = double(float(ctx.f28.f64 + ctx.f1.f64));
	// fmuls f12,f12,f30
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f30.f64));
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fmuls f31,f31,f0
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fmuls f30,f27,f0
	ctx.f30.f64 = double(float(ctx.f27.f64 * ctx.f0.f64));
	// fadds f6,f6,f2
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f2.f64));
	// fadds f2,f1,f10
	ctx.f2.f64 = double(float(ctx.f1.f64 + ctx.f10.f64));
	// fsubs f1,f13,f11
	ctx.f1.f64 = double(float(ctx.f13.f64 - ctx.f11.f64));
	// fsubs f10,f7,f30
	ctx.f10.f64 = double(float(ctx.f7.f64 - ctx.f30.f64));
	// stfs f10,-156(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -156, temp.u32);
	// fmuls f10,f3,f4
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f4.f64));
	// fsubs f1,f1,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 - ctx.f31.f64));
	// stfs f1,-160(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -160, temp.u32);
	// fmuls f1,f5,f9
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// fmuls f9,f3,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f9.f64));
	// addi r11,r1,-160
	ctx.r11.s64 = ctx.r1.s64 + -160;
	// fmuls f5,f5,f4
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f29,f4,f4
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f4.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fadds f4,f8,f12
	ctx.f4.f64 = double(float(ctx.f8.f64 + ctx.f12.f64));
	// fmuls f3,f2,f0
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmuls f2,f6,f0
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fadds f12,f30,f7
	ctx.f12.f64 = double(float(ctx.f30.f64 + ctx.f7.f64));
	// stfs f12,-148(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -148, temp.u32);
	// fmuls f8,f1,f0
	ctx.f8.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f6,f9,f0
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fnmsubs f7,f29,f0,f13
	ctx.f7.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fadds f13,f20,f3
	ctx.f13.f64 = double(float(ctx.f20.f64 + ctx.f3.f64));
	// fadds f0,f22,f2
	ctx.f0.f64 = double(float(ctx.f22.f64 + ctx.f2.f64));
	// fadds f3,f8,f10
	ctx.f3.f64 = double(float(ctx.f8.f64 + ctx.f10.f64));
	// stfs f3,-152(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + -152, temp.u32);
	// fsubs f1,f10,f8
	ctx.f1.f64 = double(float(ctx.f10.f64 - ctx.f8.f64));
	// stfs f1,-136(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -136, temp.u32);
	// fsubs f12,f6,f5
	ctx.f12.f64 = double(float(ctx.f6.f64 - ctx.f5.f64));
	// stfs f12,-140(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -140, temp.u32);
	// fsubs f2,f7,f31
	ctx.f2.f64 = double(float(ctx.f7.f64 - ctx.f31.f64));
	// stfs f2,-144(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + -144, temp.u32);
	// fadds f10,f5,f6
	ctx.f10.f64 = double(float(ctx.f5.f64 + ctx.f6.f64));
	// stfs f10,-132(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -132, temp.u32);
	// fsubs f9,f7,f11
	ctx.f9.f64 = double(float(ctx.f7.f64 - ctx.f11.f64));
	// stfs f9,-128(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -128, temp.u32);
	// fadds f12,f4,f23
	ctx.f12.f64 = double(float(ctx.f4.f64 + ctx.f23.f64));
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830C7F0C:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c7f0c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C7F0C;
	// stfs f12,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f0,40(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f13,44(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
loc_830C7F38:
	// lfs f0,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,0(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// lfs f13,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// lfs f12,56(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,8(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// lfs f11,336(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,340(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	ctx.f10.f64 = double(temp.f32);
	// fadds f9,f10,f11
	ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f11.f64));
	// stfs f9,12(r4)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6b0c
	ctx.lr = 0x830C7F68;
	__restfpr_18(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C7F74"))) PPC_WEAK_FUNC(sub_830C7F74);
PPC_FUNC_IMPL(__imp__sub_830C7F74) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C7F78"))) PPC_WEAK_FUNC(sub_830C7F78);
PPC_FUNC_IMPL(__imp__sub_830C7F78) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10ec
	ctx.lr = 0x830C7F80;
	__savegprlr_29(ctx, base);
	// addi r12,r1,-32
	ctx.r12.s64 = ctx.r1.s64 + -32;
	// bl 0x82cb6ab0
	ctx.lr = 0x830C7F88;
	__savefpr_14(ctx, base);
	// stwu r1,-544(r1)
	ea = -544 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// lwz r11,312(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 312);
	// rlwinm r10,r11,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x8;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x830c8830
	if (ctx.cr6.eq) goto loc_830C8830;
	// bl 0x83089320
	ctx.lr = 0x830C7FA8;
	sub_83089320(ctx, base);
	// lis r11,-31890
	ctx.r11.s64 = -2089943040;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// addi r9,r11,22552
	ctx.r9.s64 = ctx.r11.s64 + 22552;
	// lfs f31,6048(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6048);
	ctx.f31.f64 = double(temp.f32);
	// lfs f0,156(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 156);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// beq cr6,0x830c8830
	if (ctx.cr6.eq) goto loc_830C8830;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x83088cd8
	ctx.lr = 0x830C7FCC;
	sub_83088CD8(ctx, base);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f13,6140(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6140);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,7676(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// lfs f9,6380(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 6380);
	ctx.f9.f64 = double(temp.f32);
	// stfs f13,128(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// beq cr6,0x830c81e4
	if (ctx.cr6.eq) goto loc_830C81E4;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c81e4
	if (ctx.cr6.eq) goto loc_830C81E4;
	// lfs f12,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f12.f64 = double(temp.f32);
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// lfs f11,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f12
	ctx.f10.f64 = ctx.f12.f64;
	// fmuls f7,f11,f12
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f8,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f8.f64 = double(temp.f32);
	// lfs f6,252(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f6.f64 = double(temp.f32);
	// fmr f5,f8
	ctx.f5.f64 = ctx.f8.f64;
	// fmr f4,f6
	ctx.f4.f64 = ctx.f6.f64;
	// lfs f3,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f1,f11,f8
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f8.f64));
	// lfs f27,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f27.f64 = double(temp.f32);
	// lfs f30,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f29,f11,f6
	ctx.f29.f64 = double(float(ctx.f11.f64 * ctx.f6.f64));
	// fmuls f28,f3,f8
	ctx.f28.f64 = double(float(ctx.f3.f64 * ctx.f8.f64));
	// lfs f2,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f2.f64 = double(temp.f32);
	// fmsubs f25,f27,f27,f9
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f27.f64 - ctx.f9.f64));
	// lfs f26,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f26.f64 = double(temp.f32);
	// lfs f24,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f24.f64 = double(temp.f32);
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// lfs f23,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f22,f30,f10
	ctx.f22.f64 = double(float(ctx.f30.f64 * ctx.f10.f64));
	// lfs f21,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f7,f3,f6,f7
	ctx.f7.f64 = double(float(ctx.f3.f64 * ctx.f6.f64 + ctx.f7.f64));
	// lfs f20,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f19,f2,f5
	ctx.f19.f64 = double(float(ctx.f2.f64 * ctx.f5.f64));
	// lfs f18,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f17,f2,f4
	ctx.f17.f64 = double(float(ctx.f2.f64 * ctx.f4.f64));
	// addi r11,r31,12
	ctx.r11.s64 = ctx.r31.s64 + 12;
	// fmadds f1,f3,f27,f1
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f27.f64 + ctx.f1.f64));
	// fmadds f29,f26,f27,f29
	ctx.f29.f64 = double(float(ctx.f26.f64 * ctx.f27.f64 + ctx.f29.f64));
	// fmsubs f11,f11,f27,f28
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f27.f64 - ctx.f28.f64));
	// fmuls f28,f23,f4
	ctx.f28.f64 = double(float(ctx.f23.f64 * ctx.f4.f64));
	// fmuls f16,f23,f25
	ctx.f16.f64 = double(float(ctx.f23.f64 * ctx.f25.f64));
	// fmuls f15,f2,f25
	ctx.f15.f64 = double(float(ctx.f2.f64 * ctx.f25.f64));
	// fmsubs f22,f23,f5,f22
	ctx.f22.f64 = double(float(ctx.f23.f64 * ctx.f5.f64 - ctx.f22.f64));
	// fmadds f7,f24,f27,f7
	ctx.f7.f64 = double(float(ctx.f24.f64 * ctx.f27.f64 + ctx.f7.f64));
	// fmsubs f19,f30,f4,f19
	ctx.f19.f64 = double(float(ctx.f30.f64 * ctx.f4.f64 - ctx.f19.f64));
	// fmadds f23,f23,f10,f17
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f10.f64 + ctx.f17.f64));
	// fmadds f1,f26,f12,f1
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f12.f64 + ctx.f1.f64));
	// fmadds f29,f24,f8,f29
	ctx.f29.f64 = double(float(ctx.f24.f64 * ctx.f8.f64 + ctx.f29.f64));
	// fnmsubs f11,f24,f12,f11
	ctx.f11.f64 = double(float(-(ctx.f24.f64 * ctx.f12.f64 - ctx.f11.f64)));
	// fmsubs f2,f2,f10,f28
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f10.f64 - ctx.f28.f64));
	// fmuls f28,f30,f25
	ctx.f28.f64 = double(float(ctx.f30.f64 * ctx.f25.f64));
	// fmuls f25,f22,f27
	ctx.f25.f64 = double(float(ctx.f22.f64 * ctx.f27.f64));
	// fnmsubs f8,f26,f8,f7
	ctx.f8.f64 = double(float(-(ctx.f26.f64 * ctx.f8.f64 - ctx.f7.f64)));
	// fmuls f7,f19,f27
	ctx.f7.f64 = double(float(ctx.f19.f64 * ctx.f27.f64));
	// fmadds f30,f30,f5,f23
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f5.f64 + ctx.f23.f64));
	// fnmsubs f1,f24,f6,f1
	ctx.f1.f64 = double(float(-(ctx.f24.f64 * ctx.f6.f64 - ctx.f1.f64)));
	// fnmsubs f3,f3,f12,f29
	ctx.f3.f64 = double(float(-(ctx.f3.f64 * ctx.f12.f64 - ctx.f29.f64)));
	// fnmsubs f6,f26,f6,f11
	ctx.f6.f64 = double(float(-(ctx.f26.f64 * ctx.f6.f64 - ctx.f11.f64)));
	// fmuls f2,f2,f27
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f27.f64));
	// fadds f12,f15,f25
	ctx.f12.f64 = double(float(ctx.f15.f64 + ctx.f25.f64));
	// fmuls f11,f8,f8
	ctx.f11.f64 = double(float(ctx.f8.f64 * ctx.f8.f64));
	// fadds f7,f16,f7
	ctx.f7.f64 = double(float(ctx.f16.f64 + ctx.f7.f64));
	// fmuls f10,f10,f30
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f30.f64));
	// fmuls f4,f4,f30
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f30.f64));
	// fmuls f5,f30,f5
	ctx.f5.f64 = double(float(ctx.f30.f64 * ctx.f5.f64));
	// fmuls f29,f8,f1
	ctx.f29.f64 = double(float(ctx.f8.f64 * ctx.f1.f64));
	// fmuls f27,f3,f3
	ctx.f27.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// fmuls f26,f6,f3
	ctx.f26.f64 = double(float(ctx.f6.f64 * ctx.f3.f64));
	// fadds f2,f28,f2
	ctx.f2.f64 = double(float(ctx.f28.f64 + ctx.f2.f64));
	// fmuls f28,f6,f8
	ctx.f28.f64 = double(float(ctx.f6.f64 * ctx.f8.f64));
	// fmuls f30,f11,f0
	ctx.f30.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fadds f11,f7,f10
	ctx.f11.f64 = double(float(ctx.f7.f64 + ctx.f10.f64));
	// fadds f10,f12,f4
	ctx.f10.f64 = double(float(ctx.f12.f64 + ctx.f4.f64));
	// fmuls f7,f29,f0
	ctx.f7.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// fmuls f4,f27,f0
	ctx.f4.f64 = double(float(ctx.f27.f64 * ctx.f0.f64));
	// fmuls f29,f26,f0
	ctx.f29.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// fadds f2,f2,f5
	ctx.f2.f64 = double(float(ctx.f2.f64 + ctx.f5.f64));
	// fsubs f12,f13,f30
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f30.f64));
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fsubs f5,f7,f29
	ctx.f5.f64 = double(float(ctx.f7.f64 - ctx.f29.f64));
	// stfs f5,148(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmuls f2,f2,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmuls f5,f3,f1
	ctx.f5.f64 = double(float(ctx.f3.f64 * ctx.f1.f64));
	// fsubs f12,f12,f4
	ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f4.f64));
	// stfs f12,144(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// fadds f12,f21,f11
	ctx.f12.f64 = double(float(ctx.f21.f64 + ctx.f11.f64));
	// fadds f11,f20,f10
	ctx.f11.f64 = double(float(ctx.f20.f64 + ctx.f10.f64));
	// fmuls f3,f3,f8
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f8.f64));
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// fmuls f27,f1,f1
	ctx.f27.f64 = double(float(ctx.f1.f64 * ctx.f1.f64));
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// fmuls f1,f6,f1
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f1.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fadds f10,f29,f7
	ctx.f10.f64 = double(float(ctx.f29.f64 + ctx.f7.f64));
	// stfs f10,156(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 156, temp.u32);
	// fmuls f8,f5,f0
	ctx.f8.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fmuls f7,f28,f0
	ctx.f7.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// fadds f10,f2,f18
	ctx.f10.f64 = double(float(ctx.f2.f64 + ctx.f18.f64));
	// fmuls f5,f3,f0
	ctx.f5.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fnmsubs f6,f27,f0,f13
	ctx.f6.f64 = double(float(-(ctx.f27.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f3,f1,f0
	ctx.f3.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fadds f2,f7,f8
	ctx.f2.f64 = double(float(ctx.f7.f64 + ctx.f8.f64));
	// stfs f2,152(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fsubs f8,f8,f7
	ctx.f8.f64 = double(float(ctx.f8.f64 - ctx.f7.f64));
	// stfs f8,168(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 168, temp.u32);
	// fsubs f1,f6,f4
	ctx.f1.f64 = double(float(ctx.f6.f64 - ctx.f4.f64));
	// stfs f1,160(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// fsubs f7,f5,f3
	ctx.f7.f64 = double(float(ctx.f5.f64 - ctx.f3.f64));
	// stfs f7,164(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// fadds f5,f3,f5
	ctx.f5.f64 = double(float(ctx.f3.f64 + ctx.f5.f64));
	// stfs f5,172(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// fsubs f4,f6,f30
	ctx.f4.f64 = double(float(ctx.f6.f64 - ctx.f30.f64));
	// stfs f4,176(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830C81B8:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bdnz 0x830c81b8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C81B8;
	// stfs f10,36(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 36, temp.u32);
	// stfs f12,40(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r11.u32 + 40, temp.u32);
	// stfs f11,44(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r11.u32 + 44, temp.u32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
loc_830C81E4:
	// lfs f12,340(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 340);
	ctx.f12.f64 = double(temp.f32);
	// addi r10,r31,12
	ctx.r10.s64 = ctx.r31.s64 + 12;
	// lfs f11,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f11.f64 = double(temp.f32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f10,28(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f8,f11,f12
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f7,40(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f6,f10,f12
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
	// fmuls f5,f7,f12
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// lfs f4,48(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,52(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 52);
	ctx.f3.f64 = double(temp.f32);
	// fmr f1,f4
	ctx.f1.f64 = ctx.f4.f64;
	// lfs f2,56(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 56);
	ctx.f2.f64 = double(temp.f32);
	// fmr f12,f3
	ctx.f12.f64 = ctx.f3.f64;
	// fmr f11,f2
	ctx.f11.f64 = ctx.f2.f64;
	// stfs f31,124(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// stfs f31,120(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// addi r9,r10,36
	ctx.r9.s64 = ctx.r10.s64 + 36;
	// stfs f31,116(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f31,84(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// stfs f31,88(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fneg f10,f8
	ctx.f10.u64 = ctx.f8.u64 ^ 0x8000000000000000;
	// stfs f31,92(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fneg f7,f6
	ctx.f7.u64 = ctx.f6.u64 ^ 0x8000000000000000;
	// stfs f31,100(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fneg f26,f5
	ctx.f26.u64 = ctx.f5.u64 ^ 0x8000000000000000;
	// stfs f31,104(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// stfs f31,108(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fadds f30,f1,f8
	ctx.f30.f64 = double(float(ctx.f1.f64 + ctx.f8.f64));
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fadds f31,f12,f6
	ctx.f31.f64 = double(float(ctx.f12.f64 + ctx.f6.f64));
	// stfs f13,96(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f29,f11,f5
	ctx.f29.f64 = double(float(ctx.f11.f64 + ctx.f5.f64));
	// stfs f13,112(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fadds f28,f4,f10
	ctx.f28.f64 = double(float(ctx.f4.f64 + ctx.f10.f64));
	// stfs f28,144(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// fadds f27,f3,f7
	ctx.f27.f64 = double(float(ctx.f3.f64 + ctx.f7.f64));
	// stfs f27,148(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fadds f26,f2,f26
	ctx.f26.f64 = double(float(ctx.f2.f64 + ctx.f26.f64));
	// stfs f26,152(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// beq cr6,0x830c8488
	if (ctx.cr6.eq) goto loc_830C8488;
	// lwz r9,280(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r9,r8
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, ctx.xer);
	// beq cr6,0x830c8488
	if (ctx.cr6.eq) goto loc_830C8488;
	// lfs f12,252(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f12.f64 = double(temp.f32);
	// addi r9,r31,112
	ctx.r9.s64 = ctx.r31.s64 + 112;
	// lfs f11,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f12
	ctx.f10.f64 = ctx.f12.f64;
	// lfs f8,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f11,f12
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f6,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f6.f64 = double(temp.f32);
	// fmr f5,f8
	ctx.f5.f64 = ctx.f8.f64;
	// fmr f4,f6
	ctx.f4.f64 = ctx.f6.f64;
	// lfs f2,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f2.f64 = double(temp.f32);
	// lfs f3,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f25,f2,f12
	ctx.f25.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// lfs f28,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f27,f2,f8
	ctx.f27.f64 = double(float(ctx.f2.f64 * ctx.f8.f64));
	// lfs f24,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f1,f11,f8
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f8.f64));
	// lfs f26,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f26.f64 = double(temp.f32);
	// fmsubs f9,f3,f3,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f3.f64 - ctx.f9.f64));
	// lfs f23,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f23.f64 = double(temp.f32);
	// addi r9,r11,244
	ctx.r9.s64 = ctx.r11.s64 + 244;
	// lfs f22,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f21,f24,f10
	ctx.f21.f64 = double(float(ctx.f24.f64 * ctx.f10.f64));
	// stfd f31,184(r1)
	PPC_STORE_U64(ctx.r1.u32 + 184, ctx.f31.u64);
	// fmadds f7,f28,f3,f7
	ctx.f7.f64 = double(float(ctx.f28.f64 * ctx.f3.f64 + ctx.f7.f64));
	// lfs f13,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f19,f26,f5
	ctx.f19.f64 = double(float(ctx.f26.f64 * ctx.f5.f64));
	// lfs f20,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f17,f24,f4
	ctx.f17.f64 = double(float(ctx.f24.f64 * ctx.f4.f64));
	// lfs f18,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f25,f23,f3,f25
	ctx.f25.f64 = double(float(ctx.f23.f64 * ctx.f3.f64 + ctx.f25.f64));
	// lfs f16,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f15,f22,f4
	ctx.f15.f64 = double(float(ctx.f22.f64 * ctx.f4.f64));
	// addi r11,r1,320
	ctx.r11.s64 = ctx.r1.s64 + 320;
	// fmsubs f1,f2,f3,f1
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f3.f64 - ctx.f1.f64));
	// fmadds f27,f11,f3,f27
	ctx.f27.f64 = double(float(ctx.f11.f64 * ctx.f3.f64 + ctx.f27.f64));
	// fmuls f31,f26,f9
	ctx.f31.f64 = double(float(ctx.f26.f64 * ctx.f9.f64));
	// fmuls f14,f24,f9
	ctx.f14.f64 = double(float(ctx.f24.f64 * ctx.f9.f64));
	// fmsubs f21,f26,f4,f21
	ctx.f21.f64 = double(float(ctx.f26.f64 * ctx.f4.f64 - ctx.f21.f64));
	// fmadds f7,f2,f6,f7
	ctx.f7.f64 = double(float(ctx.f2.f64 * ctx.f6.f64 + ctx.f7.f64));
	// fmsubs f2,f22,f10,f19
	ctx.f2.f64 = double(float(ctx.f22.f64 * ctx.f10.f64 - ctx.f19.f64));
	// fmadds f19,f22,f5,f17
	ctx.f19.f64 = double(float(ctx.f22.f64 * ctx.f5.f64 + ctx.f17.f64));
	// fmadds f25,f28,f8,f25
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f8.f64 + ctx.f25.f64));
	// fmsubs f24,f24,f5,f15
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f5.f64 - ctx.f15.f64));
	// fmuls f9,f22,f9
	ctx.f9.f64 = double(float(ctx.f22.f64 * ctx.f9.f64));
	// fnmsubs f1,f28,f6,f1
	ctx.f1.f64 = double(float(-(ctx.f28.f64 * ctx.f6.f64 - ctx.f1.f64)));
	// fmadds f27,f23,f6,f27
	ctx.f27.f64 = double(float(ctx.f23.f64 * ctx.f6.f64 + ctx.f27.f64));
	// fmuls f22,f21,f3
	ctx.f22.f64 = double(float(ctx.f21.f64 * ctx.f3.f64));
	// fnmsubs f8,f23,f8,f7
	ctx.f8.f64 = double(float(-(ctx.f23.f64 * ctx.f8.f64 - ctx.f7.f64)));
	// fmuls f7,f2,f3
	ctx.f7.f64 = double(float(ctx.f2.f64 * ctx.f3.f64));
	// fmadds f2,f26,f10,f19
	ctx.f2.f64 = double(float(ctx.f26.f64 * ctx.f10.f64 + ctx.f19.f64));
	// fnmsubs f11,f11,f6,f25
	ctx.f11.f64 = double(float(-(ctx.f11.f64 * ctx.f6.f64 - ctx.f25.f64)));
	// fmuls f6,f24,f3
	ctx.f6.f64 = double(float(ctx.f24.f64 * ctx.f3.f64));
	// fnmsubs f1,f23,f12,f1
	ctx.f1.f64 = double(float(-(ctx.f23.f64 * ctx.f12.f64 - ctx.f1.f64)));
	// fnmsubs f28,f28,f12,f27
	ctx.f28.f64 = double(float(-(ctx.f28.f64 * ctx.f12.f64 - ctx.f27.f64)));
	// fadds f3,f9,f22
	ctx.f3.f64 = double(float(ctx.f9.f64 + ctx.f22.f64));
	// fmuls f12,f8,f8
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f8.f64));
	// fadds f9,f14,f7
	ctx.f9.f64 = double(float(ctx.f14.f64 + ctx.f7.f64));
	// fmuls f7,f4,f2
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f2.f64));
	// fmuls f4,f10,f2
	ctx.f4.f64 = double(float(ctx.f10.f64 * ctx.f2.f64));
	// fmuls f5,f2,f5
	ctx.f5.f64 = double(float(ctx.f2.f64 * ctx.f5.f64));
	// fadds f6,f31,f6
	ctx.f6.f64 = double(float(ctx.f31.f64 + ctx.f6.f64));
	// fmuls f26,f1,f11
	ctx.f26.f64 = double(float(ctx.f1.f64 * ctx.f11.f64));
	// fmuls f10,f8,f28
	ctx.f10.f64 = double(float(ctx.f8.f64 * ctx.f28.f64));
	// fmuls f27,f11,f11
	ctx.f27.f64 = double(float(ctx.f11.f64 * ctx.f11.f64));
	// fmuls f2,f12,f0
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fadds f12,f9,f7
	ctx.f12.f64 = double(float(ctx.f9.f64 + ctx.f7.f64));
	// fadds f5,f3,f5
	ctx.f5.f64 = double(float(ctx.f3.f64 + ctx.f5.f64));
	// fadds f6,f6,f4
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f4.f64));
	// fmuls f7,f26,f0
	ctx.f7.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f9,f27,f0
	ctx.f9.f64 = double(float(ctx.f27.f64 * ctx.f0.f64));
	// fsubs f4,f13,f2
	ctx.f4.f64 = double(float(ctx.f13.f64 - ctx.f2.f64));
	// fmuls f3,f12,f0
	ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fmuls f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fsubs f12,f10,f7
	ctx.f12.f64 = double(float(ctx.f10.f64 - ctx.f7.f64));
	// stfs f12,324(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// fsubs f4,f4,f9
	ctx.f4.f64 = double(float(ctx.f4.f64 - ctx.f9.f64));
	// stfs f4,320(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 320, temp.u32);
	// fadds f12,f20,f3
	ctx.f12.f64 = double(float(ctx.f20.f64 + ctx.f3.f64));
	// fmuls f3,f11,f28
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f28.f64));
	// fmuls f4,f1,f8
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f8.f64));
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// fmuls f27,f28,f28
	ctx.f27.f64 = double(float(ctx.f28.f64 * ctx.f28.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fmuls f8,f11,f8
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f8.f64));
	// fmuls f1,f1,f28
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f28.f64));
	// fadds f11,f7,f10
	ctx.f11.f64 = double(float(ctx.f7.f64 + ctx.f10.f64));
	// stfs f11,332(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 332, temp.u32);
	// fadds f11,f18,f6
	ctx.f11.f64 = double(float(ctx.f18.f64 + ctx.f6.f64));
	// fadds f10,f16,f5
	ctx.f10.f64 = double(float(ctx.f16.f64 + ctx.f5.f64));
	// fmuls f7,f3,f0
	ctx.f7.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmuls f6,f4,f0
	ctx.f6.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fnmsubs f5,f27,f0,f13
	ctx.f5.f64 = double(float(-(ctx.f27.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f4,f8,f0
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmuls f3,f1,f0
	ctx.f3.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fadds f1,f6,f7
	ctx.f1.f64 = double(float(ctx.f6.f64 + ctx.f7.f64));
	// stfs f1,328(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 328, temp.u32);
	// fsubs f0,f5,f9
	ctx.f0.f64 = double(float(ctx.f5.f64 - ctx.f9.f64));
	// stfs f0,336(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// fsubs f13,f7,f6
	ctx.f13.f64 = double(float(ctx.f7.f64 - ctx.f6.f64));
	// stfs f13,344(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 344, temp.u32);
	// fsubs f9,f4,f3
	ctx.f9.f64 = double(float(ctx.f4.f64 - ctx.f3.f64));
	// stfs f9,340(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// fadds f8,f3,f4
	ctx.f8.f64 = double(float(ctx.f3.f64 + ctx.f4.f64));
	// stfs f8,348(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 348, temp.u32);
	// fsubs f7,f5,f2
	ctx.f7.f64 = double(float(ctx.f5.f64 - ctx.f2.f64));
	// stfs f7,352(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 352, temp.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// lfd f31,184(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + 184);
loc_830C8450:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bdnz 0x830c8450
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C8450;
	// stfs f12,40(r10)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 40, temp.u32);
	// stfs f11,44(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 44, temp.u32);
	// stfs f10,36(r10)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r10.u32 + 36, temp.u32);
	// lfs f26,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f26.f64 = double(temp.f32);
	// lfs f27,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f27.f64 = double(temp.f32);
	// lfs f28,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f28.f64 = double(temp.f32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lwz r9,280(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
loc_830C8488:
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// li r9,9
	ctx.r9.s64 = 9;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_830C8494:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bdnz 0x830c8494
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C8494;
	// lfs f11,336(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	ctx.f11.f64 = double(temp.f32);
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// addi r5,r1,240
	ctx.r5.s64 = ctx.r1.s64 + 240;
	// lfs f13,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f10,f0,f11
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfs f12,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f9,f13,f11
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// fmuls f7,f11,f0
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// addi r4,r1,256
	ctx.r4.s64 = ctx.r1.s64 + 256;
	// fmuls f6,f13,f11
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// fmuls f8,f12,f11
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// lfs f25,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f5,f12,f11
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// lfs f24,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f24.f64 = double(temp.f32);
	// fmr f22,f0
	ctx.f22.f64 = ctx.f0.f64;
	// lfs f23,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f23.f64 = double(temp.f32);
	// fmr f20,f13
	ctx.f20.f64 = ctx.f13.f64;
	// lfs f21,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f21.f64 = double(temp.f32);
	// lfs f19,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f19.f64 = double(temp.f32);
	// fmr f18,f12
	ctx.f18.f64 = ctx.f12.f64;
	// lfs f17,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f17.f64 = double(temp.f32);
	// fadds f4,f30,f10
	ctx.f4.f64 = double(float(ctx.f30.f64 + ctx.f10.f64));
	// stfs f4,240(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 240, temp.u32);
	// fadds f3,f31,f9
	ctx.f3.f64 = double(float(ctx.f31.f64 + ctx.f9.f64));
	// stfs f3,244(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 244, temp.u32);
	// fadds f1,f7,f28
	ctx.f1.f64 = double(float(ctx.f7.f64 + ctx.f28.f64));
	// stfs f1,256(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 256, temp.u32);
	// fadds f0,f6,f27
	ctx.f0.f64 = double(float(ctx.f6.f64 + ctx.f27.f64));
	// stfs f0,260(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 260, temp.u32);
	// fadds f2,f29,f8
	ctx.f2.f64 = double(float(ctx.f29.f64 + ctx.f8.f64));
	// stfs f2,248(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 248, temp.u32);
	// fadds f13,f5,f26
	ctx.f13.f64 = double(float(ctx.f5.f64 + ctx.f26.f64));
	// stfs f13,264(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 264, temp.u32);
	// bl 0x831bfb50
	ctx.lr = 0x830C8538;
	sub_831BFB50(ctx, base);
	// lfs f12,336(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f22,f12
	ctx.f11.f64 = double(float(ctx.f22.f64 * ctx.f12.f64));
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// fmuls f10,f20,f12
	ctx.f10.f64 = double(float(ctx.f20.f64 * ctx.f12.f64));
	// addi r5,r1,288
	ctx.r5.s64 = ctx.r1.s64 + 288;
	// fmuls f9,f18,f12
	ctx.f9.f64 = double(float(ctx.f18.f64 * ctx.f12.f64));
	// addi r4,r1,272
	ctx.r4.s64 = ctx.r1.s64 + 272;
	// fmuls f8,f12,f22
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f22.f64));
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// fmuls f7,f20,f12
	ctx.f7.f64 = double(float(ctx.f20.f64 * ctx.f12.f64));
	// fmuls f6,f18,f12
	ctx.f6.f64 = double(float(ctx.f18.f64 * ctx.f12.f64));
	// fsubs f5,f30,f11
	ctx.f5.f64 = double(float(ctx.f30.f64 - ctx.f11.f64));
	// stfs f5,288(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 288, temp.u32);
	// fsubs f4,f31,f10
	ctx.f4.f64 = double(float(ctx.f31.f64 - ctx.f10.f64));
	// stfs f4,292(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 292, temp.u32);
	// fsubs f3,f29,f9
	ctx.f3.f64 = double(float(ctx.f29.f64 - ctx.f9.f64));
	// stfs f3,296(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 296, temp.u32);
	// fsubs f2,f28,f8
	ctx.f2.f64 = double(float(ctx.f28.f64 - ctx.f8.f64));
	// stfs f2,272(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// fsubs f1,f27,f7
	ctx.f1.f64 = double(float(ctx.f27.f64 - ctx.f7.f64));
	// stfs f1,276(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 276, temp.u32);
	// fsubs f0,f26,f6
	ctx.f0.f64 = double(float(ctx.f26.f64 - ctx.f6.f64));
	// stfs f0,280(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 280, temp.u32);
	// bl 0x831bfb50
	ctx.lr = 0x830C8598;
	sub_831BFB50(ctx, base);
	// lfs f13,336(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f13,f21
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f21.f64));
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// fmuls f11,f19,f13
	ctx.f11.f64 = double(float(ctx.f19.f64 * ctx.f13.f64));
	// addi r5,r1,192
	ctx.r5.s64 = ctx.r1.s64 + 192;
	// fmuls f10,f17,f13
	ctx.f10.f64 = double(float(ctx.f17.f64 * ctx.f13.f64));
	// addi r4,r1,304
	ctx.r4.s64 = ctx.r1.s64 + 304;
	// fmuls f9,f13,f21
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f21.f64));
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// fmuls f8,f19,f13
	ctx.f8.f64 = double(float(ctx.f19.f64 * ctx.f13.f64));
	// fmuls f7,f17,f13
	ctx.f7.f64 = double(float(ctx.f17.f64 * ctx.f13.f64));
	// fadds f6,f12,f30
	ctx.f6.f64 = double(float(ctx.f12.f64 + ctx.f30.f64));
	// stfs f6,192(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 192, temp.u32);
	// fadds f5,f11,f31
	ctx.f5.f64 = double(float(ctx.f11.f64 + ctx.f31.f64));
	// stfs f5,196(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 196, temp.u32);
	// fadds f4,f10,f29
	ctx.f4.f64 = double(float(ctx.f10.f64 + ctx.f29.f64));
	// stfs f4,200(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 200, temp.u32);
	// fadds f3,f9,f28
	ctx.f3.f64 = double(float(ctx.f9.f64 + ctx.f28.f64));
	// stfs f3,304(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 304, temp.u32);
	// fadds f2,f8,f27
	ctx.f2.f64 = double(float(ctx.f8.f64 + ctx.f27.f64));
	// stfs f2,308(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// fadds f1,f7,f26
	ctx.f1.f64 = double(float(ctx.f7.f64 + ctx.f26.f64));
	// stfs f1,312(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 312, temp.u32);
	// bl 0x831bfb50
	ctx.lr = 0x830C85F8;
	sub_831BFB50(ctx, base);
	// lfs f0,336(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f13,f0,f21
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f21.f64));
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// fmuls f12,f19,f0
	ctx.f12.f64 = double(float(ctx.f19.f64 * ctx.f0.f64));
	// addi r5,r1,208
	ctx.r5.s64 = ctx.r1.s64 + 208;
	// fmuls f11,f17,f0
	ctx.f11.f64 = double(float(ctx.f17.f64 * ctx.f0.f64));
	// addi r4,r1,224
	ctx.r4.s64 = ctx.r1.s64 + 224;
	// fmuls f10,f0,f21
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f21.f64));
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// fmuls f9,f19,f0
	ctx.f9.f64 = double(float(ctx.f19.f64 * ctx.f0.f64));
	// fmuls f8,f17,f0
	ctx.f8.f64 = double(float(ctx.f17.f64 * ctx.f0.f64));
	// fsubs f7,f30,f13
	ctx.f7.f64 = double(float(ctx.f30.f64 - ctx.f13.f64));
	// stfs f7,208(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 208, temp.u32);
	// fsubs f6,f31,f12
	ctx.f6.f64 = double(float(ctx.f31.f64 - ctx.f12.f64));
	// stfs f6,212(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 212, temp.u32);
	// fsubs f5,f29,f11
	ctx.f5.f64 = double(float(ctx.f29.f64 - ctx.f11.f64));
	// stfs f5,216(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 216, temp.u32);
	// fsubs f4,f28,f10
	ctx.f4.f64 = double(float(ctx.f28.f64 - ctx.f10.f64));
	// stfs f4,224(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 224, temp.u32);
	// fsubs f3,f27,f9
	ctx.f3.f64 = double(float(ctx.f27.f64 - ctx.f9.f64));
	// stfs f3,228(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 228, temp.u32);
	// fsubs f2,f26,f8
	ctx.f2.f64 = double(float(ctx.f26.f64 - ctx.f8.f64));
	// stfs f2,232(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 232, temp.u32);
	// bl 0x831bfb50
	ctx.lr = 0x830C8658;
	sub_831BFB50(ctx, base);
	// stfs f28,116(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fneg f0,f22
	ctx.f0.u64 = ctx.f22.u64 ^ 0x8000000000000000;
	// stfs f21,88(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fneg f13,f20
	ctx.f13.u64 = ctx.f20.u64 ^ 0x8000000000000000;
	// stfs f19,100(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fneg f16,f25
	ctx.f16.u64 = ctx.f25.u64 ^ 0x8000000000000000;
	// stfs f17,112(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fneg f15,f24
	ctx.f15.u64 = ctx.f24.u64 ^ 0x8000000000000000;
	// stfs f27,120(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// fneg f14,f23
	ctx.f14.u64 = ctx.f23.u64 ^ 0x8000000000000000;
	// stfs f26,124(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// fneg f28,f18
	ctx.f28.u64 = ctx.f18.u64 ^ 0x8000000000000000;
	// stfs f16,80(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f15,92(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// li r8,1
	ctx.r8.s64 = 1;
	// stfs f14,104(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// stfs f0,84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stfs f13,96(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// li r4,20
	ctx.r4.s64 = 20;
	// stfs f28,108(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// lfs f1,336(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	ctx.f1.f64 = double(temp.f32);
	// stfs f0,128(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// stfs f13,184(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// bl 0x831c03d8
	ctx.lr = 0x830C86C4;
	sub_831C03D8(ctx, base);
	// lfs f0,128(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// li r8,1
	ctx.r8.s64 = 1;
	// lfs f0,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f0.f64 = double(temp.f32);
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// stfs f25,80(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stfs f24,92(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// li r4,20
	ctx.r4.s64 = 20;
	// stfs f23,104(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stfs f21,88(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// stfs f19,100(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stfs f17,112(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// stfs f30,116(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f31,120(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// stfs f29,124(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// stfs f28,108(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// lfs f1,336(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x831c03d8
	ctx.lr = 0x830C8718;
	sub_831C03D8(ctx, base);
	// lfs f1,336(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	ctx.f1.f64 = double(temp.f32);
	// stfs f21,84(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// li r8,1
	ctx.r8.s64 = 1;
	// stfs f19,96(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// stfs f17,108(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// stfs f22,88(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lfs f28,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f28.f64 = double(temp.f32);
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// lfs f27,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f27.f64 = double(temp.f32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lfs f26,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f26.f64 = double(temp.f32);
	// li r4,20
	ctx.r4.s64 = 20;
	// stfs f20,100(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stfs f18,112(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// stfs f28,116(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f27,120(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// stfs f26,124(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// stfs f16,80(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f15,92(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// stfs f14,104(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// bl 0x831c03d8
	ctx.lr = 0x830C8770;
	sub_831C03D8(ctx, base);
	// stfs f25,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f24,92(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// li r8,1
	ctx.r8.s64 = 1;
	// stfs f23,104(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// stfs f21,84(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stfs f19,96(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// li r4,20
	ctx.r4.s64 = 20;
	// stfs f17,108(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stfs f22,88(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// stfs f20,100(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stfs f18,112(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// stfs f30,116(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f31,120(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// stfs f29,124(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// lfs f1,336(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x831c03d8
	ctx.lr = 0x830C87BC;
	sub_831C03D8(ctx, base);
	// stfs f21,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f19,92(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// stfs f17,104(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// stfs f22,84(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stfs f20,96(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// li r4,20
	ctx.r4.s64 = 20;
	// stfs f18,108(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stfs f25,88(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// stfs f24,100(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stfs f23,112(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// stfs f28,116(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f27,120(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// stfs f26,124(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// lfs f1,336(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x831c03d8
	ctx.lr = 0x830C8808;
	sub_831C03D8(ctx, base);
	// stfs f30,116(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f31,120(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// stfs f29,124(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lfs f1,336(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	ctx.f1.f64 = double(temp.f32);
	// li r4,20
	ctx.r4.s64 = 20;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x831c03d8
	ctx.lr = 0x830C8830;
	sub_831C03D8(ctx, base);
loc_830C8830:
	// addi r1,r1,544
	ctx.r1.s64 = ctx.r1.s64 + 544;
	// addi r12,r1,-32
	ctx.r12.s64 = ctx.r1.s64 + -32;
	// bl 0x82cb6afc
	ctx.lr = 0x830C883C;
	__restfpr_14(ctx, base);
	// b 0x82cb113c
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C8840"))) PPC_WEAK_FUNC(sub_830C8840);
PPC_FUNC_IMPL(__imp__sub_830C8840) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e0
	ctx.lr = 0x830C8848;
	__savegprlr_26(ctx, base);
	// addi r12,r1,-56
	ctx.r12.s64 = ctx.r1.s64 + -56;
	// bl 0x82cb6abc
	ctx.lr = 0x830C8850;
	__savefpr_17(ctx, base);
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// fmr f29,f1
	ctx.fpscr.disableFlushMode();
	ctx.f29.f64 = ctx.f1.f64;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// mr r26,r6
	ctx.r26.u64 = ctx.r6.u64;
	// mr r30,r7
	ctx.r30.u64 = ctx.r7.u64;
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lfs f30,6140(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6140);
	ctx.f30.f64 = double(temp.f32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c8a78
	if (ctx.cr6.eq) goto loc_830C8A78;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c8a78
	if (ctx.cr6.eq) goto loc_830C8A78;
	// lfs f12,252(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f12.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f11,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f12
	ctx.f10.f64 = ctx.f12.f64;
	// fmuls f9,f11,f12
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f7,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f6.f64 = double(temp.f32);
	// fmr f31,f7
	ctx.f31.f64 = ctx.f7.f64;
	// fmuls f3,f6,f7
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// lfs f8,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f8.f64 = double(temp.f32);
	// lfs f4,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f2,f6,f12
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// fmuls f5,f11,f7
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// lfs f1,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f1.f64 = double(temp.f32);
	// fmr f26,f1
	ctx.f26.f64 = ctx.f1.f64;
	// lfs f28,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f27.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f13,6380(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6380);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// lfs f24,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f24.f64 = double(temp.f32);
	// fmsubs f13,f8,f8,f13
	ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f8.f64 - ctx.f13.f64));
	// fmuls f23,f27,f10
	ctx.f23.f64 = double(float(ctx.f27.f64 * ctx.f10.f64));
	// lfs f25,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f9,f4,f8,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f9.f64));
	// lfs f22,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f19,f27,f31
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f31.f64));
	// lfs f0,7676(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f3,f11,f8,f3
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f8.f64 + ctx.f3.f64));
	// lfs f21,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f2,f28,f8,f2
	ctx.f2.f64 = double(float(ctx.f28.f64 * ctx.f8.f64 + ctx.f2.f64));
	// lfs f20,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fmsubs f5,f6,f8,f5
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f8.f64 - ctx.f5.f64));
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// fmuls f18,f24,f26
	ctx.f18.f64 = double(float(ctx.f24.f64 * ctx.f26.f64));
	// addi r9,r31,12
	ctx.r9.s64 = ctx.r31.s64 + 12;
	// fmuls f17,f25,f10
	ctx.f17.f64 = double(float(ctx.f25.f64 * ctx.f10.f64));
	// fmadds f23,f25,f26,f23
	ctx.f23.f64 = double(float(ctx.f25.f64 * ctx.f26.f64 + ctx.f23.f64));
	// fmadds f9,f6,f1,f9
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f1.f64 + ctx.f9.f64));
	// fmsubs f19,f24,f10,f19
	ctx.f19.f64 = double(float(ctx.f24.f64 * ctx.f10.f64 - ctx.f19.f64));
	// fmadds f3,f28,f1,f3
	ctx.f3.f64 = double(float(ctx.f28.f64 * ctx.f1.f64 + ctx.f3.f64));
	// fmadds f2,f4,f7,f2
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f7.f64 + ctx.f2.f64));
	// fnmsubs f5,f4,f1,f5
	ctx.f5.f64 = double(float(-(ctx.f4.f64 * ctx.f1.f64 - ctx.f5.f64)));
	// fmuls f6,f25,f13
	ctx.f6.f64 = double(float(ctx.f25.f64 * ctx.f13.f64));
	// fmsubs f25,f25,f31,f18
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f31.f64 - ctx.f18.f64));
	// fmsubs f18,f27,f26,f17
	ctx.f18.f64 = double(float(ctx.f27.f64 * ctx.f26.f64 - ctx.f17.f64));
	// fmuls f17,f24,f13
	ctx.f17.f64 = double(float(ctx.f24.f64 * ctx.f13.f64));
	// fmadds f24,f24,f31,f23
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f31.f64 + ctx.f23.f64));
	// fnmsubs f9,f28,f7,f9
	ctx.f9.f64 = double(float(-(ctx.f28.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// fmuls f7,f27,f13
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f13.f64));
	// fnmsubs f4,f4,f12,f3
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f12.f64 - ctx.f3.f64)));
	// fnmsubs f3,f11,f1,f2
	ctx.f3.f64 = double(float(-(ctx.f11.f64 * ctx.f1.f64 - ctx.f2.f64)));
	// fnmsubs f5,f28,f12,f5
	ctx.f5.f64 = double(float(-(ctx.f28.f64 * ctx.f12.f64 - ctx.f5.f64)));
	// fmuls f2,f19,f8
	ctx.f2.f64 = double(float(ctx.f19.f64 * ctx.f8.f64));
	// fmuls f1,f25,f8
	ctx.f1.f64 = double(float(ctx.f25.f64 * ctx.f8.f64));
	// fmuls f13,f18,f8
	ctx.f13.f64 = double(float(ctx.f18.f64 * ctx.f8.f64));
	// fmuls f8,f24,f31
	ctx.f8.f64 = double(float(ctx.f24.f64 * ctx.f31.f64));
	// fmuls f11,f9,f9
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f9.f64));
	// fmuls f12,f26,f24
	ctx.f12.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// fmuls f31,f9,f4
	ctx.f31.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// fmuls f28,f3,f3
	ctx.f28.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// fmuls f27,f5,f3
	ctx.f27.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fadds f6,f6,f2
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f2.f64));
	// fadds f2,f7,f1
	ctx.f2.f64 = double(float(ctx.f7.f64 + ctx.f1.f64));
	// fadds f1,f17,f13
	ctx.f1.f64 = double(float(ctx.f17.f64 + ctx.f13.f64));
	// fmuls f10,f10,f24
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f24.f64));
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f7,f31,f0
	ctx.f7.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fmuls f31,f28,f0
	ctx.f31.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// fmuls f13,f27,f0
	ctx.f13.f64 = double(float(ctx.f27.f64 * ctx.f0.f64));
	// fadds f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 + ctx.f12.f64));
	// fadds f8,f1,f8
	ctx.f8.f64 = double(float(ctx.f1.f64 + ctx.f8.f64));
	// fadds f10,f2,f10
	ctx.f10.f64 = double(float(ctx.f2.f64 + ctx.f10.f64));
	// fsubs f6,f30,f11
	ctx.f6.f64 = double(float(ctx.f30.f64 - ctx.f11.f64));
	// fsubs f2,f7,f13
	ctx.f2.f64 = double(float(ctx.f7.f64 - ctx.f13.f64));
	// stfs f2,100(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fmuls f1,f12,f0
	ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f2,f5,f9
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// fsubs f12,f6,f31
	ctx.f12.f64 = double(float(ctx.f6.f64 - ctx.f31.f64));
	// stfs f12,96(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fmuls f6,f3,f4
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f4.f64));
	// fmuls f12,f4,f4
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f4.f64));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// fmuls f9,f3,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f9.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f5,f5,f4
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fmuls f4,f8,f0
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmuls f3,f10,f0
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f10,f6,f0
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fadds f13,f13,f7
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f7.f64));
	// stfs f13,108(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fmuls f8,f2,f0
	ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fadds f13,f21,f1
	ctx.f13.f64 = double(float(ctx.f21.f64 + ctx.f1.f64));
	// fnmsubs f7,f12,f0,f30
	ctx.f7.f64 = double(float(-(ctx.f12.f64 * ctx.f0.f64 - ctx.f30.f64)));
	// fmuls f6,f9,f0
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fadds f12,f4,f22
	ctx.f12.f64 = double(float(ctx.f4.f64 + ctx.f22.f64));
	// fadds f0,f20,f3
	ctx.f0.f64 = double(float(ctx.f20.f64 + ctx.f3.f64));
	// fadds f4,f8,f10
	ctx.f4.f64 = double(float(ctx.f8.f64 + ctx.f10.f64));
	// stfs f4,104(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fsubs f2,f10,f8
	ctx.f2.f64 = double(float(ctx.f10.f64 - ctx.f8.f64));
	// stfs f2,120(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// fsubs f3,f7,f31
	ctx.f3.f64 = double(float(ctx.f7.f64 - ctx.f31.f64));
	// stfs f3,112(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fsubs f9,f7,f11
	ctx.f9.f64 = double(float(ctx.f7.f64 - ctx.f11.f64));
	// stfs f9,128(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// fsubs f1,f6,f5
	ctx.f1.f64 = double(float(ctx.f6.f64 - ctx.f5.f64));
	// stfs f1,116(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fadds f10,f5,f6
	ctx.f10.f64 = double(float(ctx.f5.f64 + ctx.f6.f64));
	// stfs f10,124(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830C8A4C:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c8a4c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C8A4C;
	// stfs f12,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f13,40(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f0,44(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
loc_830C8A78:
	// lfs f0,340(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 340);
	ctx.f0.f64 = double(temp.f32);
	// addi r28,r27,12
	ctx.r28.s64 = ctx.r27.s64 + 12;
	// lfs f13,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r31,12
	ctx.r11.s64 = ctx.r31.s64 + 12;
	// lfs f12,28(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f10,40(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f8,f10,f0
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f6,52(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 52);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,56(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 56);
	ctx.f5.f64 = double(temp.f32);
	// fmr f3,f6
	ctx.f3.f64 = ctx.f6.f64;
	// fmr f2,f5
	ctx.f2.f64 = ctx.f5.f64;
	// lfs f7,48(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	ctx.f7.f64 = double(temp.f32);
	// fmr f4,f7
	ctx.f4.f64 = ctx.f7.f64;
	// lfs f1,336(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	ctx.f1.f64 = double(temp.f32);
	// stfs f1,120(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// fneg f0,f11
	ctx.f0.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// addi r11,r11,36
	ctx.r11.s64 = ctx.r11.s64 + 36;
	// fneg f13,f9
	ctx.f13.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// fneg f12,f8
	ctx.f12.u64 = ctx.f8.u64 ^ 0x8000000000000000;
	// fadds f10,f3,f9
	ctx.f10.f64 = double(float(ctx.f3.f64 + ctx.f9.f64));
	// stfs f10,112(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fadds f9,f2,f8
	ctx.f9.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f9,116(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fadds f11,f4,f11
	ctx.f11.f64 = double(float(ctx.f4.f64 + ctx.f11.f64));
	// stfs f11,108(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fadds f8,f7,f0
	ctx.f8.f64 = double(float(ctx.f7.f64 + ctx.f0.f64));
	// stfs f8,96(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f7,f6,f13
	ctx.f7.f64 = double(float(ctx.f6.f64 + ctx.f13.f64));
	// stfs f7,100(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f6,f5,f12
	ctx.f6.f64 = double(float(ctx.f5.f64 + ctx.f12.f64));
	// stfs f6,104(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// bl 0x82d5cea8
	ctx.lr = 0x830C8B10;
	sub_82D5CEA8(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x830c8b2c
	if (!ctx.cr6.eq) goto loc_830C8B2C;
loc_830C8B18:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// addi r12,r1,-56
	ctx.r12.s64 = ctx.r1.s64 + -56;
	// bl 0x82cb6b08
	ctx.lr = 0x830C8B28;
	__restfpr_17(ctx, base);
	// b 0x82cb1130
	__restgprlr_26(ctx, base);
	return;
loc_830C8B2C:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// lfs f31,6048(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6048);
	ctx.f31.f64 = double(temp.f32);
	// bne cr6,0x830c8b44
	if (!ctx.cr6.eq) goto loc_830C8B44;
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// b 0x830c8b74
	goto loc_830C8B74;
loc_830C8B44:
	// lfs f13,88(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f13,f31
	ctx.cr6.compare(ctx.f13.f64, ctx.f31.f64);
	// bge cr6,0x830c8b5c
	if (!ctx.cr6.lt) goto loc_830C8B5C;
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// blt cr6,0x830c8b18
	if (ctx.cr6.lt) goto loc_830C8B18;
loc_830C8B5C:
	// fcmpu cr6,f13,f0
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f13.f64, ctx.f0.f64);
	// bge cr6,0x830c8b68
	if (!ctx.cr6.lt) goto loc_830C8B68;
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
loc_830C8B68:
	// fcmpu cr6,f0,f31
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// bge cr6,0x830c8b74
	if (!ctx.cr6.lt) goto loc_830C8B74;
	// fmr f0,f31
	ctx.f0.f64 = ctx.f31.f64;
loc_830C8B74:
	// fcmpu cr6,f0,f29
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f29.f64);
	// bgt cr6,0x830c8b18
	if (ctx.cr6.gt) goto loc_830C8B18;
	// lfs f13,0(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// rlwinm r9,r26,0,25,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 0) & 0x7C;
	// lfs f12,4(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f0,f13
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f10,8(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f8,f10,f0
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f7,0(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,4(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	ctx.f6.f64 = double(temp.f32);
	// li r11,0
	ctx.r11.s64 = 0;
	// lfs f5,8(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 8);
	ctx.f5.f64 = double(temp.f32);
	// li r10,19
	ctx.r10.s64 = 19;
	// rlwinm r9,r9,0,29,25
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFFFFC7;
	// addi r29,r30,4
	ctx.r29.s64 = ctx.r30.s64 + 4;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// fadds f4,f7,f11
	ctx.f4.f64 = double(float(ctx.f7.f64 + ctx.f11.f64));
	// stfs f4,4(r30)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r30.u32 + 4, temp.u32);
	// fadds f3,f6,f9
	ctx.f3.f64 = double(float(ctx.f6.f64 + ctx.f9.f64));
	// stfs f3,8(r30)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r30.u32 + 8, temp.u32);
	// fadds f2,f5,f8
	ctx.f2.f64 = double(float(ctx.f5.f64 + ctx.f8.f64));
	// stfs f2,12(r30)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r30.u32 + 12, temp.u32);
	// stfs f0,36(r30)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r30.u32 + 36, temp.u32);
	// lwz r8,144(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 144);
	// stfs f31,40(r30)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r30.u32 + 40, temp.u32);
	// stw r8,0(r30)
	PPC_STORE_U32(ctx.r30.u32 + 0, ctx.r8.u32);
	// stfs f31,44(r30)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r30.u32 + 44, temp.u32);
	// stw r11,28(r30)
	PPC_STORE_U32(ctx.r30.u32 + 28, ctx.r11.u32);
	// stw r11,32(r30)
	PPC_STORE_U32(ctx.r30.u32 + 32, ctx.r11.u32);
	// stw r10,52(r30)
	PPC_STORE_U32(ctx.r30.u32 + 52, ctx.r10.u32);
	// beq cr6,0x830c8ccc
	if (ctx.cr6.eq) goto loc_830C8CCC;
	// li r11,87
	ctx.r11.s64 = 87;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r11,52(r30)
	PPC_STORE_U32(ctx.r30.u32 + 52, ctx.r11.u32);
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x831bdd00
	ctx.lr = 0x830C8C0C;
	sub_831BDD00(ctx, base);
	// lfs f13,96(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// lfs f8,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f8.f64 = double(temp.f32);
	// addi r11,r30,16
	ctx.r11.s64 = ctx.r30.s64 + 16;
	// lfs f11,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f11.f64 = double(temp.f32);
	// fsubs f6,f8,f13
	ctx.f6.f64 = double(float(ctx.f8.f64 - ctx.f13.f64));
	// lfs f12,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f12.f64 = double(temp.f32);
	// lfs f10,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f10.f64 = double(temp.f32);
	// lfs f7,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f7.f64 = double(temp.f32);
	// fsubs f9,f10,f11
	ctx.f9.f64 = double(float(ctx.f10.f64 - ctx.f11.f64));
	// fsubs f5,f7,f12
	ctx.f5.f64 = double(float(ctx.f7.f64 - ctx.f12.f64));
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f3,f6,f0
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f4,f9,f0
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f2,f5,f0
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fadds f0,f3,f13
	ctx.f0.f64 = double(float(ctx.f3.f64 + ctx.f13.f64));
	// stfs f0,16(r30)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r30.u32 + 16, temp.u32);
	// fadds f1,f4,f11
	ctx.f1.f64 = double(float(ctx.f4.f64 + ctx.f11.f64));
	// stfs f1,24(r30)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r30.u32 + 24, temp.u32);
	// fadds f13,f2,f12
	ctx.f13.f64 = double(float(ctx.f2.f64 + ctx.f12.f64));
	// stfs f13,20(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r30.u32 + 20, temp.u32);
	// lfs f11,0(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,8(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 8);
	ctx.f9.f64 = double(temp.f32);
	// lfs f12,4(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// fmr f6,f0
	ctx.f6.f64 = ctx.f0.f64;
	// fmr f10,f1
	ctx.f10.f64 = ctx.f1.f64;
	// fmr f7,f13
	ctx.f7.f64 = ctx.f13.f64;
	// fsubs f4,f12,f7
	ctx.f4.f64 = double(float(ctx.f12.f64 - ctx.f7.f64));
	// stfs f4,20(r30)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r30.u32 + 20, temp.u32);
	// fsubs f5,f11,f6
	ctx.f5.f64 = double(float(ctx.f11.f64 - ctx.f6.f64));
	// stfs f5,16(r30)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r30.u32 + 16, temp.u32);
	// fmr f0,f4
	ctx.f0.f64 = ctx.f4.f64;
	// fsubs f8,f9,f10
	ctx.f8.f64 = double(float(ctx.f9.f64 - ctx.f10.f64));
	// stfs f8,24(r30)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r30.u32 + 24, temp.u32);
	// fmr f13,f5
	ctx.f13.f64 = ctx.f5.f64;
	// fmuls f3,f0,f0
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f0.f64));
	// fmr f12,f8
	ctx.f12.f64 = ctx.f8.f64;
	// fmadds f2,f13,f13,f3
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 + ctx.f3.f64));
	// fmadds f1,f12,f12,f2
	ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f12.f64 + ctx.f2.f64));
	// fsqrts f11,f1
	ctx.f11.f64 = double(float(sqrt(ctx.f1.f64)));
	// fcmpu cr6,f11,f31
	ctx.cr6.compare(ctx.f11.f64, ctx.f31.f64);
	// beq cr6,0x830c8ccc
	if (ctx.cr6.eq) goto loc_830C8CCC;
	// fdivs f11,f30,f11
	ctx.f11.f64 = double(float(ctx.f30.f64 / ctx.f11.f64));
	// fmuls f10,f11,f13
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// stfs f10,0(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// fmuls f9,f11,f0
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f9,4(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r11.u32 + 4, temp.u32);
	// fmuls f8,f11,f12
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// stfs f8,8(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
loc_830C8CCC:
	// rlwinm r11,r26,0,24,24
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 0) & 0x80;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c8cec
	if (ctx.cr6.eq) goto loc_830C8CEC;
	// lwz r11,52(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 52);
	// ori r10,r11,128
	ctx.r10.u64 = ctx.r11.u64 | 128;
	// stw r10,52(r30)
	PPC_STORE_U32(ctx.r30.u32 + 52, ctx.r10.u32);
	// lhz r9,306(r31)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r31.u32 + 306);
	// sth r9,48(r30)
	PPC_STORE_U16(ctx.r30.u32 + 48, ctx.r9.u16);
loc_830C8CEC:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// addi r12,r1,-56
	ctx.r12.s64 = ctx.r1.s64 + -56;
	// bl 0x82cb6b08
	ctx.lr = 0x830C8CFC;
	__restfpr_17(ctx, base);
	// b 0x82cb1130
	__restgprlr_26(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830C8D00"))) PPC_WEAK_FUNC(sub_830C8D00);
PPC_FUNC_IMPL(__imp__sub_830C8D00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6abc
	ctx.lr = 0x830C8D10;
	__savefpr_17(ctx, base);
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// lwz r10,264(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830c8f2c
	if (ctx.cr6.eq) goto loc_830C8F2C;
	// lwz r9,280(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 280);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmplw cr6,r9,r8
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, ctx.xer);
	// beq cr6,0x830c8f2c
	if (ctx.cr6.eq) goto loc_830C8F2C;
	// lfs f12,252(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 252);
	ctx.f12.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f11,112(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 112);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f12
	ctx.f10.f64 = ctx.f12.f64;
	// fmuls f9,f11,f12
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f7,244(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 244);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,124(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 124);
	ctx.f6.f64 = double(temp.f32);
	// fmr f30,f7
	ctx.f30.f64 = ctx.f7.f64;
	// fmuls f3,f6,f7
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// lfs f1,248(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 248);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f2,f6,f12
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// lfs f8,256(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 256);
	ctx.f8.f64 = double(temp.f32);
	// lfs f4,116(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 116);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f11,f7
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// fmr f29,f1
	ctx.f29.f64 = ctx.f1.f64;
	// lfs f31,120(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 120);
	ctx.f31.f64 = double(temp.f32);
	// lfs f28,136(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 136);
	ctx.f28.f64 = double(temp.f32);
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f27,128(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 128);
	ctx.f27.f64 = double(temp.f32);
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// lfs f13,6380(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 6380);
	ctx.f13.f64 = double(temp.f32);
	// addi r9,r11,112
	ctx.r9.s64 = ctx.r11.s64 + 112;
	// lfs f26,132(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 132);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f25,f28,f10
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f10.f64));
	// fmadds f9,f4,f8,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f9.f64));
	// lfs f23,260(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 260);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f21,f28,f30
	ctx.f21.f64 = double(float(ctx.f28.f64 * ctx.f30.f64));
	// lfs f0,7676(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f3,f11,f8,f3
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f8.f64 + ctx.f3.f64));
	// lfs f22,264(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 264);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f2,f31,f8,f2
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f8.f64 + ctx.f2.f64));
	// lfs f20,268(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fmsubs f5,f6,f8,f5
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f8.f64 - ctx.f5.f64));
	// addi r9,r10,244
	ctx.r9.s64 = ctx.r10.s64 + 244;
	// fmsubs f24,f8,f8,f13
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f8.f64 - ctx.f13.f64));
	// lfs f13,6140(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 6140);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f19,f27,f29
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// addi r8,r11,12
	ctx.r8.s64 = ctx.r11.s64 + 12;
	// fmuls f18,f26,f10
	ctx.f18.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// fmadds f25,f27,f30,f25
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f30.f64 + ctx.f25.f64));
	// fmadds f9,f6,f1,f9
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f1.f64 + ctx.f9.f64));
	// fmsubs f21,f27,f10,f21
	ctx.f21.f64 = double(float(ctx.f27.f64 * ctx.f10.f64 - ctx.f21.f64));
	// fmadds f3,f31,f1,f3
	ctx.f3.f64 = double(float(ctx.f31.f64 * ctx.f1.f64 + ctx.f3.f64));
	// fmadds f2,f4,f7,f2
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f7.f64 + ctx.f2.f64));
	// fnmsubs f5,f4,f1,f5
	ctx.f5.f64 = double(float(-(ctx.f4.f64 * ctx.f1.f64 - ctx.f5.f64)));
	// fmuls f17,f28,f24
	ctx.f17.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// fmsubs f19,f26,f30,f19
	ctx.f19.f64 = double(float(ctx.f26.f64 * ctx.f30.f64 - ctx.f19.f64));
	// fmsubs f28,f28,f29,f18
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f29.f64 - ctx.f18.f64));
	// fmuls f6,f26,f24
	ctx.f6.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// fmadds f26,f26,f29,f25
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f25.f64));
	// fnmsubs f9,f31,f7,f9
	ctx.f9.f64 = double(float(-(ctx.f31.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// fmuls f7,f27,f24
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// fnmsubs f4,f4,f12,f3
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f12.f64 - ctx.f3.f64)));
	// fnmsubs f3,f11,f1,f2
	ctx.f3.f64 = double(float(-(ctx.f11.f64 * ctx.f1.f64 - ctx.f2.f64)));
	// fmuls f2,f21,f8
	ctx.f2.f64 = double(float(ctx.f21.f64 * ctx.f8.f64));
	// fnmsubs f5,f31,f12,f5
	ctx.f5.f64 = double(float(-(ctx.f31.f64 * ctx.f12.f64 - ctx.f5.f64)));
	// fmuls f1,f19,f8
	ctx.f1.f64 = double(float(ctx.f19.f64 * ctx.f8.f64));
	// fmuls f12,f28,f8
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f8.f64));
	// fmuls f31,f26,f30
	ctx.f31.f64 = double(float(ctx.f26.f64 * ctx.f30.f64));
	// fmuls f8,f9,f9
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f9.f64));
	// fmuls f11,f26,f29
	ctx.f11.f64 = double(float(ctx.f26.f64 * ctx.f29.f64));
	// fmuls f30,f9,f4
	ctx.f30.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// fmuls f29,f3,f3
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// fadds f6,f6,f2
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f2.f64));
	// fmuls f28,f5,f3
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fadds f2,f17,f1
	ctx.f2.f64 = double(float(ctx.f17.f64 + ctx.f1.f64));
	// fadds f1,f7,f12
	ctx.f1.f64 = double(float(ctx.f7.f64 + ctx.f12.f64));
	// fmuls f10,f26,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// fmuls f12,f8,f0
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmuls f8,f30,f0
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmuls f7,f29,f0
	ctx.f7.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// fadds f6,f6,f11
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f11.f64));
	// fmuls f30,f28,f0
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// fadds f2,f2,f10
	ctx.f2.f64 = double(float(ctx.f2.f64 + ctx.f10.f64));
	// fsubs f11,f13,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// fsubs f10,f8,f30
	ctx.f10.f64 = double(float(ctx.f8.f64 - ctx.f30.f64));
	// stfs f10,84(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmuls f10,f3,f4
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f4.f64));
	// fsubs f11,f11,f7
	ctx.f11.f64 = double(float(ctx.f11.f64 - ctx.f7.f64));
	// stfs f11,80(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmuls f11,f5,f9
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// fmuls f9,f3,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f9.f64));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// fmuls f5,f5,f4
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// fmuls f29,f4,f4
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f4.f64));
	// li r7,9
	ctx.r7.s64 = 9;
	// fadds f4,f1,f31
	ctx.f4.f64 = double(float(ctx.f1.f64 + ctx.f31.f64));
	// fmuls f3,f2,f0
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fadds f1,f30,f8
	ctx.f1.f64 = double(float(ctx.f30.f64 + ctx.f8.f64));
	// stfs f1,92(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fmuls f8,f11,f0
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f2,f6,f0
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f1,f9,f0
	ctx.f1.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f11,f5,f0
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fnmsubs f6,f29,f0,f13
	ctx.f6.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f9,f4,f0
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fadds f13,f20,f3
	ctx.f13.f64 = double(float(ctx.f20.f64 + ctx.f3.f64));
	// fadds f0,f22,f2
	ctx.f0.f64 = double(float(ctx.f22.f64 + ctx.f2.f64));
	// fadds f5,f8,f10
	ctx.f5.f64 = double(float(ctx.f8.f64 + ctx.f10.f64));
	// stfs f5,88(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fsubs f3,f10,f8
	ctx.f3.f64 = double(float(ctx.f10.f64 - ctx.f8.f64));
	// stfs f3,104(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fsubs f2,f1,f11
	ctx.f2.f64 = double(float(ctx.f1.f64 - ctx.f11.f64));
	// stfs f2,100(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fsubs f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 - ctx.f12.f64));
	// stfs f12,112(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fsubs f4,f6,f7
	ctx.f4.f64 = double(float(ctx.f6.f64 - ctx.f7.f64));
	// stfs f4,96(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f1,f11,f1
	ctx.f1.f64 = double(float(ctx.f11.f64 + ctx.f1.f64));
	// stfs f1,108(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fadds f12,f9,f23
	ctx.f12.f64 = double(float(ctx.f9.f64 + ctx.f23.f64));
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
loc_830C8F00:
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bdnz 0x830c8f00
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C8F00;
	// stfs f12,36(r8)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r8.u32 + 36, temp.u32);
	// stfs f0,40(r8)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 40, temp.u32);
	// stfs f13,44(r8)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r8.u32 + 44, temp.u32);
	// lwz r10,264(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	// lwz r9,280(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 280);
	// stw r9,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r9.u32);
loc_830C8F2C:
	// lfs f0,340(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 340);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r11,12
	ctx.r10.s64 = ctx.r11.s64 + 12;
	// lfs f13,16(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lfs f12,28(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f10,40(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f8,f10,f0
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f6,52(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 52);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,56(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 56);
	ctx.f5.f64 = double(temp.f32);
	// fmr f3,f6
	ctx.f3.f64 = ctx.f6.f64;
	// fmr f2,f5
	ctx.f2.f64 = ctx.f5.f64;
	// lfs f7,48(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 48);
	ctx.f7.f64 = double(temp.f32);
	// fmr f4,f7
	ctx.f4.f64 = ctx.f7.f64;
	// lfs f1,336(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 336);
	ctx.f1.f64 = double(temp.f32);
	// stfs f1,104(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// addi r10,r10,36
	ctx.r10.s64 = ctx.r10.s64 + 36;
	// fneg f0,f11
	ctx.f0.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// fneg f13,f9
	ctx.f13.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// fneg f12,f8
	ctx.f12.u64 = ctx.f8.u64 ^ 0x8000000000000000;
	// fadds f10,f3,f9
	ctx.f10.f64 = double(float(ctx.f3.f64 + ctx.f9.f64));
	// stfs f10,96(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f9,f2,f8
	ctx.f9.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f9,100(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f11,f4,f11
	ctx.f11.f64 = double(float(ctx.f4.f64 + ctx.f11.f64));
	// stfs f11,92(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fadds f8,f7,f0
	ctx.f8.f64 = double(float(ctx.f7.f64 + ctx.f0.f64));
	// stfs f8,80(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fadds f7,f6,f13
	ctx.f7.f64 = double(float(ctx.f6.f64 + ctx.f13.f64));
	// stfs f7,84(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fadds f6,f5,f12
	ctx.f6.f64 = double(float(ctx.f5.f64 + ctx.f12.f64));
	// stfs f6,88(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// bl 0x83102ad0
	ctx.lr = 0x830C8FB4;
	sub_83102AD0(ctx, base);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6b08
	ctx.lr = 0x830C8FC0;
	__restfpr_17(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C8FCC"))) PPC_WEAK_FUNC(sub_830C8FCC);
PPC_FUNC_IMPL(__imp__sub_830C8FCC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C8FD0"))) PPC_WEAK_FUNC(sub_830C8FD0);
PPC_FUNC_IMPL(__imp__sub_830C8FD0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6abc
	ctx.lr = 0x830C8FE0;
	__savefpr_17(ctx, base);
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lwz r10,264(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830c91f8
	if (ctx.cr6.eq) goto loc_830C91F8;
	// lwz r9,280(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 280);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmplw cr6,r9,r8
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, ctx.xer);
	// beq cr6,0x830c91f8
	if (ctx.cr6.eq) goto loc_830C91F8;
	// lfs f12,252(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 252);
	ctx.f12.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f11,112(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 112);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f12
	ctx.f10.f64 = ctx.f12.f64;
	// fmuls f9,f11,f12
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f7,244(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 244);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,124(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 124);
	ctx.f6.f64 = double(temp.f32);
	// fmr f30,f7
	ctx.f30.f64 = ctx.f7.f64;
	// fmuls f3,f6,f7
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// lfs f1,248(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 248);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f2,f6,f12
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// lfs f8,256(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 256);
	ctx.f8.f64 = double(temp.f32);
	// lfs f4,116(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 116);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f11,f7
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// fmr f29,f1
	ctx.f29.f64 = ctx.f1.f64;
	// lfs f31,120(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 120);
	ctx.f31.f64 = double(temp.f32);
	// lfs f28,136(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 136);
	ctx.f28.f64 = double(temp.f32);
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f27,128(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 128);
	ctx.f27.f64 = double(temp.f32);
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// lfs f13,6380(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 6380);
	ctx.f13.f64 = double(temp.f32);
	// addi r9,r11,112
	ctx.r9.s64 = ctx.r11.s64 + 112;
	// lfs f26,132(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 132);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f25,f28,f10
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f10.f64));
	// fmadds f9,f4,f8,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f9.f64));
	// lfs f23,260(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 260);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f21,f28,f30
	ctx.f21.f64 = double(float(ctx.f28.f64 * ctx.f30.f64));
	// lfs f0,7676(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f3,f11,f8,f3
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f8.f64 + ctx.f3.f64));
	// lfs f22,264(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 264);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f2,f31,f8,f2
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f8.f64 + ctx.f2.f64));
	// lfs f20,268(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fmsubs f5,f6,f8,f5
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f8.f64 - ctx.f5.f64));
	// addi r9,r10,244
	ctx.r9.s64 = ctx.r10.s64 + 244;
	// fmsubs f24,f8,f8,f13
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f8.f64 - ctx.f13.f64));
	// lfs f13,6140(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 6140);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f19,f27,f29
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// addi r8,r11,12
	ctx.r8.s64 = ctx.r11.s64 + 12;
	// fmuls f18,f26,f10
	ctx.f18.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// fmadds f25,f27,f30,f25
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f30.f64 + ctx.f25.f64));
	// fmadds f9,f6,f1,f9
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f1.f64 + ctx.f9.f64));
	// fmsubs f21,f27,f10,f21
	ctx.f21.f64 = double(float(ctx.f27.f64 * ctx.f10.f64 - ctx.f21.f64));
	// fmadds f3,f31,f1,f3
	ctx.f3.f64 = double(float(ctx.f31.f64 * ctx.f1.f64 + ctx.f3.f64));
	// fmadds f2,f4,f7,f2
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f7.f64 + ctx.f2.f64));
	// fnmsubs f5,f4,f1,f5
	ctx.f5.f64 = double(float(-(ctx.f4.f64 * ctx.f1.f64 - ctx.f5.f64)));
	// fmuls f17,f28,f24
	ctx.f17.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// fmsubs f19,f26,f30,f19
	ctx.f19.f64 = double(float(ctx.f26.f64 * ctx.f30.f64 - ctx.f19.f64));
	// fmsubs f28,f28,f29,f18
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f29.f64 - ctx.f18.f64));
	// fmuls f6,f26,f24
	ctx.f6.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// fmadds f26,f26,f29,f25
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f25.f64));
	// fnmsubs f9,f31,f7,f9
	ctx.f9.f64 = double(float(-(ctx.f31.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// fmuls f7,f27,f24
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// fnmsubs f4,f4,f12,f3
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f12.f64 - ctx.f3.f64)));
	// fnmsubs f3,f11,f1,f2
	ctx.f3.f64 = double(float(-(ctx.f11.f64 * ctx.f1.f64 - ctx.f2.f64)));
	// fmuls f2,f21,f8
	ctx.f2.f64 = double(float(ctx.f21.f64 * ctx.f8.f64));
	// fnmsubs f5,f31,f12,f5
	ctx.f5.f64 = double(float(-(ctx.f31.f64 * ctx.f12.f64 - ctx.f5.f64)));
	// fmuls f1,f19,f8
	ctx.f1.f64 = double(float(ctx.f19.f64 * ctx.f8.f64));
	// fmuls f12,f28,f8
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f8.f64));
	// fmuls f31,f26,f30
	ctx.f31.f64 = double(float(ctx.f26.f64 * ctx.f30.f64));
	// fmuls f8,f9,f9
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f9.f64));
	// fmuls f11,f26,f29
	ctx.f11.f64 = double(float(ctx.f26.f64 * ctx.f29.f64));
	// fmuls f30,f9,f4
	ctx.f30.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// fmuls f29,f3,f3
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// fadds f6,f6,f2
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f2.f64));
	// fmuls f28,f5,f3
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fadds f2,f17,f1
	ctx.f2.f64 = double(float(ctx.f17.f64 + ctx.f1.f64));
	// fadds f1,f7,f12
	ctx.f1.f64 = double(float(ctx.f7.f64 + ctx.f12.f64));
	// fmuls f10,f26,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// fmuls f12,f8,f0
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmuls f8,f30,f0
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmuls f7,f29,f0
	ctx.f7.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// fadds f6,f6,f11
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f11.f64));
	// fmuls f30,f28,f0
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// fadds f2,f2,f10
	ctx.f2.f64 = double(float(ctx.f2.f64 + ctx.f10.f64));
	// fsubs f11,f13,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// fsubs f10,f8,f30
	ctx.f10.f64 = double(float(ctx.f8.f64 - ctx.f30.f64));
	// stfs f10,84(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmuls f10,f3,f4
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f4.f64));
	// fsubs f11,f11,f7
	ctx.f11.f64 = double(float(ctx.f11.f64 - ctx.f7.f64));
	// stfs f11,80(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmuls f11,f5,f9
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// fmuls f9,f3,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f9.f64));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// fmuls f5,f5,f4
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// fmuls f29,f4,f4
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f4.f64));
	// li r7,9
	ctx.r7.s64 = 9;
	// fadds f4,f1,f31
	ctx.f4.f64 = double(float(ctx.f1.f64 + ctx.f31.f64));
	// fmuls f3,f2,f0
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fadds f1,f30,f8
	ctx.f1.f64 = double(float(ctx.f30.f64 + ctx.f8.f64));
	// stfs f1,92(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fmuls f8,f11,f0
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f2,f6,f0
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f1,f9,f0
	ctx.f1.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f11,f5,f0
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fnmsubs f6,f29,f0,f13
	ctx.f6.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f9,f4,f0
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fadds f13,f20,f3
	ctx.f13.f64 = double(float(ctx.f20.f64 + ctx.f3.f64));
	// fadds f0,f22,f2
	ctx.f0.f64 = double(float(ctx.f22.f64 + ctx.f2.f64));
	// fadds f5,f8,f10
	ctx.f5.f64 = double(float(ctx.f8.f64 + ctx.f10.f64));
	// stfs f5,88(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fsubs f3,f10,f8
	ctx.f3.f64 = double(float(ctx.f10.f64 - ctx.f8.f64));
	// stfs f3,104(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fsubs f2,f1,f11
	ctx.f2.f64 = double(float(ctx.f1.f64 - ctx.f11.f64));
	// stfs f2,100(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fsubs f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 - ctx.f12.f64));
	// stfs f12,112(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fsubs f4,f6,f7
	ctx.f4.f64 = double(float(ctx.f6.f64 - ctx.f7.f64));
	// stfs f4,96(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f1,f11,f1
	ctx.f1.f64 = double(float(ctx.f11.f64 + ctx.f1.f64));
	// stfs f1,108(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fadds f12,f9,f23
	ctx.f12.f64 = double(float(ctx.f9.f64 + ctx.f23.f64));
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
loc_830C91CC:
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bdnz 0x830c91cc
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C91CC;
	// stfs f12,36(r8)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r8.u32 + 36, temp.u32);
	// stfs f0,40(r8)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 40, temp.u32);
	// stfs f13,44(r8)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r8.u32 + 44, temp.u32);
	// lwz r10,264(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	// lwz r9,280(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 280);
	// stw r9,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r9.u32);
loc_830C91F8:
	// lfs f0,340(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 340);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r11,12
	ctx.r10.s64 = ctx.r11.s64 + 12;
	// lfs f13,16(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// addi r6,r4,24
	ctx.r6.s64 = ctx.r4.s64 + 24;
	// lfs f12,28(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f10,40(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f8,f10,f0
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f6,52(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 52);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,56(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 56);
	ctx.f5.f64 = double(temp.f32);
	// fmr f3,f6
	ctx.f3.f64 = ctx.f6.f64;
	// fmr f2,f5
	ctx.f2.f64 = ctx.f5.f64;
	// lfs f7,48(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 48);
	ctx.f7.f64 = double(temp.f32);
	// fmr f4,f7
	ctx.f4.f64 = ctx.f7.f64;
	// lfs f1,336(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 336);
	ctx.f1.f64 = double(temp.f32);
	// stfs f1,104(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// addi r5,r4,12
	ctx.r5.s64 = ctx.r4.s64 + 12;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// fneg f0,f11
	ctx.f0.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// addi r11,r10,36
	ctx.r11.s64 = ctx.r10.s64 + 36;
	// fneg f13,f9
	ctx.f13.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// fneg f12,f8
	ctx.f12.u64 = ctx.f8.u64 ^ 0x8000000000000000;
	// fadds f10,f3,f9
	ctx.f10.f64 = double(float(ctx.f3.f64 + ctx.f9.f64));
	// stfs f10,96(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f9,f2,f8
	ctx.f9.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f9,100(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f11,f4,f11
	ctx.f11.f64 = double(float(ctx.f4.f64 + ctx.f11.f64));
	// stfs f11,92(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fadds f8,f7,f0
	ctx.f8.f64 = double(float(ctx.f7.f64 + ctx.f0.f64));
	// stfs f8,80(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fadds f7,f6,f13
	ctx.f7.f64 = double(float(ctx.f6.f64 + ctx.f13.f64));
	// stfs f7,84(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fadds f6,f5,f12
	ctx.f6.f64 = double(float(ctx.f5.f64 + ctx.f12.f64));
	// stfs f6,88(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// bl 0x83137248
	ctx.lr = 0x830C9290;
	sub_83137248(ctx, base);
	// lfs f0,104(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f5,f0,f0
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f0.f64));
	// li r11,1
	ctx.r11.s64 = 1;
	// fcmpu cr6,f1,f5
	ctx.cr6.compare(ctx.f1.f64, ctx.f5.f64);
	// blt cr6,0x830c92a8
	if (ctx.cr6.lt) goto loc_830C92A8;
	// li r11,0
	ctx.r11.s64 = 0;
loc_830C92A8:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6b08
	ctx.lr = 0x830C92B8;
	__restfpr_17(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C92C4"))) PPC_WEAK_FUNC(sub_830C92C4);
PPC_FUNC_IMPL(__imp__sub_830C92C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C92C8"))) PPC_WEAK_FUNC(sub_830C92C8);
PPC_FUNC_IMPL(__imp__sub_830C92C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6ab4
	ctx.lr = 0x830C92D8;
	__savefpr_15(ctx, base);
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f12,6140(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6140);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,6380(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 6380);
	ctx.f13.f64 = double(temp.f32);
	// beq cr6,0x830c94f0
	if (ctx.cr6.eq) goto loc_830C94F0;
	// lwz r9,280(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplw cr6,r9,r8
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, ctx.xer);
	// beq cr6,0x830c94f0
	if (ctx.cr6.eq) goto loc_830C94F0;
	// lfs f11,252(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f11.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f10,112(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 112);
	ctx.f10.f64 = double(temp.f32);
	// fmr f9,f11
	ctx.f9.f64 = ctx.f11.f64;
	// fmuls f8,f10,f11
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
	// lfs f6,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f6.f64 = double(temp.f32);
	// lfs f4,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f3,f10,f6
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// lfs f2,120(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 120);
	ctx.f2.f64 = double(temp.f32);
	// fmr f27,f4
	ctx.f27.f64 = ctx.f4.f64;
	// lfs f31,124(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 124);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f30,f2,f4
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f4.f64));
	// fmuls f28,f31,f11
	ctx.f28.f64 = double(float(ctx.f31.f64 * ctx.f11.f64));
	// lfs f7,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f7.f64 = double(temp.f32);
	// lfs f29,116(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 116);
	ctx.f29.f64 = double(temp.f32);
	// fmr f1,f6
	ctx.f1.f64 = ctx.f6.f64;
	// lfs f26,136(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	ctx.f26.f64 = double(temp.f32);
	// fmsubs f5,f7,f7,f13
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f7.f64 - ctx.f13.f64));
	// lfs f25,132(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 132);
	ctx.f25.f64 = double(temp.f32);
	// addi r9,r3,112
	ctx.r9.s64 = ctx.r3.s64 + 112;
	// lfs f24,128(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 128);
	ctx.f24.f64 = double(temp.f32);
	// addi r9,r11,244
	ctx.r9.s64 = ctx.r11.s64 + 244;
	// fmuls f23,f25,f9
	ctx.f23.f64 = double(float(ctx.f25.f64 * ctx.f9.f64));
	// lfs f0,7676(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f8,f29,f7,f8
	ctx.f8.f64 = double(float(ctx.f29.f64 * ctx.f7.f64 + ctx.f8.f64));
	// lfs f22,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f22.f64 = double(temp.f32);
	// fmsubs f3,f31,f7,f3
	ctx.f3.f64 = double(float(ctx.f31.f64 * ctx.f7.f64 - ctx.f3.f64));
	// lfs f20,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f17,f24,f27
	ctx.f17.f64 = double(float(ctx.f24.f64 * ctx.f27.f64));
	// lfs f19,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f30,f31,f6,f30
	ctx.f30.f64 = double(float(ctx.f31.f64 * ctx.f6.f64 + ctx.f30.f64));
	// addi r8,r3,12
	ctx.r8.s64 = ctx.r3.s64 + 12;
	// fmadds f28,f2,f7,f28
	ctx.f28.f64 = double(float(ctx.f2.f64 * ctx.f7.f64 + ctx.f28.f64));
	// fmuls f18,f26,f1
	ctx.f18.f64 = double(float(ctx.f26.f64 * ctx.f1.f64));
	// fmuls f16,f25,f27
	ctx.f16.f64 = double(float(ctx.f25.f64 * ctx.f27.f64));
	// fmuls f15,f25,f5
	ctx.f15.f64 = double(float(ctx.f25.f64 * ctx.f5.f64));
	// fmuls f21,f24,f5
	ctx.f21.f64 = double(float(ctx.f24.f64 * ctx.f5.f64));
	// fmsubs f23,f26,f27,f23
	ctx.f23.f64 = double(float(ctx.f26.f64 * ctx.f27.f64 - ctx.f23.f64));
	// fmadds f8,f31,f4,f8
	ctx.f8.f64 = double(float(ctx.f31.f64 * ctx.f4.f64 + ctx.f8.f64));
	// fnmsubs f3,f29,f4,f3
	ctx.f3.f64 = double(float(-(ctx.f29.f64 * ctx.f4.f64 - ctx.f3.f64)));
	// fmsubs f25,f25,f1,f17
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f1.f64 - ctx.f17.f64));
	// fmadds f30,f10,f7,f30
	ctx.f30.f64 = double(float(ctx.f10.f64 * ctx.f7.f64 + ctx.f30.f64));
	// fmadds f28,f29,f6,f28
	ctx.f28.f64 = double(float(ctx.f29.f64 * ctx.f6.f64 + ctx.f28.f64));
	// fmsubs f31,f24,f9,f18
	ctx.f31.f64 = double(float(ctx.f24.f64 * ctx.f9.f64 - ctx.f18.f64));
	// fmadds f24,f24,f1,f16
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f1.f64 + ctx.f16.f64));
	// fmuls f5,f26,f5
	ctx.f5.f64 = double(float(ctx.f26.f64 * ctx.f5.f64));
	// fmuls f23,f23,f7
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f7.f64));
	// fnmsubs f8,f2,f6,f8
	ctx.f8.f64 = double(float(-(ctx.f2.f64 * ctx.f6.f64 - ctx.f8.f64)));
	// fnmsubs f6,f2,f11,f3
	ctx.f6.f64 = double(float(-(ctx.f2.f64 * ctx.f11.f64 - ctx.f3.f64)));
	// fnmsubs f2,f29,f11,f30
	ctx.f2.f64 = double(float(-(ctx.f29.f64 * ctx.f11.f64 - ctx.f30.f64)));
	// fnmsubs f11,f10,f4,f28
	ctx.f11.f64 = double(float(-(ctx.f10.f64 * ctx.f4.f64 - ctx.f28.f64)));
	// fmuls f3,f31,f7
	ctx.f3.f64 = double(float(ctx.f31.f64 * ctx.f7.f64));
	// fmuls f10,f25,f7
	ctx.f10.f64 = double(float(ctx.f25.f64 * ctx.f7.f64));
	// fmadds f7,f26,f9,f24
	ctx.f7.f64 = double(float(ctx.f26.f64 * ctx.f9.f64 + ctx.f24.f64));
	// fadds f4,f21,f23
	ctx.f4.f64 = double(float(ctx.f21.f64 + ctx.f23.f64));
	// fmuls f31,f8,f8
	ctx.f31.f64 = double(float(ctx.f8.f64 * ctx.f8.f64));
	// fmuls f30,f8,f2
	ctx.f30.f64 = double(float(ctx.f8.f64 * ctx.f2.f64));
	// fmuls f29,f11,f11
	ctx.f29.f64 = double(float(ctx.f11.f64 * ctx.f11.f64));
	// fmuls f28,f6,f11
	ctx.f28.f64 = double(float(ctx.f6.f64 * ctx.f11.f64));
	// fadds f5,f5,f10
	ctx.f5.f64 = double(float(ctx.f5.f64 + ctx.f10.f64));
	// fmuls f1,f7,f1
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f1.f64));
	// fmuls f9,f7,f9
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f9.f64));
	// fmuls f27,f7,f27
	ctx.f27.f64 = double(float(ctx.f7.f64 * ctx.f27.f64));
	// fmuls f7,f31,f0
	ctx.f7.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fadds f3,f15,f3
	ctx.f3.f64 = double(float(ctx.f15.f64 + ctx.f3.f64));
	// fmuls f10,f30,f0
	ctx.f10.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmuls f31,f29,f0
	ctx.f31.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// fmuls f30,f28,f0
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// fadds f4,f4,f1
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f1.f64));
	// fadds f9,f5,f9
	ctx.f9.f64 = double(float(ctx.f5.f64 + ctx.f9.f64));
	// fsubs f1,f12,f7
	ctx.f1.f64 = double(float(ctx.f12.f64 - ctx.f7.f64));
	// fadds f3,f3,f27
	ctx.f3.f64 = double(float(ctx.f3.f64 + ctx.f27.f64));
	// fsubs f5,f10,f30
	ctx.f5.f64 = double(float(ctx.f10.f64 - ctx.f30.f64));
	// stfs f5,84(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmuls f5,f11,f2
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f2.f64));
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fmuls f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fsubs f1,f1,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 - ctx.f31.f64));
	// stfs f1,80(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmuls f3,f3,f0
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmuls f1,f6,f8
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f8.f64));
	// fmuls f29,f2,f2
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f2.f64));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// fmuls f6,f6,f2
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f2.f64));
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// fmuls f8,f11,f8
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f8.f64));
	// li r7,9
	ctx.r7.s64 = 9;
	// fadds f2,f30,f10
	ctx.f2.f64 = double(float(ctx.f30.f64 + ctx.f10.f64));
	// stfs f2,92(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fadds f10,f19,f9
	ctx.f10.f64 = double(float(ctx.f19.f64 + ctx.f9.f64));
	// fadds f11,f20,f3
	ctx.f11.f64 = double(float(ctx.f20.f64 + ctx.f3.f64));
	// fadds f9,f22,f4
	ctx.f9.f64 = double(float(ctx.f22.f64 + ctx.f4.f64));
	// fmuls f4,f1,f0
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fnmsubs f3,f29,f0,f12
	ctx.f3.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// fmuls f1,f6,f0
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f2,f8,f0
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fadds f0,f4,f5
	ctx.f0.f64 = double(float(ctx.f4.f64 + ctx.f5.f64));
	// stfs f0,88(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fsubs f6,f5,f4
	ctx.f6.f64 = double(float(ctx.f5.f64 - ctx.f4.f64));
	// stfs f6,104(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fsubs f8,f3,f31
	ctx.f8.f64 = double(float(ctx.f3.f64 - ctx.f31.f64));
	// stfs f8,96(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fsubs f3,f3,f7
	ctx.f3.f64 = double(float(ctx.f3.f64 - ctx.f7.f64));
	// stfs f3,112(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fsubs f5,f2,f1
	ctx.f5.f64 = double(float(ctx.f2.f64 - ctx.f1.f64));
	// stfs f5,100(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f4,f1,f2
	ctx.f4.f64 = double(float(ctx.f1.f64 + ctx.f2.f64));
	// stfs f4,108(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
loc_830C94C4:
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bdnz 0x830c94c4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C94C4;
	// stfs f9,36(r8)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r8.u32 + 36, temp.u32);
	// stfs f11,40(r8)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r8.u32 + 40, temp.u32);
	// stfs f10,44(r8)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r8.u32 + 44, temp.u32);
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// lwz r9,280(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
loc_830C94F0:
	// lfs f0,340(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	ctx.f0.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f8,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f8.f64 = double(temp.f32);
	// addi r11,r3,12
	ctx.r11.s64 = ctx.r3.s64 + 12;
	// lfs f2,12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f6,f8,f0
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmr f8,f2
	ctx.f8.f64 = ctx.f2.f64;
	// lfs f11,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,28(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f11,f0
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// lfs f5,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f7,f10,f0
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f3,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// fadds f0,f5,f2
	ctx.f0.f64 = double(float(ctx.f5.f64 + ctx.f2.f64));
	// lfs f11,20(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	ctx.f11.f64 = double(temp.f32);
	// li r8,0
	ctx.r8.s64 = 0;
	// fadds f2,f11,f3
	ctx.f2.f64 = double(float(ctx.f11.f64 + ctx.f3.f64));
	// lfs f4,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f4.f64 = double(temp.f32);
	// lfs f1,16(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	ctx.f1.f64 = double(temp.f32);
	// li r7,0
	ctx.r7.s64 = 0;
	// fadds f10,f1,f4
	ctx.f10.f64 = double(float(ctx.f1.f64 + ctx.f4.f64));
	// lfs f30,56(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	ctx.f30.f64 = double(temp.f32);
	// lfs f25,336(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	ctx.f25.f64 = double(temp.f32);
	// addi r6,r1,152
	ctx.r6.s64 = ctx.r1.s64 + 152;
	// fneg f29,f6
	ctx.f29.u64 = ctx.f6.u64 ^ 0x8000000000000000;
	// stfs f12,152(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fsubs f11,f8,f5
	ctx.f11.f64 = double(float(ctx.f8.f64 - ctx.f5.f64));
	// lfs f8,20(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	ctx.f8.f64 = double(temp.f32);
	// fsubs f5,f1,f4
	ctx.f5.f64 = double(float(ctx.f1.f64 - ctx.f4.f64));
	// stfs f12,168(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 168, temp.u32);
	// fsubs f4,f8,f3
	ctx.f4.f64 = double(float(ctx.f8.f64 - ctx.f3.f64));
	// lfs f3,48(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	ctx.f3.f64 = double(temp.f32);
	// fmr f28,f3
	ctx.f28.f64 = ctx.f3.f64;
	// lfs f8,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f8.f64 = double(temp.f32);
	// fneg f1,f9
	ctx.f1.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// stfs f12,184(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// fmr f27,f8
	ctx.f27.f64 = ctx.f8.f64;
	// stfs f25,104(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fneg f31,f7
	ctx.f31.u64 = ctx.f7.u64 ^ 0x8000000000000000;
	// addi r5,r1,140
	ctx.r5.s64 = ctx.r1.s64 + 140;
	// fmuls f26,f0,f13
	ctx.f26.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f0,6048(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6048);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f10,f10,f13
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// stfs f0,156(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 156, temp.u32);
	// fmuls f2,f2,f13
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// stfs f0,160(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// fmuls f11,f11,f13
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// stfs f0,164(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// fmuls f5,f5,f13
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// stfs f0,172(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// fmuls f4,f4,f13
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// stfs f0,176(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// fadds f9,f9,f28
	ctx.f9.f64 = double(float(ctx.f9.f64 + ctx.f28.f64));
	// stfs f0,180(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmr f28,f30
	ctx.f28.f64 = ctx.f30.f64;
	// stfs f26,128(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// fadds f13,f27,f7
	ctx.f13.f64 = double(float(ctx.f27.f64 + ctx.f7.f64));
	// stfs f10,132(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 132, temp.u32);
	// stfs f2,136(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// stfs f11,140(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 140, temp.u32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stfs f5,144(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// addi r10,r10,12
	ctx.r10.s64 = ctx.r10.s64 + 12;
	// stfs f4,148(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// addi r11,r11,36
	ctx.r11.s64 = ctx.r11.s64 + 36;
	// stfs f9,92(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// stfs f13,96(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f7,f28,f6
	ctx.f7.f64 = double(float(ctx.f28.f64 + ctx.f6.f64));
	// stfs f7,100(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f6,f1,f3
	ctx.f6.f64 = double(float(ctx.f1.f64 + ctx.f3.f64));
	// stfs f6,80(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fadds f3,f8,f31
	ctx.f3.f64 = double(float(ctx.f8.f64 + ctx.f31.f64));
	// stfs f3,84(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fadds f1,f30,f29
	ctx.f1.f64 = double(float(ctx.f30.f64 + ctx.f29.f64));
	// stfs f1,88(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// bl 0x83137248
	ctx.lr = 0x830C9624;
	sub_83137248(ctx, base);
	// lfs f0,104(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f0,f0
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f0.f64));
	// li r11,1
	ctx.r11.s64 = 1;
	// fcmpu cr6,f1,f0
	ctx.cr6.compare(ctx.f1.f64, ctx.f0.f64);
	// blt cr6,0x830c963c
	if (ctx.cr6.lt) goto loc_830C963C;
	// li r11,0
	ctx.r11.s64 = 0;
loc_830C963C:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6b00
	ctx.lr = 0x830C964C;
	__restfpr_15(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C9658"))) PPC_WEAK_FUNC(sub_830C9658);
PPC_FUNC_IMPL(__imp__sub_830C9658) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// addi r12,r1,-16
	ctx.r12.s64 = ctx.r1.s64 + -16;
	// bl 0x82cb6abc
	ctx.lr = 0x830C966C;
	__savefpr_17(ctx, base);
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// lwz r10,264(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x830c9888
	if (ctx.cr6.eq) goto loc_830C9888;
	// lwz r9,280(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 280);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmplw cr6,r9,r8
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, ctx.xer);
	// beq cr6,0x830c9888
	if (ctx.cr6.eq) goto loc_830C9888;
	// lfs f12,252(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 252);
	ctx.f12.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f11,112(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 112);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f12
	ctx.f10.f64 = ctx.f12.f64;
	// fmuls f9,f11,f12
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f6,248(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 248);
	ctx.f6.f64 = double(temp.f32);
	// lfs f4,120(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 120);
	ctx.f4.f64 = double(temp.f32);
	// fmr f29,f6
	ctx.f29.f64 = ctx.f6.f64;
	// lfs f2,124(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 124);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f4,f6
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f6.f64));
	// lfs f7,244(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 244);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f30,f2,f12
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// fmuls f5,f11,f7
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// lfs f8,256(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 256);
	ctx.f8.f64 = double(temp.f32);
	// lfs f31,116(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 116);
	ctx.f31.f64 = double(temp.f32);
	// fmr f3,f7
	ctx.f3.f64 = ctx.f7.f64;
	// lfs f27,128(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 128);
	ctx.f27.f64 = double(temp.f32);
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f28,136(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 136);
	ctx.f28.f64 = double(temp.f32);
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// lfs f13,6380(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 6380);
	ctx.f13.f64 = double(temp.f32);
	// addi r9,r11,112
	ctx.r9.s64 = ctx.r11.s64 + 112;
	// lfs f26,132(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 132);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f25,f28,f10
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f10.f64));
	// fmadds f9,f31,f8,f9
	ctx.f9.f64 = double(float(ctx.f31.f64 * ctx.f8.f64 + ctx.f9.f64));
	// lfs f22,260(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 260);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f19,f27,f29
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// lfs f0,7676(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f1,f11,f8,f1
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f8.f64 + ctx.f1.f64));
	// lfs f21,264(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 264);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f30,f4,f8,f30
	ctx.f30.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f30.f64));
	// lfs f20,268(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fmsubs f5,f2,f8,f5
	ctx.f5.f64 = double(float(ctx.f2.f64 * ctx.f8.f64 - ctx.f5.f64));
	// addi r9,r10,244
	ctx.r9.s64 = ctx.r10.s64 + 244;
	// fmsubs f24,f8,f8,f13
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f8.f64 - ctx.f13.f64));
	// lfs f13,6140(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 6140);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f23,f28,f3
	ctx.f23.f64 = double(float(ctx.f28.f64 * ctx.f3.f64));
	// addi r8,r11,12
	ctx.r8.s64 = ctx.r11.s64 + 12;
	// fmuls f18,f26,f10
	ctx.f18.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// fmadds f25,f27,f3,f25
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f3.f64 + ctx.f25.f64));
	// fmadds f9,f2,f6,f9
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f6.f64 + ctx.f9.f64));
	// fmadds f2,f2,f7,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f7.f64 + ctx.f1.f64));
	// fmadds f1,f31,f7,f30
	ctx.f1.f64 = double(float(ctx.f31.f64 * ctx.f7.f64 + ctx.f30.f64));
	// fnmsubs f5,f31,f6,f5
	ctx.f5.f64 = double(float(-(ctx.f31.f64 * ctx.f6.f64 - ctx.f5.f64)));
	// fmsubs f30,f26,f3,f19
	ctx.f30.f64 = double(float(ctx.f26.f64 * ctx.f3.f64 - ctx.f19.f64));
	// fmsubs f23,f27,f10,f23
	ctx.f23.f64 = double(float(ctx.f27.f64 * ctx.f10.f64 - ctx.f23.f64));
	// fmuls f19,f28,f24
	ctx.f19.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// fmsubs f28,f28,f29,f18
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f29.f64 - ctx.f18.f64));
	// fmuls f17,f26,f24
	ctx.f17.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// fnmsubs f9,f4,f7,f9
	ctx.f9.f64 = double(float(-(ctx.f4.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// fmadds f26,f26,f29,f25
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f25.f64));
	// fnmsubs f2,f31,f12,f2
	ctx.f2.f64 = double(float(-(ctx.f31.f64 * ctx.f12.f64 - ctx.f2.f64)));
	// fnmsubs f1,f11,f6,f1
	ctx.f1.f64 = double(float(-(ctx.f11.f64 * ctx.f6.f64 - ctx.f1.f64)));
	// fnmsubs f5,f4,f12,f5
	ctx.f5.f64 = double(float(-(ctx.f4.f64 * ctx.f12.f64 - ctx.f5.f64)));
	// fmuls f12,f30,f8
	ctx.f12.f64 = double(float(ctx.f30.f64 * ctx.f8.f64));
	// fmuls f4,f23,f8
	ctx.f4.f64 = double(float(ctx.f23.f64 * ctx.f8.f64));
	// fmuls f7,f27,f24
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// fmuls f11,f28,f8
	ctx.f11.f64 = double(float(ctx.f28.f64 * ctx.f8.f64));
	// fmuls f6,f9,f9
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f9.f64));
	// fmuls f8,f26,f29
	ctx.f8.f64 = double(float(ctx.f26.f64 * ctx.f29.f64));
	// fmuls f10,f26,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// fmuls f31,f9,f2
	ctx.f31.f64 = double(float(ctx.f9.f64 * ctx.f2.f64));
	// fmuls f30,f1,f1
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f1.f64));
	// fadds f12,f19,f12
	ctx.f12.f64 = double(float(ctx.f19.f64 + ctx.f12.f64));
	// fadds f4,f17,f4
	ctx.f4.f64 = double(float(ctx.f17.f64 + ctx.f4.f64));
	// fmuls f29,f5,f1
	ctx.f29.f64 = double(float(ctx.f5.f64 * ctx.f1.f64));
	// fadds f11,f7,f11
	ctx.f11.f64 = double(float(ctx.f7.f64 + ctx.f11.f64));
	// fmuls f3,f26,f3
	ctx.f3.f64 = double(float(ctx.f26.f64 * ctx.f3.f64));
	// fmuls f7,f6,f0
	ctx.f7.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fadds f12,f12,f10
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f10.f64));
	// fadds f6,f4,f8
	ctx.f6.f64 = double(float(ctx.f4.f64 + ctx.f8.f64));
	// fmuls f4,f31,f0
	ctx.f4.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fmuls f8,f30,f0
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmuls f31,f29,f0
	ctx.f31.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// fsubs f10,f13,f7
	ctx.f10.f64 = double(float(ctx.f13.f64 - ctx.f7.f64));
	// fsubs f30,f4,f31
	ctx.f30.f64 = double(float(ctx.f4.f64 - ctx.f31.f64));
	// stfs f30,100(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fsubs f10,f10,f8
	ctx.f10.f64 = double(float(ctx.f10.f64 - ctx.f8.f64));
	// stfs f10,96(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fmuls f10,f1,f2
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f2.f64));
	// fmuls f30,f5,f9
	ctx.f30.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// fmuls f9,f1,f9
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f9.f64));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// fmuls f5,f5,f2
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f2.f64));
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// fmuls f29,f2,f2
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f2.f64));
	// li r7,9
	ctx.r7.s64 = 9;
	// fadds f3,f11,f3
	ctx.f3.f64 = double(float(ctx.f11.f64 + ctx.f3.f64));
	// fmuls f2,f12,f0
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fadds f12,f31,f4
	ctx.f12.f64 = double(float(ctx.f31.f64 + ctx.f4.f64));
	// stfs f12,108(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fmuls f1,f6,f0
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f11,f10,f0
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f10,f30,f0
	ctx.f10.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmuls f4,f9,f0
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f12,f5,f0
	ctx.f12.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fnmsubs f6,f29,f0,f13
	ctx.f6.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f9,f3,f0
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fadds f13,f20,f2
	ctx.f13.f64 = double(float(ctx.f20.f64 + ctx.f2.f64));
	// fadds f0,f21,f1
	ctx.f0.f64 = double(float(ctx.f21.f64 + ctx.f1.f64));
	// fadds f5,f10,f11
	ctx.f5.f64 = double(float(ctx.f10.f64 + ctx.f11.f64));
	// stfs f5,104(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fsubs f2,f11,f10
	ctx.f2.f64 = double(float(ctx.f11.f64 - ctx.f10.f64));
	// stfs f2,120(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// fsubs f1,f4,f12
	ctx.f1.f64 = double(float(ctx.f4.f64 - ctx.f12.f64));
	// stfs f1,116(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fadds f12,f12,f4
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f4.f64));
	// stfs f12,124(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// fsubs f3,f6,f8
	ctx.f3.f64 = double(float(ctx.f6.f64 - ctx.f8.f64));
	// stfs f3,112(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fsubs f11,f6,f7
	ctx.f11.f64 = double(float(ctx.f6.f64 - ctx.f7.f64));
	// stfs f11,128(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// fadds f12,f22,f9
	ctx.f12.f64 = double(float(ctx.f22.f64 + ctx.f9.f64));
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
loc_830C985C:
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bdnz 0x830c985c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C985C;
	// stfs f12,36(r8)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r8.u32 + 36, temp.u32);
	// stfs f0,40(r8)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 40, temp.u32);
	// stfs f13,44(r8)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r8.u32 + 44, temp.u32);
	// lwz r10,264(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	// lwz r9,280(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 280);
	// stw r9,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r9.u32);
loc_830C9888:
	// lfs f0,340(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 340);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r11,12
	ctx.r10.s64 = ctx.r11.s64 + 12;
	// lfs f13,16(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lfs f12,28(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f10,40(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f8,f10,f0
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f6,52(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 52);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,56(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 56);
	ctx.f5.f64 = double(temp.f32);
	// fmr f3,f6
	ctx.f3.f64 = ctx.f6.f64;
	// fmr f2,f5
	ctx.f2.f64 = ctx.f5.f64;
	// lfs f7,48(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 48);
	ctx.f7.f64 = double(temp.f32);
	// fmr f4,f7
	ctx.f4.f64 = ctx.f7.f64;
	// lfs f1,336(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 336);
	ctx.f1.f64 = double(temp.f32);
	// stfs f1,120(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// addi r10,r10,36
	ctx.r10.s64 = ctx.r10.s64 + 36;
	// fneg f0,f11
	ctx.f0.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// fneg f13,f9
	ctx.f13.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// fneg f12,f8
	ctx.f12.u64 = ctx.f8.u64 ^ 0x8000000000000000;
	// fadds f10,f3,f9
	ctx.f10.f64 = double(float(ctx.f3.f64 + ctx.f9.f64));
	// stfs f10,112(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fadds f9,f2,f8
	ctx.f9.f64 = double(float(ctx.f2.f64 + ctx.f8.f64));
	// stfs f9,116(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fadds f11,f4,f11
	ctx.f11.f64 = double(float(ctx.f4.f64 + ctx.f11.f64));
	// stfs f11,108(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fadds f8,f7,f0
	ctx.f8.f64 = double(float(ctx.f7.f64 + ctx.f0.f64));
	// stfs f8,96(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f7,f6,f13
	ctx.f7.f64 = double(float(ctx.f6.f64 + ctx.f13.f64));
	// stfs f7,100(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fadds f6,f5,f12
	ctx.f6.f64 = double(float(ctx.f5.f64 + ctx.f12.f64));
	// stfs f6,104(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// bl 0x831373b8
	ctx.lr = 0x830C991C;
	sub_831373B8(ctx, base);
	// lfs f5,24(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f4.f64 = double(temp.f32);
	// li r11,1
	ctx.r11.s64 = 1;
	// fadds f3,f5,f4
	ctx.f3.f64 = double(float(ctx.f5.f64 + ctx.f4.f64));
	// fmuls f2,f3,f3
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// fcmpu cr6,f1,f2
	ctx.cr6.compare(ctx.f1.f64, ctx.f2.f64);
	// blt cr6,0x830c993c
	if (ctx.cr6.lt) goto loc_830C993C;
	// li r11,0
	ctx.r11.s64 = 0;
loc_830C993C:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// addi r12,r1,-16
	ctx.r12.s64 = ctx.r1.s64 + -16;
	// bl 0x82cb6b08
	ctx.lr = 0x830C994C;
	__restfpr_17(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C995C"))) PPC_WEAK_FUNC(sub_830C995C);
PPC_FUNC_IMPL(__imp__sub_830C995C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C9960"))) PPC_WEAK_FUNC(sub_830C9960);
PPC_FUNC_IMPL(__imp__sub_830C9960) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6ac0
	ctx.lr = 0x830C9970;
	__savefpr_18(ctx, base);
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c9b80
	if (ctx.cr6.eq) goto loc_830C9B80;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c9b80
	if (ctx.cr6.eq) goto loc_830C9B80;
	// lfs f12,252(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f12.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f11,112(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 112);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f12
	ctx.f10.f64 = ctx.f12.f64;
	// fmuls f9,f11,f12
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f7,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,124(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 124);
	ctx.f6.f64 = double(temp.f32);
	// fmr f30,f7
	ctx.f30.f64 = ctx.f7.f64;
	// lfs f1,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f3,f6,f7
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// fmuls f2,f6,f12
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// lfs f8,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f8.f64 = double(temp.f32);
	// fmr f29,f1
	ctx.f29.f64 = ctx.f1.f64;
	// lfs f4,116(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 116);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f11,f7
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// lfs f31,120(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 120);
	ctx.f31.f64 = double(temp.f32);
	// lfs f13,6380(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6380);
	ctx.f13.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f27,136(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	ctx.f27.f64 = double(temp.f32);
	// fmsubs f24,f8,f8,f13
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f8.f64 - ctx.f13.f64));
	// lfs f26,128(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 128);
	ctx.f26.f64 = double(temp.f32);
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f28,132(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 132);
	ctx.f28.f64 = double(temp.f32);
	// addi r10,r3,112
	ctx.r10.s64 = ctx.r3.s64 + 112;
	// fmadds f9,f4,f8,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f9.f64));
	// lfs f23,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f21,f27,f30
	ctx.f21.f64 = double(float(ctx.f27.f64 * ctx.f30.f64));
	// lfs f0,7676(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f3,f11,f8,f3
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f8.f64 + ctx.f3.f64));
	// lfs f22,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f2,f31,f8,f2
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f8.f64 + ctx.f2.f64));
	// lfs f13,6140(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 6140);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f19,f26,f29
	ctx.f19.f64 = double(float(ctx.f26.f64 * ctx.f29.f64));
	// lfs f20,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fmsubs f5,f6,f8,f5
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f8.f64 - ctx.f5.f64));
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// fmuls f18,f28,f29
	ctx.f18.f64 = double(float(ctx.f28.f64 * ctx.f29.f64));
	// addi r9,r3,12
	ctx.r9.s64 = ctx.r3.s64 + 12;
	// fmuls f25,f28,f10
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f10.f64));
	// fmadds f9,f6,f1,f9
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f1.f64 + ctx.f9.f64));
	// fmuls f6,f28,f24
	ctx.f6.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// fmadds f3,f31,f1,f3
	ctx.f3.f64 = double(float(ctx.f31.f64 * ctx.f1.f64 + ctx.f3.f64));
	// fmadds f2,f4,f7,f2
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f7.f64 + ctx.f2.f64));
	// fmsubs f21,f26,f10,f21
	ctx.f21.f64 = double(float(ctx.f26.f64 * ctx.f10.f64 - ctx.f21.f64));
	// fnmsubs f5,f4,f1,f5
	ctx.f5.f64 = double(float(-(ctx.f4.f64 * ctx.f1.f64 - ctx.f5.f64)));
	// fmsubs f28,f28,f30,f19
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f30.f64 - ctx.f19.f64));
	// fmadds f19,f27,f10,f18
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f10.f64 + ctx.f18.f64));
	// fmsubs f25,f27,f29,f25
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f29.f64 - ctx.f25.f64));
	// fmuls f27,f27,f24
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// fnmsubs f9,f31,f7,f9
	ctx.f9.f64 = double(float(-(ctx.f31.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// fmuls f24,f26,f24
	ctx.f24.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// fnmsubs f4,f4,f12,f3
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f12.f64 - ctx.f3.f64)));
	// fnmsubs f3,f11,f1,f2
	ctx.f3.f64 = double(float(-(ctx.f11.f64 * ctx.f1.f64 - ctx.f2.f64)));
	// fmuls f2,f21,f8
	ctx.f2.f64 = double(float(ctx.f21.f64 * ctx.f8.f64));
	// fnmsubs f5,f31,f12,f5
	ctx.f5.f64 = double(float(-(ctx.f31.f64 * ctx.f12.f64 - ctx.f5.f64)));
	// fmuls f1,f28,f8
	ctx.f1.f64 = double(float(ctx.f28.f64 * ctx.f8.f64));
	// fmadds f12,f26,f30,f19
	ctx.f12.f64 = double(float(ctx.f26.f64 * ctx.f30.f64 + ctx.f19.f64));
	// fmuls f7,f25,f8
	ctx.f7.f64 = double(float(ctx.f25.f64 * ctx.f8.f64));
	// fmuls f11,f9,f9
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f9.f64));
	// fmuls f31,f3,f3
	ctx.f31.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// fadds f6,f6,f2
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f2.f64));
	// fmuls f28,f5,f3
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fadds f1,f27,f1
	ctx.f1.f64 = double(float(ctx.f27.f64 + ctx.f1.f64));
	// fmuls f2,f29,f12
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f12.f64));
	// fmuls f10,f10,f12
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
	// fadds f8,f24,f7
	ctx.f8.f64 = double(float(ctx.f24.f64 + ctx.f7.f64));
	// fmuls f7,f9,f4
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f12,f12,f30
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f30.f64));
	// fmuls f31,f31,f0
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fmuls f30,f28,f0
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// fadds f6,f6,f2
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f2.f64));
	// fadds f2,f1,f10
	ctx.f2.f64 = double(float(ctx.f1.f64 + ctx.f10.f64));
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// fsubs f1,f13,f11
	ctx.f1.f64 = double(float(ctx.f13.f64 - ctx.f11.f64));
	// fsubs f10,f7,f30
	ctx.f10.f64 = double(float(ctx.f7.f64 - ctx.f30.f64));
	// stfs f10,-156(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -156, temp.u32);
	// fsubs f1,f1,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 - ctx.f31.f64));
	// stfs f1,-160(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -160, temp.u32);
	// fmuls f10,f3,f4
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f4.f64));
	// fmuls f1,f5,f9
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// fmuls f9,f3,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f9.f64));
	// addi r11,r1,-160
	ctx.r11.s64 = ctx.r1.s64 + -160;
	// fmuls f5,f5,f4
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f29,f4,f4
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f4.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fadds f4,f8,f12
	ctx.f4.f64 = double(float(ctx.f8.f64 + ctx.f12.f64));
	// fmuls f3,f2,f0
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmuls f2,f6,f0
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fadds f12,f30,f7
	ctx.f12.f64 = double(float(ctx.f30.f64 + ctx.f7.f64));
	// stfs f12,-148(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -148, temp.u32);
	// fmuls f8,f1,f0
	ctx.f8.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f6,f9,f0
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fnmsubs f7,f29,f0,f13
	ctx.f7.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fadds f13,f20,f3
	ctx.f13.f64 = double(float(ctx.f20.f64 + ctx.f3.f64));
	// fadds f0,f22,f2
	ctx.f0.f64 = double(float(ctx.f22.f64 + ctx.f2.f64));
	// fadds f3,f8,f10
	ctx.f3.f64 = double(float(ctx.f8.f64 + ctx.f10.f64));
	// stfs f3,-152(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + -152, temp.u32);
	// fsubs f1,f10,f8
	ctx.f1.f64 = double(float(ctx.f10.f64 - ctx.f8.f64));
	// stfs f1,-136(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -136, temp.u32);
	// fsubs f12,f6,f5
	ctx.f12.f64 = double(float(ctx.f6.f64 - ctx.f5.f64));
	// stfs f12,-140(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -140, temp.u32);
	// fsubs f2,f7,f31
	ctx.f2.f64 = double(float(ctx.f7.f64 - ctx.f31.f64));
	// stfs f2,-144(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + -144, temp.u32);
	// fadds f10,f5,f6
	ctx.f10.f64 = double(float(ctx.f5.f64 + ctx.f6.f64));
	// stfs f10,-132(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -132, temp.u32);
	// fsubs f9,f7,f11
	ctx.f9.f64 = double(float(ctx.f7.f64 - ctx.f11.f64));
	// stfs f9,-128(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -128, temp.u32);
	// fadds f12,f23,f4
	ctx.f12.f64 = double(float(ctx.f23.f64 + ctx.f4.f64));
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830C9B54:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c9b54
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C9B54;
	// stfs f12,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f0,40(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f13,44(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
loc_830C9B80:
	// lfs f0,340(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r3,12
	ctx.r11.s64 = ctx.r3.s64 + 12;
	// lfs f12,28(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	ctx.f12.f64 = double(temp.f32);
	// addi r10,r4,12
	ctx.r10.s64 = ctx.r4.s64 + 12;
	// lfs f13,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f10,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fmuls f8,f10,f0
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f6,48(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	ctx.f6.f64 = double(temp.f32);
	// fmr f1,f6
	ctx.f1.f64 = ctx.f6.f64;
	// lfs f4,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,56(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	ctx.f2.f64 = double(temp.f32);
	// fmr f0,f4
	ctx.f0.f64 = ctx.f4.f64;
	// lfs f7,336(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	ctx.f7.f64 = double(temp.f32);
	// fmr f13,f2
	ctx.f13.f64 = ctx.f2.f64;
	// fmr f3,f7
	ctx.f3.f64 = ctx.f7.f64;
	// addi r11,r11,36
	ctx.r11.s64 = ctx.r11.s64 + 36;
	// fmr f5,f7
	ctx.f5.f64 = ctx.f7.f64;
	// fneg f10,f9
	ctx.f10.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// fneg f12,f11
	ctx.f12.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// fneg f31,f8
	ctx.f31.u64 = ctx.f8.u64 ^ 0x8000000000000000;
	// fadds f1,f1,f11
	ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f11.f64));
	// fadds f0,f0,f9
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f9.f64));
	// fadds f13,f13,f8
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f8.f64));
	// fadds f11,f4,f10
	ctx.f11.f64 = double(float(ctx.f4.f64 + ctx.f10.f64));
	// fadds f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 + ctx.f12.f64));
	// fadds f10,f2,f31
	ctx.f10.f64 = double(float(ctx.f2.f64 + ctx.f31.f64));
	// fsubs f8,f11,f3
	ctx.f8.f64 = double(float(ctx.f11.f64 - ctx.f3.f64));
	// stfs f8,4(r4)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// fsubs f9,f12,f5
	ctx.f9.f64 = double(float(ctx.f12.f64 - ctx.f5.f64));
	// stfs f9,0(r4)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fsubs f6,f10,f7
	ctx.f6.f64 = double(float(ctx.f10.f64 - ctx.f7.f64));
	// stfs f6,8(r4)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// fadds f4,f3,f11
	ctx.f4.f64 = double(float(ctx.f3.f64 + ctx.f11.f64));
	// stfs f4,16(r4)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// fadds f3,f7,f10
	ctx.f3.f64 = double(float(ctx.f7.f64 + ctx.f10.f64));
	// stfs f3,20(r4)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// fadds f5,f5,f12
	ctx.f5.f64 = double(float(ctx.f5.f64 + ctx.f12.f64));
	// stfs f5,12(r4)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// lfs f2,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// lfs f12,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,336(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	ctx.f10.f64 = double(temp.f32);
	// fmr f9,f10
	ctx.f9.f64 = ctx.f10.f64;
	// fmr f8,f10
	ctx.f8.f64 = ctx.f10.f64;
	// fsubs f7,f1,f9
	ctx.f7.f64 = double(float(ctx.f1.f64 - ctx.f9.f64));
	// fsubs f4,f13,f10
	ctx.f4.f64 = double(float(ctx.f13.f64 - ctx.f10.f64));
	// fadds f6,f1,f9
	ctx.f6.f64 = double(float(ctx.f1.f64 + ctx.f9.f64));
	// fsubs f5,f0,f8
	ctx.f5.f64 = double(float(ctx.f0.f64 - ctx.f8.f64));
	// fadds f1,f0,f8
	ctx.f1.f64 = double(float(ctx.f0.f64 + ctx.f8.f64));
	// fadds f0,f13,f10
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f10.f64));
	// fsubs f3,f2,f7
	ctx.f3.f64 = double(float(ctx.f2.f64 - ctx.f7.f64));
	// fsubs f10,f11,f4
	ctx.f10.f64 = double(float(ctx.f11.f64 - ctx.f4.f64));
	// fsubs f13,f12,f5
	ctx.f13.f64 = double(float(ctx.f12.f64 - ctx.f5.f64));
	// fsel f9,f3,f7,f2
	ctx.f9.f64 = ctx.f3.f64 >= 0.0 ? ctx.f7.f64 : ctx.f2.f64;
	// stfs f9,0(r4)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fsel f7,f10,f4,f11
	ctx.f7.f64 = ctx.f10.f64 >= 0.0 ? ctx.f4.f64 : ctx.f11.f64;
	// stfs f7,8(r4)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// fsel f8,f13,f5,f12
	ctx.f8.f64 = ctx.f13.f64 >= 0.0 ? ctx.f5.f64 : ctx.f12.f64;
	// stfs f8,4(r4)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// lfs f4,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f3.f64 = double(temp.f32);
	// lfs f5,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f5.f64 = double(temp.f32);
	// fsubs f12,f5,f0
	ctx.f12.f64 = double(float(ctx.f5.f64 - ctx.f0.f64));
	// fsubs f2,f3,f1
	ctx.f2.f64 = double(float(ctx.f3.f64 - ctx.f1.f64));
	// fsubs f13,f4,f6
	ctx.f13.f64 = double(float(ctx.f4.f64 - ctx.f6.f64));
	// fsel f9,f12,f5,f0
	ctx.f9.f64 = ctx.f12.f64 >= 0.0 ? ctx.f5.f64 : ctx.f0.f64;
	// stfs f9,20(r4)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// fsel f11,f2,f3,f1
	ctx.f11.f64 = ctx.f2.f64 >= 0.0 ? ctx.f3.f64 : ctx.f1.f64;
	// stfs f11,16(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// fsel f10,f13,f4,f6
	ctx.f10.f64 = ctx.f13.f64 >= 0.0 ? ctx.f4.f64 : ctx.f6.f64;
	// stfs f10,12(r4)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6b0c
	ctx.lr = 0x830C9CAC;
	__restfpr_18(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C9CB8"))) PPC_WEAK_FUNC(sub_830C9CB8);
PPC_FUNC_IMPL(__imp__sub_830C9CB8) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C9CBC"))) PPC_WEAK_FUNC(sub_830C9CBC);
PPC_FUNC_IMPL(__imp__sub_830C9CBC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C9CC0"))) PPC_WEAK_FUNC(sub_830C9CC0);
PPC_FUNC_IMPL(__imp__sub_830C9CC0) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C9CC4"))) PPC_WEAK_FUNC(sub_830C9CC4);
PPC_FUNC_IMPL(__imp__sub_830C9CC4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C9CC8"))) PPC_WEAK_FUNC(sub_830C9CC8);
PPC_FUNC_IMPL(__imp__sub_830C9CC8) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C9CCC"))) PPC_WEAK_FUNC(sub_830C9CCC);
PPC_FUNC_IMPL(__imp__sub_830C9CCC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C9CD0"))) PPC_WEAK_FUNC(sub_830C9CD0);
PPC_FUNC_IMPL(__imp__sub_830C9CD0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f1,336(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	ctx.f1.f64 = double(temp.f32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C9CD8"))) PPC_WEAK_FUNC(sub_830C9CD8);
PPC_FUNC_IMPL(__imp__sub_830C9CD8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f13,340(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,7676(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f1,f13,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C9CEC"))) PPC_WEAK_FUNC(sub_830C9CEC);
PPC_FUNC_IMPL(__imp__sub_830C9CEC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C9CF0"))) PPC_WEAK_FUNC(sub_830C9CF0);
PPC_FUNC_IMPL(__imp__sub_830C9CF0) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,344(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 344);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C9CF8"))) PPC_WEAK_FUNC(sub_830C9CF8);
PPC_FUNC_IMPL(__imp__sub_830C9CF8) {
	PPC_FUNC_PROLOGUE();
	// stw r4,344(r3)
	PPC_STORE_U32(ctx.r3.u32 + 344, ctx.r4.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C9D00"))) PPC_WEAK_FUNC(sub_830C9D00);
PPC_FUNC_IMPL(__imp__sub_830C9D00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f1,336(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	ctx.f1.f64 = double(temp.f32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C9D08"))) PPC_WEAK_FUNC(sub_830C9D08);
PPC_FUNC_IMPL(__imp__sub_830C9D08) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f13,340(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,7676(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f1,f13,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C9D1C"))) PPC_WEAK_FUNC(sub_830C9D1C);
PPC_FUNC_IMPL(__imp__sub_830C9D1C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830C9D20"))) PPC_WEAK_FUNC(sub_830C9D20);
PPC_FUNC_IMPL(__imp__sub_830C9D20) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,344(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 344);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830C9D28"))) PPC_WEAK_FUNC(sub_830C9D28);
PPC_FUNC_IMPL(__imp__sub_830C9D28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6abc
	ctx.lr = 0x830C9D38;
	__savefpr_17(ctx, base);
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830c9f48
	if (ctx.cr6.eq) goto loc_830C9F48;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830c9f48
	if (ctx.cr6.eq) goto loc_830C9F48;
	// lfs f12,252(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f12.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f11,112(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 112);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f12
	ctx.f10.f64 = ctx.f12.f64;
	// fmuls f9,f11,f12
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f7,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f5,f11,f7
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// lfs f4,120(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 120);
	ctx.f4.f64 = double(temp.f32);
	// fmr f3,f7
	ctx.f3.f64 = ctx.f7.f64;
	// lfs f2,124(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 124);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f4,f6
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f6.f64));
	// fmuls f30,f2,f12
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// lfs f8,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f8.f64 = double(temp.f32);
	// lfs f31,116(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 116);
	ctx.f31.f64 = double(temp.f32);
	// fmr f29,f6
	ctx.f29.f64 = ctx.f6.f64;
	// lfs f28,136(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	ctx.f28.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f27,128(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 128);
	ctx.f27.f64 = double(temp.f32);
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f13,6380(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6380);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r3,112
	ctx.r10.s64 = ctx.r3.s64 + 112;
	// lfs f26,132(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 132);
	ctx.f26.f64 = double(temp.f32);
	// fmsubs f24,f8,f8,f13
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f8.f64 - ctx.f13.f64));
	// fmadds f9,f31,f8,f9
	ctx.f9.f64 = double(float(ctx.f31.f64 * ctx.f8.f64 + ctx.f9.f64));
	// lfs f22,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f22.f64 = double(temp.f32);
	// fmsubs f5,f2,f8,f5
	ctx.f5.f64 = double(float(ctx.f2.f64 * ctx.f8.f64 - ctx.f5.f64));
	// lfs f0,7676(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f23,f28,f3
	ctx.f23.f64 = double(float(ctx.f28.f64 * ctx.f3.f64));
	// lfs f13,6140(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 6140);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f1,f11,f8,f1
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f8.f64 + ctx.f1.f64));
	// lfs f21,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f30,f4,f8,f30
	ctx.f30.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f30.f64));
	// lfs f20,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f19,f27,f29
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// fmuls f25,f26,f10
	ctx.f25.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// addi r9,r3,12
	ctx.r9.s64 = ctx.r3.s64 + 12;
	// fmuls f18,f27,f3
	ctx.f18.f64 = double(float(ctx.f27.f64 * ctx.f3.f64));
	// fmuls f17,f26,f24
	ctx.f17.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// fmadds f9,f2,f6,f9
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f6.f64 + ctx.f9.f64));
	// fnmsubs f5,f31,f6,f5
	ctx.f5.f64 = double(float(-(ctx.f31.f64 * ctx.f6.f64 - ctx.f5.f64)));
	// fmsubs f23,f27,f10,f23
	ctx.f23.f64 = double(float(ctx.f27.f64 * ctx.f10.f64 - ctx.f23.f64));
	// fmadds f2,f2,f7,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f7.f64 + ctx.f1.f64));
	// fmadds f1,f31,f7,f30
	ctx.f1.f64 = double(float(ctx.f31.f64 * ctx.f7.f64 + ctx.f30.f64));
	// fmsubs f30,f26,f3,f19
	ctx.f30.f64 = double(float(ctx.f26.f64 * ctx.f3.f64 - ctx.f19.f64));
	// fmsubs f25,f28,f29,f25
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f29.f64 - ctx.f25.f64));
	// fmadds f26,f26,f29,f18
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f18.f64));
	// fmuls f19,f28,f24
	ctx.f19.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// fmuls f27,f27,f24
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// fnmsubs f9,f4,f7,f9
	ctx.f9.f64 = double(float(-(ctx.f4.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// fnmsubs f5,f4,f12,f5
	ctx.f5.f64 = double(float(-(ctx.f4.f64 * ctx.f12.f64 - ctx.f5.f64)));
	// fmuls f4,f23,f8
	ctx.f4.f64 = double(float(ctx.f23.f64 * ctx.f8.f64));
	// fnmsubs f2,f31,f12,f2
	ctx.f2.f64 = double(float(-(ctx.f31.f64 * ctx.f12.f64 - ctx.f2.f64)));
	// fnmsubs f1,f11,f6,f1
	ctx.f1.f64 = double(float(-(ctx.f11.f64 * ctx.f6.f64 - ctx.f1.f64)));
	// fmuls f12,f30,f8
	ctx.f12.f64 = double(float(ctx.f30.f64 * ctx.f8.f64));
	// fmuls f7,f25,f8
	ctx.f7.f64 = double(float(ctx.f25.f64 * ctx.f8.f64));
	// fmadds f11,f28,f10,f26
	ctx.f11.f64 = double(float(ctx.f28.f64 * ctx.f10.f64 + ctx.f26.f64));
	// fmuls f8,f9,f9
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f9.f64));
	// fadds f6,f17,f4
	ctx.f6.f64 = double(float(ctx.f17.f64 + ctx.f4.f64));
	// fmuls f4,f9,f2
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f2.f64));
	// fmuls f31,f1,f1
	ctx.f31.f64 = double(float(ctx.f1.f64 * ctx.f1.f64));
	// fadds f12,f19,f12
	ctx.f12.f64 = double(float(ctx.f19.f64 + ctx.f12.f64));
	// fmuls f30,f5,f1
	ctx.f30.f64 = double(float(ctx.f5.f64 * ctx.f1.f64));
	// fmuls f10,f10,f11
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
	// fmuls f29,f29,f11
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f11.f64));
	// fmuls f3,f11,f3
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f3.f64));
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fadds f7,f27,f7
	ctx.f7.f64 = double(float(ctx.f27.f64 + ctx.f7.f64));
	// fmuls f11,f4,f0
	ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fmuls f4,f31,f0
	ctx.f4.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fmuls f31,f30,f0
	ctx.f31.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fadds f12,f12,f10
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f10.f64));
	// fadds f6,f6,f29
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f29.f64));
	// fsubs f10,f13,f8
	ctx.f10.f64 = double(float(ctx.f13.f64 - ctx.f8.f64));
	// fsubs f30,f11,f31
	ctx.f30.f64 = double(float(ctx.f11.f64 - ctx.f31.f64));
	// stfs f30,-172(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + -172, temp.u32);
	// fmuls f30,f5,f9
	ctx.f30.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// fsubs f10,f10,f4
	ctx.f10.f64 = double(float(ctx.f10.f64 - ctx.f4.f64));
	// stfs f10,-176(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -176, temp.u32);
	// fmuls f10,f1,f2
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f2.f64));
	// fmuls f9,f1,f9
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f9.f64));
	// addi r11,r1,-176
	ctx.r11.s64 = ctx.r1.s64 + -176;
	// fmuls f29,f2,f2
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f2.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f5,f5,f2
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f2.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fadds f3,f7,f3
	ctx.f3.f64 = double(float(ctx.f7.f64 + ctx.f3.f64));
	// fmuls f2,f12,f0
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fadds f12,f31,f11
	ctx.f12.f64 = double(float(ctx.f31.f64 + ctx.f11.f64));
	// stfs f12,-164(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -164, temp.u32);
	// fmuls f11,f10,f0
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f10,f30,f0
	ctx.f10.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmuls f1,f6,f0
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f6,f9,f0
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fnmsubs f7,f29,f0,f13
	ctx.f7.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fmuls f3,f3,f0
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fadds f13,f20,f2
	ctx.f13.f64 = double(float(ctx.f20.f64 + ctx.f2.f64));
	// fsubs f12,f11,f10
	ctx.f12.f64 = double(float(ctx.f11.f64 - ctx.f10.f64));
	// stfs f12,-152(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -152, temp.u32);
	// fadds f2,f10,f11
	ctx.f2.f64 = double(float(ctx.f10.f64 + ctx.f11.f64));
	// stfs f2,-168(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + -168, temp.u32);
	// fadds f0,f21,f1
	ctx.f0.f64 = double(float(ctx.f21.f64 + ctx.f1.f64));
	// fsubs f1,f7,f4
	ctx.f1.f64 = double(float(ctx.f7.f64 - ctx.f4.f64));
	// stfs f1,-160(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -160, temp.u32);
	// fsubs f11,f6,f5
	ctx.f11.f64 = double(float(ctx.f6.f64 - ctx.f5.f64));
	// stfs f11,-156(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -156, temp.u32);
	// fadds f10,f5,f6
	ctx.f10.f64 = double(float(ctx.f5.f64 + ctx.f6.f64));
	// stfs f10,-148(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -148, temp.u32);
	// fsubs f9,f7,f8
	ctx.f9.f64 = double(float(ctx.f7.f64 - ctx.f8.f64));
	// stfs f9,-144(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -144, temp.u32);
	// fadds f12,f22,f3
	ctx.f12.f64 = double(float(ctx.f22.f64 + ctx.f3.f64));
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830C9F1C:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830c9f1c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830C9F1C;
	// stfs f12,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f0,40(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f13,44(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
loc_830C9F48:
	// lfs f0,16(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r3,12
	ctx.r11.s64 = ctx.r3.s64 + 12;
	// stfs f0,12(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// fmr f11,f0
	ctx.f11.f64 = ctx.f0.f64;
	// addi r10,r4,12
	ctx.r10.s64 = ctx.r4.s64 + 12;
	// addi r11,r11,36
	ctx.r11.s64 = ctx.r11.s64 + 36;
	// lfs f13,28(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,16(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// lfs f12,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f12.f64 = double(temp.f32);
	// fmr f8,f12
	ctx.f8.f64 = ctx.f12.f64;
	// stfs f12,20(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// lfs f9,340(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f5,f11,f9
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f9.f64));
	// fmuls f6,f10,f9
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f9.f64));
	// stfs f6,16(r4)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// fmuls f7,f8,f9
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// stfs f7,20(r4)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// fmr f2,f5
	ctx.f2.f64 = ctx.f5.f64;
	// stfs f5,12(r4)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// fmr f4,f6
	ctx.f4.f64 = ctx.f6.f64;
	// fmr f3,f7
	ctx.f3.f64 = ctx.f7.f64;
	// fneg f1,f2
	ctx.f1.u64 = ctx.f2.u64 ^ 0x8000000000000000;
	// stfs f1,0(r4)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fneg f0,f4
	ctx.f0.u64 = ctx.f4.u64 ^ 0x8000000000000000;
	// stfs f0,4(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// fneg f13,f3
	ctx.f13.u64 = ctx.f3.u64 ^ 0x8000000000000000;
	// stfs f13,8(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// lfs f12,48(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f1,f12
	ctx.f11.f64 = double(float(ctx.f1.f64 + ctx.f12.f64));
	// stfs f11,0(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// lfs f10,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f10.f64 = double(temp.f32);
	// fadds f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// stfs f9,4(r4)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// lfs f8,56(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	ctx.f8.f64 = double(temp.f32);
	// fadds f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 + ctx.f13.f64));
	// stfs f7,8(r4)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// lfs f6,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,48(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f3.f64 = double(temp.f32);
	// fadds f2,f4,f3
	ctx.f2.f64 = double(float(ctx.f4.f64 + ctx.f3.f64));
	// stfs f2,12(r4)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// lfs f1,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f1.f64 = double(temp.f32);
	// fadds f0,f1,f6
	ctx.f0.f64 = double(float(ctx.f1.f64 + ctx.f6.f64));
	// stfs f0,16(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// lfs f13,56(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f5
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f5.f64));
	// stfs f12,20(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6b08
	ctx.lr = 0x830CA014;
	__restfpr_17(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA020"))) PPC_WEAK_FUNC(sub_830CA020);
PPC_FUNC_IMPL(__imp__sub_830CA020) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6abc
	ctx.lr = 0x830CA030;
	__savefpr_17(ctx, base);
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830ca240
	if (ctx.cr6.eq) goto loc_830CA240;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830ca240
	if (ctx.cr6.eq) goto loc_830CA240;
	// lfs f12,252(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f12.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f11,112(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 112);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f12
	ctx.f10.f64 = ctx.f12.f64;
	// fmuls f9,f11,f12
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f7,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,124(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 124);
	ctx.f6.f64 = double(temp.f32);
	// fmr f30,f7
	ctx.f30.f64 = ctx.f7.f64;
	// fmuls f3,f6,f7
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// lfs f1,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f2,f6,f12
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// lfs f8,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f8.f64 = double(temp.f32);
	// lfs f4,116(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 116);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f11,f7
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// fmr f29,f1
	ctx.f29.f64 = ctx.f1.f64;
	// lfs f31,120(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 120);
	ctx.f31.f64 = double(temp.f32);
	// lfs f28,136(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	ctx.f28.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f27,128(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 128);
	ctx.f27.f64 = double(temp.f32);
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f13,6380(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6380);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r3,112
	ctx.r10.s64 = ctx.r3.s64 + 112;
	// lfs f26,132(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 132);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f25,f28,f10
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f10.f64));
	// fmadds f9,f4,f8,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f9.f64));
	// lfs f23,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f21,f28,f30
	ctx.f21.f64 = double(float(ctx.f28.f64 * ctx.f30.f64));
	// lfs f0,7676(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f3,f11,f8,f3
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f8.f64 + ctx.f3.f64));
	// lfs f22,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f2,f31,f8,f2
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f8.f64 + ctx.f2.f64));
	// lfs f20,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fmsubs f5,f6,f8,f5
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f8.f64 - ctx.f5.f64));
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// fmsubs f24,f8,f8,f13
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f8.f64 - ctx.f13.f64));
	// lfs f13,6140(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 6140);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f19,f27,f29
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// addi r9,r3,12
	ctx.r9.s64 = ctx.r3.s64 + 12;
	// fmuls f18,f26,f10
	ctx.f18.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// fmadds f25,f27,f30,f25
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f30.f64 + ctx.f25.f64));
	// fmadds f9,f6,f1,f9
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f1.f64 + ctx.f9.f64));
	// fmsubs f21,f27,f10,f21
	ctx.f21.f64 = double(float(ctx.f27.f64 * ctx.f10.f64 - ctx.f21.f64));
	// fmadds f3,f31,f1,f3
	ctx.f3.f64 = double(float(ctx.f31.f64 * ctx.f1.f64 + ctx.f3.f64));
	// fmadds f2,f4,f7,f2
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f7.f64 + ctx.f2.f64));
	// fnmsubs f5,f4,f1,f5
	ctx.f5.f64 = double(float(-(ctx.f4.f64 * ctx.f1.f64 - ctx.f5.f64)));
	// fmuls f17,f28,f24
	ctx.f17.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// fmsubs f19,f26,f30,f19
	ctx.f19.f64 = double(float(ctx.f26.f64 * ctx.f30.f64 - ctx.f19.f64));
	// fmsubs f28,f28,f29,f18
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f29.f64 - ctx.f18.f64));
	// fmuls f6,f26,f24
	ctx.f6.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// fmadds f26,f26,f29,f25
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f25.f64));
	// fnmsubs f9,f31,f7,f9
	ctx.f9.f64 = double(float(-(ctx.f31.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// fmuls f7,f27,f24
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// fnmsubs f4,f4,f12,f3
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f12.f64 - ctx.f3.f64)));
	// fnmsubs f3,f11,f1,f2
	ctx.f3.f64 = double(float(-(ctx.f11.f64 * ctx.f1.f64 - ctx.f2.f64)));
	// fmuls f2,f21,f8
	ctx.f2.f64 = double(float(ctx.f21.f64 * ctx.f8.f64));
	// fnmsubs f5,f31,f12,f5
	ctx.f5.f64 = double(float(-(ctx.f31.f64 * ctx.f12.f64 - ctx.f5.f64)));
	// fmuls f1,f19,f8
	ctx.f1.f64 = double(float(ctx.f19.f64 * ctx.f8.f64));
	// fmuls f12,f28,f8
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f8.f64));
	// fmuls f31,f26,f30
	ctx.f31.f64 = double(float(ctx.f26.f64 * ctx.f30.f64));
	// fmuls f8,f9,f9
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f9.f64));
	// fmuls f11,f26,f29
	ctx.f11.f64 = double(float(ctx.f26.f64 * ctx.f29.f64));
	// fmuls f30,f9,f4
	ctx.f30.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// fmuls f29,f3,f3
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f3.f64));
	// fadds f6,f6,f2
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f2.f64));
	// fmuls f28,f5,f3
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fadds f2,f17,f1
	ctx.f2.f64 = double(float(ctx.f17.f64 + ctx.f1.f64));
	// fadds f1,f7,f12
	ctx.f1.f64 = double(float(ctx.f7.f64 + ctx.f12.f64));
	// fmuls f10,f26,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// fmuls f12,f8,f0
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmuls f8,f30,f0
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmuls f7,f29,f0
	ctx.f7.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// fadds f6,f6,f11
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f11.f64));
	// fmuls f30,f28,f0
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// fadds f2,f2,f10
	ctx.f2.f64 = double(float(ctx.f2.f64 + ctx.f10.f64));
	// fsubs f11,f13,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// fsubs f10,f8,f30
	ctx.f10.f64 = double(float(ctx.f8.f64 - ctx.f30.f64));
	// stfs f10,-172(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -172, temp.u32);
	// fmuls f10,f3,f4
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f4.f64));
	// fsubs f11,f11,f7
	ctx.f11.f64 = double(float(ctx.f11.f64 - ctx.f7.f64));
	// stfs f11,-176(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -176, temp.u32);
	// fmuls f11,f5,f9
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// fmuls f9,f3,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f9.f64));
	// addi r11,r1,-176
	ctx.r11.s64 = ctx.r1.s64 + -176;
	// fmuls f5,f5,f4
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f29,f4,f4
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f4.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fadds f4,f1,f31
	ctx.f4.f64 = double(float(ctx.f1.f64 + ctx.f31.f64));
	// fmuls f3,f2,f0
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fadds f1,f30,f8
	ctx.f1.f64 = double(float(ctx.f30.f64 + ctx.f8.f64));
	// stfs f1,-164(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -164, temp.u32);
	// fmuls f8,f11,f0
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f2,f6,f0
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f1,f9,f0
	ctx.f1.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f11,f5,f0
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fnmsubs f6,f29,f0,f13
	ctx.f6.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f9,f4,f0
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fadds f13,f20,f3
	ctx.f13.f64 = double(float(ctx.f20.f64 + ctx.f3.f64));
	// fadds f0,f22,f2
	ctx.f0.f64 = double(float(ctx.f22.f64 + ctx.f2.f64));
	// fadds f5,f8,f10
	ctx.f5.f64 = double(float(ctx.f8.f64 + ctx.f10.f64));
	// stfs f5,-168(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + -168, temp.u32);
	// fsubs f3,f10,f8
	ctx.f3.f64 = double(float(ctx.f10.f64 - ctx.f8.f64));
	// stfs f3,-152(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + -152, temp.u32);
	// fsubs f2,f1,f11
	ctx.f2.f64 = double(float(ctx.f1.f64 - ctx.f11.f64));
	// stfs f2,-156(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + -156, temp.u32);
	// fsubs f12,f6,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 - ctx.f12.f64));
	// stfs f12,-144(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -144, temp.u32);
	// fsubs f4,f6,f7
	ctx.f4.f64 = double(float(ctx.f6.f64 - ctx.f7.f64));
	// stfs f4,-160(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + -160, temp.u32);
	// fadds f1,f11,f1
	ctx.f1.f64 = double(float(ctx.f11.f64 + ctx.f1.f64));
	// stfs f1,-148(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -148, temp.u32);
	// fadds f12,f23,f9
	ctx.f12.f64 = double(float(ctx.f23.f64 + ctx.f9.f64));
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830CA214:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830ca214
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830CA214;
	// stfs f12,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f0,40(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f13,44(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
loc_830CA240:
	// lfs f0,16(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r3,12
	ctx.r11.s64 = ctx.r3.s64 + 12;
	// stfs f0,12(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// fmr f11,f0
	ctx.f11.f64 = ctx.f0.f64;
	// addi r10,r4,12
	ctx.r10.s64 = ctx.r4.s64 + 12;
	// addi r11,r11,36
	ctx.r11.s64 = ctx.r11.s64 + 36;
	// lfs f13,28(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,16(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// lfs f12,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f12.f64 = double(temp.f32);
	// fmr f8,f12
	ctx.f8.f64 = ctx.f12.f64;
	// stfs f12,20(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// lfs f9,340(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f5,f9,f11
	ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// fmuls f6,f10,f9
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f9.f64));
	// stfs f6,16(r4)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// fmuls f7,f8,f9
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// stfs f7,20(r4)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// fmr f2,f5
	ctx.f2.f64 = ctx.f5.f64;
	// stfs f5,12(r4)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// fmr f4,f6
	ctx.f4.f64 = ctx.f6.f64;
	// fmr f3,f7
	ctx.f3.f64 = ctx.f7.f64;
	// fneg f1,f2
	ctx.f1.u64 = ctx.f2.u64 ^ 0x8000000000000000;
	// stfs f1,0(r4)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fneg f0,f4
	ctx.f0.u64 = ctx.f4.u64 ^ 0x8000000000000000;
	// stfs f0,4(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// fneg f13,f3
	ctx.f13.u64 = ctx.f3.u64 ^ 0x8000000000000000;
	// stfs f13,8(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// lfs f12,48(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f12,f1
	ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f1.f64));
	// stfs f11,0(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// lfs f10,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f10.f64 = double(temp.f32);
	// fadds f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// stfs f9,4(r4)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// lfs f8,56(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	ctx.f8.f64 = double(temp.f32);
	// fadds f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 + ctx.f13.f64));
	// stfs f7,8(r4)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// lfs f6,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,48(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f3.f64 = double(temp.f32);
	// fadds f2,f3,f4
	ctx.f2.f64 = double(float(ctx.f3.f64 + ctx.f4.f64));
	// stfs f2,12(r4)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// lfs f1,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f1.f64 = double(temp.f32);
	// fadds f0,f1,f6
	ctx.f0.f64 = double(float(ctx.f1.f64 + ctx.f6.f64));
	// stfs f0,16(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// lfs f13,56(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f5
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f5.f64));
	// stfs f12,20(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// lfs f11,336(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,24(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 24, temp.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6b08
	ctx.lr = 0x830CA314;
	__restfpr_17(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA320"))) PPC_WEAK_FUNC(sub_830CA320);
PPC_FUNC_IMPL(__imp__sub_830CA320) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6abc
	ctx.lr = 0x830CA330;
	__savefpr_17(ctx, base);
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830ca540
	if (ctx.cr6.eq) goto loc_830CA540;
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x830ca540
	if (ctx.cr6.eq) goto loc_830CA540;
	// lfs f12,252(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 252);
	ctx.f12.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f11,112(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 112);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f12
	ctx.f10.f64 = ctx.f12.f64;
	// fmuls f9,f11,f12
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f6,248(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 248);
	ctx.f6.f64 = double(temp.f32);
	// lfs f4,120(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 120);
	ctx.f4.f64 = double(temp.f32);
	// fmr f29,f6
	ctx.f29.f64 = ctx.f6.f64;
	// lfs f2,124(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 124);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f4,f6
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f6.f64));
	// lfs f7,244(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 244);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f30,f2,f12
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// fmuls f5,f11,f7
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// lfs f8,256(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 256);
	ctx.f8.f64 = double(temp.f32);
	// lfs f31,116(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 116);
	ctx.f31.f64 = double(temp.f32);
	// fmr f3,f7
	ctx.f3.f64 = ctx.f7.f64;
	// lfs f27,128(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 128);
	ctx.f27.f64 = double(temp.f32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f28,136(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	ctx.f28.f64 = double(temp.f32);
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f13,6380(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6380);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r3,112
	ctx.r10.s64 = ctx.r3.s64 + 112;
	// lfs f26,132(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 132);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f25,f28,f10
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f10.f64));
	// fmadds f9,f31,f8,f9
	ctx.f9.f64 = double(float(ctx.f31.f64 * ctx.f8.f64 + ctx.f9.f64));
	// lfs f22,260(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 260);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f19,f27,f29
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// lfs f0,7676(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f1,f11,f8,f1
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f8.f64 + ctx.f1.f64));
	// lfs f21,264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 264);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f30,f4,f8,f30
	ctx.f30.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f30.f64));
	// lfs f20,268(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fmsubs f5,f2,f8,f5
	ctx.f5.f64 = double(float(ctx.f2.f64 * ctx.f8.f64 - ctx.f5.f64));
	// addi r10,r11,244
	ctx.r10.s64 = ctx.r11.s64 + 244;
	// fmsubs f24,f8,f8,f13
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f8.f64 - ctx.f13.f64));
	// lfs f13,6140(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 6140);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f23,f28,f3
	ctx.f23.f64 = double(float(ctx.f28.f64 * ctx.f3.f64));
	// addi r9,r3,12
	ctx.r9.s64 = ctx.r3.s64 + 12;
	// fmuls f18,f26,f10
	ctx.f18.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// fmadds f25,f27,f3,f25
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f3.f64 + ctx.f25.f64));
	// fmadds f9,f2,f6,f9
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f6.f64 + ctx.f9.f64));
	// fmadds f2,f2,f7,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f7.f64 + ctx.f1.f64));
	// fmadds f1,f31,f7,f30
	ctx.f1.f64 = double(float(ctx.f31.f64 * ctx.f7.f64 + ctx.f30.f64));
	// fnmsubs f5,f31,f6,f5
	ctx.f5.f64 = double(float(-(ctx.f31.f64 * ctx.f6.f64 - ctx.f5.f64)));
	// fmsubs f30,f26,f3,f19
	ctx.f30.f64 = double(float(ctx.f26.f64 * ctx.f3.f64 - ctx.f19.f64));
	// fmsubs f23,f27,f10,f23
	ctx.f23.f64 = double(float(ctx.f27.f64 * ctx.f10.f64 - ctx.f23.f64));
	// fmuls f19,f28,f24
	ctx.f19.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// fmsubs f28,f28,f29,f18
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f29.f64 - ctx.f18.f64));
	// fmuls f17,f26,f24
	ctx.f17.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// fnmsubs f9,f4,f7,f9
	ctx.f9.f64 = double(float(-(ctx.f4.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// fmadds f26,f26,f29,f25
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f25.f64));
	// fnmsubs f2,f31,f12,f2
	ctx.f2.f64 = double(float(-(ctx.f31.f64 * ctx.f12.f64 - ctx.f2.f64)));
	// fnmsubs f1,f11,f6,f1
	ctx.f1.f64 = double(float(-(ctx.f11.f64 * ctx.f6.f64 - ctx.f1.f64)));
	// fnmsubs f5,f4,f12,f5
	ctx.f5.f64 = double(float(-(ctx.f4.f64 * ctx.f12.f64 - ctx.f5.f64)));
	// fmuls f12,f30,f8
	ctx.f12.f64 = double(float(ctx.f30.f64 * ctx.f8.f64));
	// fmuls f4,f23,f8
	ctx.f4.f64 = double(float(ctx.f23.f64 * ctx.f8.f64));
	// fmuls f7,f27,f24
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// fmuls f11,f28,f8
	ctx.f11.f64 = double(float(ctx.f28.f64 * ctx.f8.f64));
	// fmuls f6,f9,f9
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f9.f64));
	// fmuls f8,f26,f29
	ctx.f8.f64 = double(float(ctx.f26.f64 * ctx.f29.f64));
	// fmuls f10,f26,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// fmuls f31,f9,f2
	ctx.f31.f64 = double(float(ctx.f9.f64 * ctx.f2.f64));
	// fmuls f30,f1,f1
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f1.f64));
	// fadds f12,f19,f12
	ctx.f12.f64 = double(float(ctx.f19.f64 + ctx.f12.f64));
	// fadds f4,f17,f4
	ctx.f4.f64 = double(float(ctx.f17.f64 + ctx.f4.f64));
	// fmuls f29,f5,f1
	ctx.f29.f64 = double(float(ctx.f5.f64 * ctx.f1.f64));
	// fadds f11,f7,f11
	ctx.f11.f64 = double(float(ctx.f7.f64 + ctx.f11.f64));
	// fmuls f3,f26,f3
	ctx.f3.f64 = double(float(ctx.f26.f64 * ctx.f3.f64));
	// fmuls f7,f6,f0
	ctx.f7.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fadds f12,f12,f10
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f10.f64));
	// fadds f6,f4,f8
	ctx.f6.f64 = double(float(ctx.f4.f64 + ctx.f8.f64));
	// fmuls f4,f31,f0
	ctx.f4.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fmuls f8,f30,f0
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmuls f31,f29,f0
	ctx.f31.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// fsubs f10,f13,f7
	ctx.f10.f64 = double(float(ctx.f13.f64 - ctx.f7.f64));
	// fsubs f30,f4,f31
	ctx.f30.f64 = double(float(ctx.f4.f64 - ctx.f31.f64));
	// stfs f30,-172(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + -172, temp.u32);
	// fsubs f10,f10,f8
	ctx.f10.f64 = double(float(ctx.f10.f64 - ctx.f8.f64));
	// stfs f10,-176(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -176, temp.u32);
	// fmuls f10,f1,f2
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f2.f64));
	// fmuls f30,f5,f9
	ctx.f30.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// fmuls f9,f1,f9
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f9.f64));
	// addi r11,r1,-176
	ctx.r11.s64 = ctx.r1.s64 + -176;
	// fmuls f5,f5,f2
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f2.f64));
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fmuls f29,f2,f2
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f2.f64));
	// li r8,9
	ctx.r8.s64 = 9;
	// fadds f3,f11,f3
	ctx.f3.f64 = double(float(ctx.f11.f64 + ctx.f3.f64));
	// fmuls f2,f12,f0
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fadds f12,f31,f4
	ctx.f12.f64 = double(float(ctx.f31.f64 + ctx.f4.f64));
	// stfs f12,-164(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -164, temp.u32);
	// fmuls f1,f6,f0
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fmuls f11,f10,f0
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f10,f30,f0
	ctx.f10.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmuls f4,f9,f0
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f12,f5,f0
	ctx.f12.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fnmsubs f6,f29,f0,f13
	ctx.f6.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f9,f3,f0
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fadds f13,f20,f2
	ctx.f13.f64 = double(float(ctx.f20.f64 + ctx.f2.f64));
	// fadds f0,f21,f1
	ctx.f0.f64 = double(float(ctx.f21.f64 + ctx.f1.f64));
	// fadds f5,f10,f11
	ctx.f5.f64 = double(float(ctx.f10.f64 + ctx.f11.f64));
	// stfs f5,-168(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + -168, temp.u32);
	// fsubs f2,f11,f10
	ctx.f2.f64 = double(float(ctx.f11.f64 - ctx.f10.f64));
	// stfs f2,-152(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + -152, temp.u32);
	// fsubs f1,f4,f12
	ctx.f1.f64 = double(float(ctx.f4.f64 - ctx.f12.f64));
	// stfs f1,-156(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -156, temp.u32);
	// fadds f12,f12,f4
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f4.f64));
	// stfs f12,-148(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -148, temp.u32);
	// fsubs f3,f6,f8
	ctx.f3.f64 = double(float(ctx.f6.f64 - ctx.f8.f64));
	// stfs f3,-160(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + -160, temp.u32);
	// fsubs f11,f6,f7
	ctx.f11.f64 = double(float(ctx.f6.f64 - ctx.f7.f64));
	// stfs f11,-144(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -144, temp.u32);
	// fadds f12,f22,f9
	ctx.f12.f64 = double(float(ctx.f22.f64 + ctx.f9.f64));
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_830CA514:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x830ca514
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_830CA514;
	// stfs f12,36(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 36, temp.u32);
	// stfs f0,40(r9)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 40, temp.u32);
	// stfs f13,44(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 44, temp.u32);
	// lwz r11,264(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 264);
	// lwz r10,280(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 280);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
loc_830CA540:
	// lfs f0,16(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r3,12
	ctx.r11.s64 = ctx.r3.s64 + 12;
	// stfs f0,12(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// fmr f11,f0
	ctx.f11.f64 = ctx.f0.f64;
	// addi r10,r4,12
	ctx.r10.s64 = ctx.r4.s64 + 12;
	// addi r11,r11,36
	ctx.r11.s64 = ctx.r11.s64 + 36;
	// lfs f13,28(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,16(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// lfs f12,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f12.f64 = double(temp.f32);
	// fmr f8,f12
	ctx.f8.f64 = ctx.f12.f64;
	// stfs f12,20(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// lfs f9,340(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f5,f11,f9
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f9.f64));
	// fmuls f6,f10,f9
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f9.f64));
	// stfs f6,16(r4)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// fmuls f7,f8,f9
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// stfs f7,20(r4)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// fmr f2,f5
	ctx.f2.f64 = ctx.f5.f64;
	// stfs f5,12(r4)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// fmr f4,f6
	ctx.f4.f64 = ctx.f6.f64;
	// fmr f3,f7
	ctx.f3.f64 = ctx.f7.f64;
	// fneg f1,f2
	ctx.f1.u64 = ctx.f2.u64 ^ 0x8000000000000000;
	// stfs f1,0(r4)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// fneg f0,f4
	ctx.f0.u64 = ctx.f4.u64 ^ 0x8000000000000000;
	// stfs f0,4(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// fneg f13,f3
	ctx.f13.u64 = ctx.f3.u64 ^ 0x8000000000000000;
	// stfs f13,8(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// lfs f12,48(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f12,f1
	ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f1.f64));
	// stfs f11,0(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// lfs f10,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f10.f64 = double(temp.f32);
	// fadds f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// stfs f9,4(r4)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// lfs f8,56(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	ctx.f8.f64 = double(temp.f32);
	// fadds f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 + ctx.f13.f64));
	// stfs f7,8(r4)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// lfs f6,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,48(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,12(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	ctx.f3.f64 = double(temp.f32);
	// fadds f2,f3,f4
	ctx.f2.f64 = double(float(ctx.f3.f64 + ctx.f4.f64));
	// stfs f2,12(r4)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// lfs f1,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f1.f64 = double(temp.f32);
	// fadds f0,f1,f6
	ctx.f0.f64 = double(float(ctx.f1.f64 + ctx.f6.f64));
	// stfs f0,16(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// lfs f13,56(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 56);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f5
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f5.f64));
	// stfs f12,20(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// lfs f11,336(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 336);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,24(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 24, temp.u32);
	// addi r12,r1,-8
	ctx.r12.s64 = ctx.r1.s64 + -8;
	// bl 0x82cb6b08
	ctx.lr = 0x830CA614;
	__restfpr_17(ctx, base);
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA620"))) PPC_WEAK_FUNC(sub_830CA620);
PPC_FUNC_IMPL(__imp__sub_830CA620) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// addi r10,r11,6264
	ctx.r10.s64 = ctx.r11.s64 + 6264;
	// stw r10,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r10.u32);
	// bl 0x83088de0
	ctx.lr = 0x830CA64C;
	sub_83088DE0(ctx, base);
	// clrlwi r9,r30,31
	ctx.r9.u64 = ctx.r30.u32 & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x830ca674
	if (ctx.cr6.eq) goto loc_830CA674;
	// lis r11,-31901
	ctx.r11.s64 = -2090663936;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lwz r3,-32308(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -32308);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,20(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x830CA674;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_830CA674:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA690"))) PPC_WEAK_FUNC(sub_830CA690);
PPC_FUNC_IMPL(__imp__sub_830CA690) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f0,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,0(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// lfs f13,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// lfs f12,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,8(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA6AC"))) PPC_WEAK_FUNC(sub_830CA6AC);
PPC_FUNC_IMPL(__imp__sub_830CA6AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830CA6B0"))) PPC_WEAK_FUNC(sub_830CA6B0);
PPC_FUNC_IMPL(__imp__sub_830CA6B0) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,340
	ctx.r3.s64 = ctx.r3.s64 + 340;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA6B8"))) PPC_WEAK_FUNC(sub_830CA6B8);
PPC_FUNC_IMPL(__imp__sub_830CA6B8) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// lis r10,-32248
	ctx.r10.s64 = -2113404928;
	// addi r9,r11,6976
	ctx.r9.s64 = ctx.r11.s64 + 6976;
	// addi r8,r10,6928
	ctx.r8.s64 = ctx.r10.s64 + 6928;
	// stw r9,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r9.u32);
	// stw r8,336(r3)
	PPC_STORE_U32(ctx.r3.u32 + 336, ctx.r8.u32);
	// b 0x83088de0
	sub_83088DE0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830CA6D4"))) PPC_WEAK_FUNC(sub_830CA6D4);
PPC_FUNC_IMPL(__imp__sub_830CA6D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830CA6D8"))) PPC_WEAK_FUNC(sub_830CA6D8);
PPC_FUNC_IMPL(__imp__sub_830CA6D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f0,6048(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6048);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,8(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// stfs f0,4(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// stfs f0,0(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// lfs f0,340(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,348(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 348);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,344(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 344);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f12,f12
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f12.f64));
	// fmadds f10,f0,f0,f11
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f0.f64 + ctx.f11.f64));
	// fmadds f9,f13,f13,f10
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 + ctx.f10.f64));
	// fsqrts f8,f9
	ctx.f8.f64 = double(float(sqrt(ctx.f9.f64)));
	// stfs f8,12(r4)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA710"))) PPC_WEAK_FUNC(sub_830CA710);
PPC_FUNC_IMPL(__imp__sub_830CA710) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f0,340(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r3,340
	ctx.r11.s64 = ctx.r3.s64 + 340;
	// lfs f13,344(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 344);
	ctx.f13.f64 = double(temp.f32);
	// fneg f12,f0
	ctx.f12.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lfs f11,348(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 348);
	ctx.f11.f64 = double(temp.f32);
	// fneg f10,f13
	ctx.f10.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// fneg f9,f11
	ctx.f9.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// stfs f12,0(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// stfs f10,4(r4)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// stfs f9,8(r4)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// lfs f8,340(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 340);
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,12(r4)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// lfs f7,344(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 344);
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,16(r4)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// lfs f6,348(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 348);
	ctx.f6.f64 = double(temp.f32);
	// stfs f6,20(r4)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA754"))) PPC_WEAK_FUNC(sub_830CA754);
PPC_FUNC_IMPL(__imp__sub_830CA754) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830CA758"))) PPC_WEAK_FUNC(sub_830CA758);
PPC_FUNC_IMPL(__imp__sub_830CA758) {
	PPC_FUNC_PROLOGUE();
	// li r3,8
	ctx.r3.s64 = 8;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA760"))) PPC_WEAK_FUNC(sub_830CA760);
PPC_FUNC_IMPL(__imp__sub_830CA760) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA768"))) PPC_WEAK_FUNC(sub_830CA768);
PPC_FUNC_IMPL(__imp__sub_830CA768) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-31907
	ctx.r11.s64 = -2091057152;
	// addi r3,r11,-15280
	ctx.r3.s64 = ctx.r11.s64 + -15280;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA774"))) PPC_WEAK_FUNC(sub_830CA774);
PPC_FUNC_IMPL(__imp__sub_830CA774) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830CA778"))) PPC_WEAK_FUNC(sub_830CA778);
PPC_FUNC_IMPL(__imp__sub_830CA778) {
	PPC_FUNC_PROLOGUE();
	// li r3,6
	ctx.r3.s64 = 6;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA780"))) PPC_WEAK_FUNC(sub_830CA780);
PPC_FUNC_IMPL(__imp__sub_830CA780) {
	PPC_FUNC_PROLOGUE();
	// rlwinm r11,r4,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// addi r3,r11,112
	ctx.r3.s64 = ctx.r11.s64 + 112;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA798"))) PPC_WEAK_FUNC(sub_830CA798);
PPC_FUNC_IMPL(__imp__sub_830CA798) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,112
	ctx.r3.s64 = ctx.r3.s64 + 112;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA7A0"))) PPC_WEAK_FUNC(sub_830CA7A0);
PPC_FUNC_IMPL(__imp__sub_830CA7A0) {
	PPC_FUNC_PROLOGUE();
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA7A8"))) PPC_WEAK_FUNC(sub_830CA7A8);
PPC_FUNC_IMPL(__imp__sub_830CA7A8) {
	PPC_FUNC_PROLOGUE();
	// li r3,12
	ctx.r3.s64 = 12;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA7B0"))) PPC_WEAK_FUNC(sub_830CA7B0);
PPC_FUNC_IMPL(__imp__sub_830CA7B0) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-31907
	ctx.r11.s64 = -2091057152;
	// addi r3,r11,-15136
	ctx.r3.s64 = ctx.r11.s64 + -15136;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA7BC"))) PPC_WEAK_FUNC(sub_830CA7BC);
PPC_FUNC_IMPL(__imp__sub_830CA7BC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830CA7C0"))) PPC_WEAK_FUNC(sub_830CA7C0);
PPC_FUNC_IMPL(__imp__sub_830CA7C0) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-31907
	ctx.r11.s64 = -2091057152;
	// addi r3,r11,-15112
	ctx.r3.s64 = ctx.r11.s64 + -15112;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA7CC"))) PPC_WEAK_FUNC(sub_830CA7CC);
PPC_FUNC_IMPL(__imp__sub_830CA7CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830CA7D0"))) PPC_WEAK_FUNC(sub_830CA7D0);
PPC_FUNC_IMPL(__imp__sub_830CA7D0) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-31907
	ctx.r11.s64 = -2091057152;
	// addi r3,r11,-15016
	ctx.r3.s64 = ctx.r11.s64 + -15016;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CA7DC"))) PPC_WEAK_FUNC(sub_830CA7DC);
PPC_FUNC_IMPL(__imp__sub_830CA7DC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830CA7E0"))) PPC_WEAK_FUNC(sub_830CA7E0);
PPC_FUNC_IMPL(__imp__sub_830CA7E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10e8
	ctx.lr = 0x830CA7E8;
	__savegprlr_28(ctx, base);
	// stfd f30,-56(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.f30.u64);
	// stfd f31,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.f31.u64);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r31,352
	ctx.r4.s64 = ctx.r31.s64 + 352;
	// addi r30,r31,340
	ctx.r30.s64 = ctx.r31.s64 + 340;
	// lfs f0,340(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 340);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,344(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 344);
	ctx.f13.f64 = double(temp.f32);
	// fneg f12,f0
	ctx.f12.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lfs f11,348(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 348);
	ctx.f11.f64 = double(temp.f32);
	// fneg f10,f13
	ctx.f10.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// fneg f9,f11
	ctx.f9.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// stfs f12,80(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f10,84(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// stfs f9,88(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// stfs f0,92(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// stfs f13,96(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// stfs f11,100(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// bl 0x82d7a030
	ctx.lr = 0x830CA838;
	sub_82D7A030(ctx, base);
	// lis r10,-32248
	ctx.r10.s64 = -2113404928;
	// addi r11,r31,452
	ctx.r11.s64 = ctx.r31.s64 + 452;
	// addi r8,r10,6880
	ctx.r8.s64 = ctx.r10.s64 + 6880;
	// li r10,6
	ctx.r10.s64 = 6;
	// addi r9,r8,-24
	ctx.r9.s64 = ctx.r8.s64 + -24;
	// li r7,4
	ctx.r7.s64 = 4;
loc_830CA850:
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// addic. r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r8,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// sth r7,-4(r11)
	PPC_STORE_U16(ctx.r11.u32 + -4, ctx.r7.u16);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// addi r11,r11,36
	ctx.r11.s64 = ctx.r11.s64 + 36;
	// bne 0x830ca850
	if (!ctx.cr0.eq) goto loc_830CA850;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f9,8(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f9.f64 = double(temp.f32);
	// lis r8,-32222
	ctx.r8.s64 = -2111700992;
	// fneg f8,f9
	ctx.f8.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f11,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// fneg f7,f11
	ctx.f7.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// fneg f6,f10
	ctx.f6.u64 = ctx.f10.u64 ^ 0x8000000000000000;
	// addi r11,r31,496
	ctx.r11.s64 = ctx.r31.s64 + 496;
	// lfs f0,6048(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 6048);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r31,568
	ctx.r11.s64 = ctx.r31.s64 + 568;
	// lfs f12,-18324(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -18324);
	ctx.f12.f64 = double(temp.f32);
	// fmr f4,f0
	ctx.f4.f64 = ctx.f0.f64;
	// fmr f5,f12
	ctx.f5.f64 = ctx.f12.f64;
	// lfs f13,6140(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6140);
	ctx.f13.f64 = double(temp.f32);
	// fmr f3,f0
	ctx.f3.f64 = ctx.f0.f64;
	// stfs f13,496(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r31.u32 + 496, temp.u32);
	// stfs f0,500(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 500, temp.u32);
	// addi r11,r31,604
	ctx.r11.s64 = ctx.r31.s64 + 604;
	// stfs f0,504(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 504, temp.u32);
	// addi r30,r31,336
	ctx.r30.s64 = ctx.r31.s64 + 336;
	// stfs f12,568(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + 568, temp.u32);
	// addi r11,r31,532
	ctx.r11.s64 = ctx.r31.s64 + 532;
	// stfs f0,572(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 572, temp.u32);
	// addi r11,r31,460
	ctx.r11.s64 = ctx.r31.s64 + 460;
	// stfs f0,576(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 576, temp.u32);
	// addi r11,r31,640
	ctx.r11.s64 = ctx.r31.s64 + 640;
	// stfs f13,608(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r31.u32 + 608, temp.u32);
	// li r28,0
	ctx.r28.s64 = 0;
	// stfs f0,604(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 604, temp.u32);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// stfs f0,612(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 612, temp.u32);
	// fmuls f1,f5,f8
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f8.f64));
	// stfs f0,640(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 640, temp.u32);
	// fadds f2,f4,f3
	ctx.f2.f64 = double(float(ctx.f4.f64 + ctx.f3.f64));
	// stfs f12,644(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + 644, temp.u32);
	// stfs f0,648(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 648, temp.u32);
	// stfs f13,540(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r31.u32 + 540, temp.u32);
	// stfs f0,532(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 532, temp.u32);
	// stfs f0,536(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 536, temp.u32);
	// stfs f0,460(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 460, temp.u32);
	// stfs f0,464(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 464, temp.u32);
	// stfs f12,468(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + 468, temp.u32);
	// lwz r7,336(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 336);
	// fnmadds f13,f2,f0,f1
	ctx.f13.f64 = double(float(-(ctx.f2.f64 * ctx.f0.f64 + ctx.f1.f64)));
	// stfs f13,472(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r31.u32 + 472, temp.u32);
	// lfs f8,504(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 504);
	ctx.f8.f64 = double(temp.f32);
	// lfs f5,500(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 500);
	ctx.f5.f64 = double(temp.f32);
	// lfs f12,496(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 496);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f3,f12,f11
	ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// fadds f4,f8,f5
	ctx.f4.f64 = double(float(ctx.f8.f64 + ctx.f5.f64));
	// lwz r6,12(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// fnmadds f2,f4,f0,f3
	ctx.f2.f64 = double(float(-(ctx.f4.f64 * ctx.f0.f64 + ctx.f3.f64)));
	// stfs f2,508(r31)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r31.u32 + 508, temp.u32);
	// lfs f13,536(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 536);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,532(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 532);
	ctx.f12.f64 = double(temp.f32);
	// lfs f1,540(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 540);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f9,f1,f9
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f9.f64));
	// fadds f11,f13,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// fnmadds f8,f11,f0,f9
	ctx.f8.f64 = double(float(-(ctx.f11.f64 * ctx.f0.f64 + ctx.f9.f64)));
	// stfs f8,544(r31)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r31.u32 + 544, temp.u32);
	// lfs f4,576(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 576);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,572(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 572);
	ctx.f3.f64 = double(temp.f32);
	// lfs f5,568(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 568);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f1,f5,f7
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f7.f64));
	// fadds f2,f4,f3
	ctx.f2.f64 = double(float(ctx.f4.f64 + ctx.f3.f64));
	// fnmadds f13,f2,f0,f1
	ctx.f13.f64 = double(float(-(ctx.f2.f64 * ctx.f0.f64 + ctx.f1.f64)));
	// stfs f13,580(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r31.u32 + 580, temp.u32);
	// lfs f11,612(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 612);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,604(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 604);
	ctx.f9.f64 = double(temp.f32);
	// lfs f12,608(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 608);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f7,f12,f10
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// fadds f8,f11,f9
	ctx.f8.f64 = double(float(ctx.f11.f64 + ctx.f9.f64));
	// fnmadds f5,f8,f0,f7
	ctx.f5.f64 = double(float(-(ctx.f8.f64 * ctx.f0.f64 + ctx.f7.f64)));
	// stfs f5,616(r31)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r31.u32 + 616, temp.u32);
	// lfs f3,648(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 648);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,640(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 640);
	ctx.f1.f64 = double(temp.f32);
	// lfs f4,644(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 644);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f2,f4,f6
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f6.f64));
	// fadds f13,f3,f1
	ctx.f13.f64 = double(float(ctx.f3.f64 + ctx.f1.f64));
	// fnmadds f12,f13,f0,f2
	ctx.f12.f64 = double(float(-(ctx.f13.f64 * ctx.f0.f64 + ctx.f2.f64)));
	// stfs f12,652(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + 652, temp.u32);
	// mtctr r6
	ctx.ctr.u64 = ctx.r6.u64;
	// bctrl 
	ctx.lr = 0x830CA9C4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x830caa98
	if (ctx.cr6.eq) goto loc_830CAA98;
	// lis r11,-32222
	ctx.r11.s64 = -2111700992;
	// lis r10,-32222
	ctx.r10.s64 = -2111700992;
	// addi r31,r31,464
	ctx.r31.s64 = ctx.r31.s64 + 464;
	// lfs f30,-18268(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -18268);
	ctx.f30.f64 = double(temp.f32);
	// lfs f31,-18264(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -18264);
	ctx.f31.f64 = double(temp.f32);
loc_830CA9E0:
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830CA9F4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x830CAA0C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// stfs f31,12(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r31.u32 + 12, temp.u32);
	// stfs f30,16(r31)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r31.u32 + 16, temp.u32);
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x830caa74
	if (ctx.cr6.eq) goto loc_830CAA74;
loc_830CAA1C:
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lfs f0,0(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// addi r29,r29,-1
	ctx.r29.s64 = ctx.r29.s64 + -1;
	// lfs f12,-4(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + -4);
	ctx.f12.f64 = double(temp.f32);
	// addi r3,r3,12
	ctx.r3.s64 = ctx.r3.s64 + 12;
	// lfs f11,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f8,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f6,f8,f13,f9
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f13.f64 + ctx.f9.f64));
	// fmadds f0,f7,f12,f6
	ctx.f0.f64 = double(float(ctx.f7.f64 * ctx.f12.f64 + ctx.f6.f64));
	// fcmpu cr6,f0,f11
	ctx.cr6.compare(ctx.f0.f64, ctx.f11.f64);
	// bge cr6,0x830caa5c
	if (!ctx.cr6.lt) goto loc_830CAA5C;
	// stfs f0,12(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 12, temp.u32);
loc_830CAA5C:
	// lfs f13,16(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// ble cr6,0x830caa6c
	if (!ctx.cr6.gt) goto loc_830CAA6C;
	// stfs f0,16(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 16, temp.u32);
loc_830CAA6C:
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// bne cr6,0x830caa1c
	if (!ctx.cr6.eq) goto loc_830CAA1C;
loc_830CAA74:
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// addi r31,r31,36
	ctx.r31.s64 = ctx.r31.s64 + 36;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x830CAA90;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmplw cr6,r28,r3
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r3.u32, ctx.xer);
	// blt cr6,0x830ca9e0
	if (ctx.cr6.lt) goto loc_830CA9E0;
loc_830CAA98:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// lfd f30,-56(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// lfd f31,-48(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x82cb1138
	__restgprlr_28(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830CAAA8"))) PPC_WEAK_FUNC(sub_830CAAA8);
PPC_FUNC_IMPL(__imp__sub_830CAAA8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r31.u64);
	// lfs f11,4(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lfs f10,36(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 36);
	ctx.f10.f64 = double(temp.f32);
	// lwz r4,8(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// fmuls f9,f10,f11
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
	// lfs f4,20(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 20);
	ctx.f4.f64 = double(temp.f32);
	// lfs f6,8(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f3,f4,f11
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// lfs f1,8(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	ctx.f1.f64 = double(temp.f32);
	// addi r11,r3,4
	ctx.r11.s64 = ctx.r3.s64 + 4;
	// lfs f8,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f0,f1,f6
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f6.f64));
	// lfs f7,32(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 32);
	ctx.f7.f64 = double(temp.f32);
	// lwz r7,4(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lfs f2,16(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	ctx.f2.f64 = double(temp.f32);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// lfs f12,4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// mr r30,r7
	ctx.r30.u64 = ctx.r7.u64;
	// lfs f5,40(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 40);
	ctx.f5.f64 = double(temp.f32);
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// lfs f10,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lfs f1,52(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 52);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f9,f7,f8,f9
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f8.f64 + ctx.f9.f64));
	// lfs f7,56(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 56);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f3,f2,f8,f3
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f8.f64 + ctx.f3.f64));
	// lfs f13,24(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 24);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f4,f6,f7
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// lfs f7,48(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 48);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f2,f12,f11,f0
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f11.f64 + ctx.f0.f64));
	// fmadds f0,f5,f6,f9
	ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f6.f64 + ctx.f9.f64));
	// stfs f0,-56(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -56, temp.u32);
	// lwz r3,-56(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + -56);
	// rlwinm r11,r3,0,0,0
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0x80000000;
	// fneg f12,f0
	ctx.f12.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// stfs f12,-40(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// lwz r8,-40(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	// fmadds f12,f8,f10,f2
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f10.f64 + ctx.f2.f64));
	// or r3,r11,r10
	ctx.r3.u64 = ctx.r11.u64 | ctx.r10.u64;
	// stfs f12,-64(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + -64, temp.u32);
	// lwz r10,-64(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -64);
	// fmadds f13,f13,f6,f3
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f6.f64 + ctx.f3.f64));
	// stw r3,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, ctx.r3.u32);
	// rlwinm r3,r10,0,0,0
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000000;
	// rlwinm r10,r8,0,0,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x80000000;
	// fmadds f6,f11,f1,f4
	ctx.f6.f64 = double(float(ctx.f11.f64 * ctx.f1.f64 + ctx.f4.f64));
	// fneg f4,f12
	ctx.f4.u64 = ctx.f12.u64 ^ 0x8000000000000000;
	// stfs f13,-60(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -60, temp.u32);
	// lwz r11,-60(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -60);
	// or r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stfs f4,-48(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// lwz r10,-48(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	// lfs f9,-24(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -24);
	ctx.f9.f64 = double(temp.f32);
	// stw r9,-40(r1)
	PPC_STORE_U32(ctx.r1.u32 + -40, ctx.r9.u32);
	// lfs f3,-40(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -40);
	ctx.f3.f64 = double(temp.f32);
	// fneg f5,f13
	ctx.f5.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// stfs f5,-44(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// lwz r8,-44(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// or r7,r3,r7
	ctx.r7.u64 = ctx.r3.u64 | ctx.r7.u64;
	// rlwinm r3,r8,0,0,0
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0x80000000;
	// rlwinm r9,r11,0,0,0
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x80000000;
	// stw r7,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, ctx.r7.u32);
	// rlwinm r11,r10,0,0,0
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000000;
	// lfs f11,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f11.f64 = double(temp.f32);
	// or r10,r3,r31
	ctx.r10.u64 = ctx.r3.u64 | ctx.r31.u64;
	// fmadds f1,f8,f7,f6
	ctx.f1.f64 = double(float(ctx.f8.f64 * ctx.f7.f64 + ctx.f6.f64));
	// or r4,r9,r4
	ctx.r4.u64 = ctx.r9.u64 | ctx.r4.u64;
	// fmuls f2,f9,f0
	ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// stw r10,-44(r1)
	PPC_STORE_U32(ctx.r1.u32 + -44, ctx.r10.u32);
	// lfs f6,-44(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	ctx.f6.f64 = double(temp.f32);
	// or r9,r11,r30
	ctx.r9.u64 = ctx.r11.u64 | ctx.r30.u64;
	// stw r4,-28(r1)
	PPC_STORE_U32(ctx.r1.u32 + -28, ctx.r4.u32);
	// lfs f10,-28(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -28);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f7,f3,f0
	ctx.f7.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// stw r9,-48(r1)
	PPC_STORE_U32(ctx.r1.u32 + -48, ctx.r9.u32);
	// lfs f5,-48(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -48);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f9,f10,f13,f2
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f13.f64 + ctx.f2.f64));
	// fmadds f8,f11,f12,f9
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f12.f64 + ctx.f9.f64));
	// fmadds f3,f6,f13,f7
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f13.f64 + ctx.f7.f64));
	// fadds f4,f8,f1
	ctx.f4.f64 = double(float(ctx.f8.f64 + ctx.f1.f64));
	// stfs f4,0(r6)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// fmadds f2,f5,f12,f3
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f12.f64 + ctx.f3.f64));
	// fadds f1,f2,f1
	ctx.f1.f64 = double(float(ctx.f2.f64 + ctx.f1.f64));
	// stfs f1,0(r5)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// ld r30,-16(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CAC08"))) PPC_WEAK_FUNC(sub_830CAC08);
PPC_FUNC_IMPL(__imp__sub_830CAC08) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f0,6048(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 6048);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,8(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// stfs f0,4(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// stfs f0,0(r4)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// lfs f0,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,12(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f12,f12
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f12.f64));
	// fmadds f10,f0,f0,f11
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f0.f64 + ctx.f11.f64));
	// fmadds f9,f13,f13,f10
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 + ctx.f10.f64));
	// fsqrts f8,f9
	ctx.f8.f64 = double(float(sqrt(ctx.f9.f64)));
	// stfs f8,12(r4)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_830CAC40"))) PPC_WEAK_FUNC(sub_830CAC40);
PPC_FUNC_IMPL(__imp__sub_830CAC40) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82cb10ec
	ctx.lr = 0x830CAC48;
	__savegprlr_29(ctx, base);
	// addi r12,r1,-32
	ctx.r12.s64 = ctx.r1.s64 + -32;
	// bl 0x82cb6abc
	ctx.lr = 0x830CAC50;
	__savefpr_17(ctx, base);
	// stwu r1,-384(r1)
	ea = -384 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// mr r29,r5
	ctx.r29.u64 = ctx.r5.u64;
	// bl 0x8308a278
	ctx.lr = 0x830CAC64;
	sub_8308A278(ctx, base);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// lis r10,-32248
	ctx.r10.s64 = -2113404928;
	// lis r9,-32248
	ctx.r9.s64 = -2113404928;
	// addi r8,r11,-5860
	ctx.r8.s64 = ctx.r11.s64 + -5860;
	// addi r7,r10,6976
	ctx.r7.s64 = ctx.r10.s64 + 6976;
	// addi r6,r9,6928
	ctx.r6.s64 = ctx.r9.s64 + 6928;
	// stw r8,336(r31)
	PPC_STORE_U32(ctx.r31.u32 + 336, ctx.r8.u32);
	// li r5,2
	ctx.r5.s64 = 2;
	// stw r7,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r7.u32);
	// stw r6,336(r31)
	PPC_STORE_U32(ctx.r31.u32 + 336, ctx.r6.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r5,288(r31)
	PPC_STORE_U32(ctx.r31.u32 + 288, ctx.r5.u32);
	// lfs f0,92(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 92);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,340(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 340, temp.u32);
	// addi r11,r30,92
	ctx.r11.s64 = ctx.r30.s64 + 92;
	// lfs f13,96(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,344(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r31.u32 + 344, temp.u32);
	// lfs f12,100(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 100);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,348(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + 348, temp.u32);
	// bl 0x830ca7e0
	ctx.lr = 0x830CACB4;
	sub_830CA7E0(ctx, base);
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// bl 0x8315fb70
	ctx.lr = 0x830CACBC;
	sub_8315FB70(ctx, base);
	// li r11,5
	ctx.r11.s64 = 5;
	// lfs f11,92(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 92);
	ctx.f11.f64 = double(temp.f32);
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// lfs f10,96(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 96);
	ctx.f10.f64 = double(temp.f32);
	// stw r11,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r11.u32);
	// lfs f9,100(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 100);
	ctx.f9.f64 = double(temp.f32);
	// lwz r3,1412(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1412);
	// stfs f11,184(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// stfs f10,188(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 188, temp.u32);
	// stfs f9,192(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 192, temp.u32);
	// bl 0x8315fb80
	ctx.lr = 0x830CACE8;
	sub_8315FB80(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,152(r31)
	PPC_STORE_U32(ctx.r31.u32 + 152, ctx.r3.u32);
	// beq cr6,0x830cb190
	if (ctx.cr6.eq) goto loc_830CB190;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x8315fb70
	ctx.lr = 0x830CACFC;
	sub_8315FB70(ctx, base);
	// li r10,10
	ctx.r10.s64 = 10;
	// lwz r11,264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// stw r10,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r10.u32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x830caff0
	if (ctx.cr6.eq) goto loc_830CAFF0;
	// lfs f0,220(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 220);
	ctx.f0.f64 = double(temp.f32);
	// lfs f11,232(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 232);
	ctx.f11.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f13,224(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 224);
	ctx.f13.f64 = double(temp.f32);
	// fneg f8,f11
	ctx.f8.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// lfs f9,236(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 236);
	ctx.f9.f64 = double(temp.f32);
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// lfs f6,216(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 216);
	ctx.f6.f64 = double(temp.f32);
	// fneg f7,f9
	ctx.f7.u64 = ctx.f9.u64 ^ 0x8000000000000000;
	// lfs f5,240(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 240);
	ctx.f5.f64 = double(temp.f32);
	// fmr f4,f6
	ctx.f4.f64 = ctx.f6.f64;
	// fneg f3,f5
	ctx.f3.u64 = ctx.f5.u64 ^ 0x8000000000000000;
	// lfs f2,228(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 228);
	ctx.f2.f64 = double(temp.f32);
	// fneg f1,f6
	ctx.f1.u64 = ctx.f6.u64 ^ 0x8000000000000000;
	// lfs f11,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f11.f64 = double(temp.f32);
	// fneg f9,f13
	ctx.f9.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// lfs f13,6380(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6380);
	ctx.f13.f64 = double(temp.f32);
	// fneg f5,f0
	ctx.f5.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lfs f6,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f6.f64 = double(temp.f32);
	// fmr f30,f2
	ctx.f30.f64 = ctx.f2.f64;
	// lfs f31,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f31.f64 = double(temp.f32);
	// fmsubs f29,f2,f2,f13
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f2.f64 - ctx.f13.f64));
	// lfs f0,7676(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f28,f12,f8
	ctx.f28.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// addi r10,r11,216
	ctx.r10.s64 = ctx.r11.s64 + 216;
	// addi r10,r31,112
	ctx.r10.s64 = ctx.r31.s64 + 112;
	// fmuls f27,f10,f7
	ctx.f27.f64 = double(float(ctx.f10.f64 * ctx.f7.f64));
	// fmuls f26,f12,f7
	ctx.f26.f64 = double(float(ctx.f12.f64 * ctx.f7.f64));
	// fmuls f25,f4,f3
	ctx.f25.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// fmuls f24,f11,f1
	ctx.f24.f64 = double(float(ctx.f11.f64 * ctx.f1.f64));
	// fmuls f23,f9,f6
	ctx.f23.f64 = double(float(ctx.f9.f64 * ctx.f6.f64));
	// fmuls f22,f5,f6
	ctx.f22.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmuls f21,f5,f31
	ctx.f21.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// fmuls f19,f29,f8
	ctx.f19.f64 = double(float(ctx.f29.f64 * ctx.f8.f64));
	// fmsubs f28,f4,f7,f28
	ctx.f28.f64 = double(float(ctx.f4.f64 * ctx.f7.f64 - ctx.f28.f64));
	// fmuls f7,f29,f7
	ctx.f7.f64 = double(float(ctx.f29.f64 * ctx.f7.f64));
	// fmsubs f27,f12,f3,f27
	ctx.f27.f64 = double(float(ctx.f12.f64 * ctx.f3.f64 - ctx.f27.f64));
	// fmadds f26,f10,f3,f26
	ctx.f26.f64 = double(float(ctx.f10.f64 * ctx.f3.f64 + ctx.f26.f64));
	// fmsubs f25,f10,f8,f25
	ctx.f25.f64 = double(float(ctx.f10.f64 * ctx.f8.f64 - ctx.f25.f64));
	// fmuls f3,f29,f3
	ctx.f3.f64 = double(float(ctx.f29.f64 * ctx.f3.f64));
	// fmsubs f20,f30,f30,f13
	ctx.f20.f64 = double(float(ctx.f30.f64 * ctx.f30.f64 - ctx.f13.f64));
	// fmsubs f24,f9,f31,f24
	ctx.f24.f64 = double(float(ctx.f9.f64 * ctx.f31.f64 - ctx.f24.f64));
	// fmsubs f23,f5,f11,f23
	ctx.f23.f64 = double(float(ctx.f5.f64 * ctx.f11.f64 - ctx.f23.f64));
	// fmadds f22,f31,f1,f22
	ctx.f22.f64 = double(float(ctx.f31.f64 * ctx.f1.f64 + ctx.f22.f64));
	// fmuls f29,f28,f2
	ctx.f29.f64 = double(float(ctx.f28.f64 * ctx.f2.f64));
	// fmsubs f21,f6,f1,f21
	ctx.f21.f64 = double(float(ctx.f6.f64 * ctx.f1.f64 - ctx.f21.f64));
	// fmuls f28,f27,f2
	ctx.f28.f64 = double(float(ctx.f27.f64 * ctx.f2.f64));
	// fmadds f8,f4,f8,f26
	ctx.f8.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f26.f64));
	// fmuls f2,f25,f2
	ctx.f2.f64 = double(float(ctx.f25.f64 * ctx.f2.f64));
	// fmuls f27,f11,f20
	ctx.f27.f64 = double(float(ctx.f11.f64 * ctx.f20.f64));
	// fmuls f31,f31,f20
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f20.f64));
	// fmuls f6,f6,f20
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f20.f64));
	// fmadds f11,f9,f11,f22
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f11.f64 + ctx.f22.f64));
	// fsubs f3,f3,f29
	ctx.f3.f64 = double(float(ctx.f3.f64 - ctx.f29.f64));
	// fmuls f26,f24,f30
	ctx.f26.f64 = double(float(ctx.f24.f64 * ctx.f30.f64));
	// fmuls f25,f30,f23
	ctx.f25.f64 = double(float(ctx.f30.f64 * ctx.f23.f64));
	// fmuls f4,f8,f4
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// fsubs f29,f19,f28
	ctx.f29.f64 = double(float(ctx.f19.f64 - ctx.f28.f64));
	// fmuls f24,f21,f30
	ctx.f24.f64 = double(float(ctx.f21.f64 * ctx.f30.f64));
	// fmuls f10,f10,f8
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f8.f64));
	// fmuls f8,f12,f8
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// fsubs f7,f7,f2
	ctx.f7.f64 = double(float(ctx.f7.f64 - ctx.f2.f64));
	// fmuls f12,f11,f1
	ctx.f12.f64 = double(float(ctx.f11.f64 * ctx.f1.f64));
	// fmuls f28,f5,f11
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// fadds f6,f6,f26
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f26.f64));
	// fadds f2,f31,f25
	ctx.f2.f64 = double(float(ctx.f31.f64 + ctx.f25.f64));
	// fmuls f11,f9,f11
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// fadds f4,f29,f4
	ctx.f4.f64 = double(float(ctx.f29.f64 + ctx.f4.f64));
	// fadds f31,f27,f24
	ctx.f31.f64 = double(float(ctx.f27.f64 + ctx.f24.f64));
	// fadds f10,f3,f10
	ctx.f10.f64 = double(float(ctx.f3.f64 + ctx.f10.f64));
	// fadds f3,f7,f8
	ctx.f3.f64 = double(float(ctx.f7.f64 + ctx.f8.f64));
	// fadds f8,f6,f28
	ctx.f8.f64 = double(float(ctx.f6.f64 + ctx.f28.f64));
	// fadds f7,f2,f12
	ctx.f7.f64 = double(float(ctx.f2.f64 + ctx.f12.f64));
	// fmuls f12,f4,f0
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// lfs f4,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f4.f64 = double(temp.f32);
	// fadds f6,f31,f11
	ctx.f6.f64 = double(float(ctx.f31.f64 + ctx.f11.f64));
	// lfs f11,120(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f2,f10,f0
	ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f31,f30,f4
	ctx.f31.f64 = double(float(ctx.f30.f64 * ctx.f4.f64));
	// fmuls f10,f3,f0
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// fmuls f3,f30,f11
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f11.f64));
	// fmuls f28,f9,f4
	ctx.f28.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// lfs f27,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfs f29,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f23,176(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 176);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lfs f24,180(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f3,f27,f1,f3
	ctx.f3.f64 = double(float(ctx.f27.f64 * ctx.f1.f64 + ctx.f3.f64));
	// lfs f25,184(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 184);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f26,f4,f1
	ctx.f26.f64 = double(float(ctx.f4.f64 * ctx.f1.f64));
	// lfs f22,160(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f31,f29,f1,f31
	ctx.f31.f64 = double(float(ctx.f29.f64 * ctx.f1.f64 + ctx.f31.f64));
	// lfs f21,172(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 172);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,168(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 168);
	ctx.f20.f64 = double(temp.f32);
	// addi r10,r31,160
	ctx.r10.s64 = ctx.r31.s64 + 160;
	// lfs f19,164(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 164);
	ctx.f19.f64 = double(temp.f32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// fmadds f28,f30,f27,f28
	ctx.f28.f64 = double(float(ctx.f30.f64 * ctx.f27.f64 + ctx.f28.f64));
	// fadds f10,f8,f10
	ctx.f10.f64 = double(float(ctx.f8.f64 + ctx.f10.f64));
	// fadds f8,f7,f12
	ctx.f8.f64 = double(float(ctx.f7.f64 + ctx.f12.f64));
	// fadds f7,f6,f2
	ctx.f7.f64 = double(float(ctx.f6.f64 + ctx.f2.f64));
	// fmadds f6,f9,f29,f3
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f29.f64 + ctx.f3.f64));
	// fmsubs f30,f30,f29,f26
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f29.f64 - ctx.f26.f64));
	// fmadds f3,f5,f11,f31
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f11.f64 + ctx.f31.f64));
	// fmadds f2,f5,f29,f28
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f29.f64 + ctx.f28.f64));
	// fnmsubs f6,f5,f4,f6
	ctx.f6.f64 = double(float(-(ctx.f5.f64 * ctx.f4.f64 - ctx.f6.f64)));
	// fnmsubs f12,f5,f27,f30
	ctx.f12.f64 = double(float(-(ctx.f5.f64 * ctx.f27.f64 - ctx.f30.f64)));
	// fnmsubs f5,f9,f27,f3
	ctx.f5.f64 = double(float(-(ctx.f9.f64 * ctx.f27.f64 - ctx.f3.f64)));
	// fnmsubs f4,f11,f1,f2
	ctx.f4.f64 = double(float(-(ctx.f11.f64 * ctx.f1.f64 - ctx.f2.f64)));
	// fmuls f2,f6,f24
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f24.f64));
	// fnmsubs f3,f9,f11,f12
	ctx.f3.f64 = double(float(-(ctx.f9.f64 * ctx.f11.f64 - ctx.f12.f64)));
	// fmuls f1,f25,f5
	ctx.f1.f64 = double(float(ctx.f25.f64 * ctx.f5.f64));
	// fmuls f31,f6,f22
	ctx.f31.f64 = double(float(ctx.f6.f64 * ctx.f22.f64));
	// fmuls f30,f6,f21
	ctx.f30.f64 = double(float(ctx.f6.f64 * ctx.f21.f64));
	// fmuls f29,f22,f5
	ctx.f29.f64 = double(float(ctx.f22.f64 * ctx.f5.f64));
	// fmuls f12,f4,f23
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f23.f64));
	// fmuls f11,f4,f24
	ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f24.f64));
	// fmsubs f2,f4,f25,f2
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f25.f64 - ctx.f2.f64));
	// fmsubs f9,f3,f3,f13
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f3.f64 - ctx.f13.f64));
	// fmsubs f1,f6,f23,f1
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f23.f64 - ctx.f1.f64));
	// fmuls f13,f3,f22
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f22.f64));
	// fmadds f31,f3,f19,f31
	ctx.f31.f64 = double(float(ctx.f3.f64 * ctx.f19.f64 + ctx.f31.f64));
	// fmadds f30,f3,f20,f30
	ctx.f30.f64 = double(float(ctx.f3.f64 * ctx.f20.f64 + ctx.f30.f64));
	// fmsubs f12,f24,f5,f12
	ctx.f12.f64 = double(float(ctx.f24.f64 * ctx.f5.f64 - ctx.f12.f64));
	// fmadds f11,f23,f5,f11
	ctx.f11.f64 = double(float(ctx.f23.f64 * ctx.f5.f64 + ctx.f11.f64));
	// fmuls f2,f3,f2
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f2.f64));
	// fmuls f28,f23,f9
	ctx.f28.f64 = double(float(ctx.f23.f64 * ctx.f9.f64));
	// fmuls f27,f24,f9
	ctx.f27.f64 = double(float(ctx.f24.f64 * ctx.f9.f64));
	// fmuls f9,f25,f9
	ctx.f9.f64 = double(float(ctx.f25.f64 * ctx.f9.f64));
	// fmuls f1,f1,f3
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f3.f64));
	// fmadds f13,f21,f5,f13
	ctx.f13.f64 = double(float(ctx.f21.f64 * ctx.f5.f64 + ctx.f13.f64));
	// fmuls f12,f12,f3
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f3.f64));
	// fmadds f11,f6,f25,f11
	ctx.f11.f64 = double(float(ctx.f6.f64 * ctx.f25.f64 + ctx.f11.f64));
	// fmsubs f3,f3,f21,f29
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f21.f64 - ctx.f29.f64));
	// fmadds f31,f4,f21,f31
	ctx.f31.f64 = double(float(ctx.f4.f64 * ctx.f21.f64 + ctx.f31.f64));
	// fmadds f30,f19,f5,f30
	ctx.f30.f64 = double(float(ctx.f19.f64 * ctx.f5.f64 + ctx.f30.f64));
	// fadds f2,f28,f2
	ctx.f2.f64 = double(float(ctx.f28.f64 + ctx.f2.f64));
	// fadds f1,f27,f1
	ctx.f1.f64 = double(float(ctx.f27.f64 + ctx.f1.f64));
	// fmadds f13,f4,f20,f13
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f20.f64 + ctx.f13.f64));
	// fadds f12,f9,f12
	ctx.f12.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
	// fmuls f9,f11,f5
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f5.f64));
	// fmuls f29,f4,f11
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmuls f11,f6,f11
	ctx.f11.f64 = double(float(ctx.f6.f64 * ctx.f11.f64));
	// fnmsubs f3,f4,f19,f3
	ctx.f3.f64 = double(float(-(ctx.f4.f64 * ctx.f19.f64 - ctx.f3.f64)));
	// fnmsubs f5,f20,f5,f31
	ctx.f5.f64 = double(float(-(ctx.f20.f64 * ctx.f5.f64 - ctx.f31.f64)));
	// stfs f5,84(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f4,f4,f22,f30
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f22.f64 - ctx.f30.f64)));
	// stfs f4,88(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fnmsubs f13,f6,f19,f13
	ctx.f13.f64 = double(float(-(ctx.f6.f64 * ctx.f19.f64 - ctx.f13.f64)));
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fadds f2,f2,f9
	ctx.f2.f64 = double(float(ctx.f2.f64 + ctx.f9.f64));
	// fadds f1,f1,f29
	ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f29.f64));
	// fadds f13,f12,f11
	ctx.f13.f64 = double(float(ctx.f12.f64 + ctx.f11.f64));
	// fnmsubs f12,f6,f20,f3
	ctx.f12.f64 = double(float(-(ctx.f6.f64 * ctx.f20.f64 - ctx.f3.f64)));
	// fmuls f11,f2,f0
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmuls f9,f1,f0
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f6,f13,f0
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fadds f3,f11,f8
	ctx.f3.f64 = double(float(ctx.f11.f64 + ctx.f8.f64));
	// stfs f3,96(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f2,f9,f10
	ctx.f2.f64 = double(float(ctx.f9.f64 + ctx.f10.f64));
	// fadds f1,f6,f7
	ctx.f1.f64 = double(float(ctx.f6.f64 + ctx.f7.f64));
	// lwz r3,184(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 184);
	// stfs f2,100(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// lwz r6,100(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// stfs f12,92(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// addi r4,r1,140
	ctx.r4.s64 = ctx.r1.s64 + 140;
	// stfs f1,104(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// lwz r11,4(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r10,12(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lwz r9,8(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r7,96(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r3,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r3.u32);
	// stw r11,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r11.u32);
	// stw r10,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r10.u32);
	// stw r9,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r9.u32);
	// stw r8,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r8.u32);
	// stw r7,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r7.u32);
	// stw r6,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r6.u32);
	// b 0x830cb15c
	goto loc_830CB15C;
loc_830CAFF0:
	// lfs f12,120(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 120);
	ctx.f12.f64 = double(temp.f32);
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// lfs f13,112(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// fmr f9,f12
	ctx.f9.f64 = ctx.f12.f64;
	// lfs f10,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f10.f64 = double(temp.f32);
	// fmr f11,f13
	ctx.f11.f64 = ctx.f13.f64;
	// fmr f8,f10
	ctx.f8.f64 = ctx.f10.f64;
	// lfs f4,124(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	ctx.f4.f64 = double(temp.f32);
	// lfs f7,184(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 184);
	ctx.f7.f64 = double(temp.f32);
	// li r8,0
	ctx.r8.s64 = 0;
	// lfs f0,6380(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 6380);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r31,160
	ctx.r11.s64 = ctx.r31.s64 + 160;
	// lfs f5,176(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 176);
	ctx.f5.f64 = double(temp.f32);
	// fmsubs f3,f4,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f4.f64 - ctx.f0.f64));
	// lfs f6,180(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	ctx.f6.f64 = double(temp.f32);
	// addi r11,r31,112
	ctx.r11.s64 = ctx.r31.s64 + 112;
	// lfs f2,160(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	ctx.f2.f64 = double(temp.f32);
	// addi r6,r1,140
	ctx.r6.s64 = ctx.r1.s64 + 140;
	// lfs f1,172(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 172);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f31,f2,f12
	ctx.f31.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// fmuls f30,f1,f13
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
	// lfs f27,164(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 164);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f26,f9,f7
	ctx.f26.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// lfs f29,168(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 168);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f28,f7,f11
	ctx.f28.f64 = double(float(ctx.f7.f64 * ctx.f11.f64));
	// lfs f0,7676(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 7676);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f25,f8,f5
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// lfs f24,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f23,f9,f6
	ctx.f23.f64 = double(float(ctx.f9.f64 * ctx.f6.f64));
	// lfs f22,128(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f21,f1,f12
	ctx.f21.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// lfs f20,136(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f18,f6,f3
	ctx.f18.f64 = double(float(ctx.f6.f64 * ctx.f3.f64));
	// stw r8,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r8.u32);
	// fmuls f17,f5,f3
	ctx.f17.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fmuls f3,f7,f3
	ctx.f3.f64 = double(float(ctx.f7.f64 * ctx.f3.f64));
	// fmuls f19,f2,f13
	ctx.f19.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// fmadds f30,f2,f4,f30
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f4.f64 + ctx.f30.f64));
	// fmadds f26,f8,f6,f26
	ctx.f26.f64 = double(float(ctx.f8.f64 * ctx.f6.f64 + ctx.f26.f64));
	// fmsubs f28,f9,f5,f28
	ctx.f28.f64 = double(float(ctx.f9.f64 * ctx.f5.f64 - ctx.f28.f64));
	// fmsubs f6,f6,f11,f25
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f11.f64 - ctx.f25.f64));
	// fmsubs f7,f8,f7,f23
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f7.f64 - ctx.f23.f64));
	// fmadds f31,f27,f4,f31
	ctx.f31.f64 = double(float(ctx.f27.f64 * ctx.f4.f64 + ctx.f31.f64));
	// fmadds f25,f29,f4,f21
	ctx.f25.f64 = double(float(ctx.f29.f64 * ctx.f4.f64 + ctx.f21.f64));
	// fmsubs f23,f1,f4,f19
	ctx.f23.f64 = double(float(ctx.f1.f64 * ctx.f4.f64 - ctx.f19.f64));
	// fmadds f5,f5,f11,f26
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f11.f64 + ctx.f26.f64));
	// fmuls f28,f28,f4
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f4.f64));
	// fmuls f6,f6,f4
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// fmuls f4,f7,f4
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f4.f64));
	// fmadds f7,f29,f10,f30
	ctx.f7.f64 = double(float(ctx.f29.f64 * ctx.f10.f64 + ctx.f30.f64));
	// fmadds f1,f1,f10,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f10.f64 + ctx.f31.f64));
	// fmadds f31,f27,f13,f25
	ctx.f31.f64 = double(float(ctx.f27.f64 * ctx.f13.f64 + ctx.f25.f64));
	// fnmsubs f30,f27,f10,f23
	ctx.f30.f64 = double(float(-(ctx.f27.f64 * ctx.f10.f64 - ctx.f23.f64)));
	// fmuls f8,f8,f5
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// fmuls f9,f9,f5
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f5.f64));
	// fmuls f11,f5,f11
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// fadds f6,f3,f6
	ctx.f6.f64 = double(float(ctx.f3.f64 + ctx.f6.f64));
	// fadds f5,f17,f4
	ctx.f5.f64 = double(float(ctx.f17.f64 + ctx.f4.f64));
	// fadds f28,f18,f28
	ctx.f28.f64 = double(float(ctx.f18.f64 + ctx.f28.f64));
	// fnmsubs f3,f29,f13,f1
	ctx.f3.f64 = double(float(-(ctx.f29.f64 * ctx.f13.f64 - ctx.f1.f64)));
	// stfs f3,84(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fnmsubs f4,f27,f12,f7
	ctx.f4.f64 = double(float(-(ctx.f27.f64 * ctx.f12.f64 - ctx.f7.f64)));
	// stfs f4,80(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fnmsubs f1,f29,f12,f30
	ctx.f1.f64 = double(float(-(ctx.f29.f64 * ctx.f12.f64 - ctx.f30.f64)));
	// stfs f1,92(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fnmsubs f2,f2,f10,f31
	ctx.f2.f64 = double(float(-(ctx.f2.f64 * ctx.f10.f64 - ctx.f31.f64)));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fadds f12,f6,f9
	ctx.f12.f64 = double(float(ctx.f6.f64 + ctx.f9.f64));
	// fadds f11,f5,f11
	ctx.f11.f64 = double(float(ctx.f5.f64 + ctx.f11.f64));
	// fadds f13,f28,f8
	ctx.f13.f64 = double(float(ctx.f28.f64 + ctx.f8.f64));
	// fmuls f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f8,f11,f0
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f10,f13,f0
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fadds f6,f20,f9
	ctx.f6.f64 = double(float(ctx.f20.f64 + ctx.f9.f64));
	// stfs f6,104(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fadds f5,f8,f22
	ctx.f5.f64 = double(float(ctx.f8.f64 + ctx.f22.f64));
	// stfs f5,96(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fadds f7,f24,f10
	ctx.f7.f64 = double(float(ctx.f24.f64 + ctx.f10.f64));
	// stfs f7,100(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// lwz r4,8(r7)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// lwz r3,12(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// lwz r10,4(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r9,96(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// stw r4,8(r6)
	PPC_STORE_U32(ctx.r6.u32 + 8, ctx.r4.u32);
	// stw r10,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r10.u32);
	// stw r8,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r8.u32);
	// stw r3,12(r6)
	PPC_STORE_U32(ctx.r6.u32 + 12, ctx.r3.u32);
	// stw r9,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r9.u32);
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r11.u32);
loc_830CB15C:
	// lwz r5,104(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r11,152(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 152);
	// lwz r3,1412(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1412);
	// stw r5,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r5.u32);
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r11.u32);
	// bl 0x8315fb80
	ctx.lr = 0x830CB178;
	sub_8315FB80(ctx, base);
	// stw r3,156(r31)
	PPC_STORE_U32(ctx.r31.u32 + 156, ctx.r3.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,384
	ctx.r1.s64 = ctx.r1.s64 + 384;
	// addi r12,r1,-32
	ctx.r12.s64 = ctx.r1.s64 + -32;
	// bl 0x82cb6b08
	ctx.lr = 0x830CB18C;
	__restfpr_17(ctx, base);
	// b 0x82cb113c
	__restgprlr_29(ctx, base);
	return;
loc_830CB190:
	// li r11,0
	ctx.r11.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r11,156(r31)
	PPC_STORE_U32(ctx.r31.u32 + 156, ctx.r11.u32);
	// addi r1,r1,384
	ctx.r1.s64 = ctx.r1.s64 + 384;
	// addi r12,r1,-32
	ctx.r12.s64 = ctx.r1.s64 + -32;
	// bl 0x82cb6b08
	ctx.lr = 0x830CB1A8;
	__restfpr_17(ctx, base);
	// b 0x82cb113c
	__restgprlr_29(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_830CB1AC"))) PPC_WEAK_FUNC(sub_830CB1AC);
PPC_FUNC_IMPL(__imp__sub_830CB1AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_830CB1B0"))) PPC_WEAK_FUNC(sub_830CB1B0);
PPC_FUNC_IMPL(__imp__sub_830CB1B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,340(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 340, temp.u32);
	// lfs f13,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,344(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r31.u32 + 344, temp.u32);
	// lfs f12,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,348(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + 348, temp.u32);
	// bl 0x830ca7e0
	ctx.lr = 0x830CB1E8;
	sub_830CA7E0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x830886e0
	ctx.lr = 0x830CB1F0;
	sub_830886E0(ctx, base);
	// lwz r11,8(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// li r4,1
	ctx.r4.s64 = 1;
	// ld r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// rldicr r6,r11,32,63
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// lwz r3,152(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 152);
	// bl 0x83160870
	ctx.lr = 0x830CB208;
	sub_83160870(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

